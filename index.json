[{"content":"目录 # 为什么需要无锁队列？ 硬件基础：理解现代CPU的行为 内存序：无锁编程的核心武器 SPSC队列：最简单的无锁实现 进阶：多生产者多消费者的挑战 性能分析与最佳实践 实际应用场景与选择指南 1. 为什么需要无锁队列？ #传统锁机制的痛点 #想象一个高频交易系统，每秒需要处理数百万笔订单。传统的基于锁的队列会带来什么问题？\n// 传统锁机制的队列 class ThreadSafeQueue { std::mutex mtx; std::queue\u0026lt;Order\u0026gt; orders; public: void push(Order order) { std::lock_guard\u0026lt;std::mutex\u0026gt; lock(mtx); // 可能阻塞！ orders.push(order); } Order pop() { std::lock_guard\u0026lt;std::mutex\u0026gt; lock(mtx); // 可能阻塞！ // ... 取数据 } }; 核心问题：\n上下文切换开销：线程阻塞时需要保存/恢复CPU状态 锁竞争：多个线程同时访问时，只有一个能获得锁 优先级反转：高优先级线程可能被低优先级线程阻塞 不可预测的延迟：延迟取决于锁的竞争情况 无锁编程的承诺 #无锁编程通过原子操作和精心设计的算法，让多个线程能够无阻塞地协作：\n// 无锁队列的理想状态 class LockFreeQueue { public: bool push(T item) { // 原子操作，永不阻塞 // 要么成功，要么失败，但不会等待 } std::optional\u0026lt;T\u0026gt; pop() { // 同样是原子操作 // 要么返回数据，要么返回空，但不会阻塞 } }; 关键优势：\n确定性延迟：操作在固定步骤内完成 高并发性能：多个线程可以同时操作 无死锁风险：没有锁就没有死锁 2. 硬件基础：理解现代CPU的行为 #2.1 CPU缓存架构 #现代CPU的内存层次结构：\nCPU Core 1 CPU Core 2 | | L1 Cache L1 Cache | | L2 Cache L2 Cache | | L3 Cache (共享) | 主内存 (RAM) 关键问题：当Core 1修改了某个值，Core 2什么时候能看到？\n2.2 缓存一致性协议 (MESI) #CPU使用MESI协议来维护缓存一致性：\n状态 含义 行为 Modified 已修改，独占 可读写，需要写回主内存 Exclusive 独占，未修改 可读写，与主内存一致 Shared 共享 只读，多个核心都有副本 Invalid 无效 不可用，需要重新加载 实际影响：\n// 线程1执行 data = 42; // 导致其他核心的缓存失效 flag = true; // 触发缓存同步 // 线程2执行 if (flag) { // 可能看到flag=true use(data); // 但data可能还是旧值！ } 2.3 指令重排序：单线程 vs 多线程 #指令重排序的影响在单线程和多线程环境中截然不同：\n单线程环境中的重排序 #在单线程中，编译器和CPU可以自由重排序指令，但必须保证：\nas-if-serial语义：重排序后的执行结果与程序顺序执行的结果完全一致 数据依赖性：有真实数据依赖的指令不能重排序 对程序员来说是\u0026quot;透明\u0026quot;的 // 单线程中的重排序示例 int a = 1; // ① int b = 2; // ② 可能与①重排序，因为无依赖关系 int c = a + b; // ③ 不能重排到①②之前，因为有数据依赖 // CPU可能的执行顺序：②①③ 或 ①②③，但结果都相同 多线程环境中的重排序问题 #多线程中，重排序会破坏线程间的同步，导致严重问题：\n// 多线程中的经典问题 // 共享变量 int data = 0; bool ready = false; // 线程1（生产者） void producer() { data = 42; // ① ready = true; // ② // CPU可能重排序为：② ① } // 线程2（消费者） void consumer() { if (ready) { // ③ 看到ready=true use(data); // ④ 但可能读到data=0！ } } 关键区别：\n单线程：重排序不影响程序正确性，编译器/CPU可以自由优化 多线程：重排序破坏线程间的happen-before关系，需要显式同步 这就是为什么无锁编程需要内存序！我们需要工具来控制多线程环境中的指令重排序。\n3. 内存序：无锁编程的核心武器 #3.1 内存序的本质 #内存序是对编译器和CPU的约束指令，告诉它们哪些操作不能重排序。\nenum memory_order { memory_order_relaxed, // 最宽松：只保证原子性 memory_order_acquire, // 获取：防止后续操作前移 memory_order_release, // 释放：防止之前操作后移 memory_order_acq_rel, // 获取-释放：两者结合 memory_order_seq_cst // 顺序一致：最严格 }; 3.2 Acquire-Release模型详解 #这是无锁编程中最重要的概念：\n// 生产者线程 void producer() { data.store(42, memory_order_relaxed); // ① ready.store(true, memory_order_release); // ② release // release确保①不会重排到②之后 } // 消费者线程 void consumer() { if (ready.load(memory_order_acquire)) { // ③ acquire int value = data.load(memory_order_relaxed); // ④ // acquire确保④不会重排到③之前 assert(value == 42); // 保证成功！ } } 同步保证：\nRelease操作：确保①不会重排到②之后 Acquire操作：确保④不会重排到③之前 Happens-before关系：如果③看到②的结果，那么④一定能看到①的结果 3.3 内存序的性能影响 #不同内存序的性能开销（典型x86-64架构）：\n// 性能从高到低： memory_order_relaxed // ~1 cycle memory_order_acquire // ~1-2 cycles memory_order_release // ~1-2 cycles memory_order_acq_rel // ~2-3 cycles memory_order_seq_cst // ~10-20 cycles（需要内存屏障） 关键原则：使用能满足需求的最弱内存序。\n4. SPSC队列：最简单的无锁实现 #4.1 设计思路 #单生产者单消费者(SPSC)队列是最简单的无锁队列：\ntemplate\u0026lt;typename T, size_t Capacity\u0026gt; class SPSCQueue { private: std::array\u0026lt;T, Capacity\u0026gt; buffer; // 关键设计：head只被消费者修改，tail只被生产者修改 alignas(64) std::atomic\u0026lt;size_t\u0026gt; head{0}; // 消费者索引 alignas(64) std::atomic\u0026lt;size_t\u0026gt; tail{0}; // 生产者索引 public: // 生产者调用 bool push(T item); // 消费者调用 std::optional\u0026lt;T\u0026gt; pop(); }; 4.2 Push操作的精妙设计 #bool push(T item) { const size_t current_tail = tail.load(std::memory_order_relaxed); // ① const size_t next_tail = (current_tail + 1) % Capacity; // 检查队列是否满 if (next_tail == head.load(std::memory_order_acquire)) { // ② return false; } buffer[current_tail] = std::move(item); // ③ tail.store(next_tail, std::memory_order_release); // ④ return true; } 内存序分析：\n①: relaxed - 读取自己的tail，无需同步 ②: acquire - 确保看到消费者的最新head值 ③: 数据写入，受release保护 ④: release - 确保数据写入完成后，才让消费者看到新tail 4.3 Pop操作的对称设计 #std::optional\u0026lt;T\u0026gt; pop() { const size_t current_head = head.load(std::memory_order_relaxed); // ① // 检查队列是否空 if (current_head == tail.load(std::memory_order_acquire)) { // ② return std::nullopt; } T item = std::move(buffer[current_head]); // ③ head.store((current_head + 1) % Capacity, std::memory_order_release); // ④ return item; } 对称的内存序：\n②: acquire - 确保看到生产者的最新tail值 ④: release - 确保数据读取完成后，才让生产者看到新head 基于ring buffer实现的spsc #Ring Buffer 的核心特征：\n使用固定大小数组作为缓存区；\n写指针（tail）和读指针（head）循环推进；\n当 tail == head，队列为空；\n当 (tail + 1) % N == head，队列为满；\n所有操作不涉及动态内存分配；\n利用了数组\u0026quot;环绕\u0026quot;特性：写满从头开始，读完从头开始。\n为什么用 ring buffer 做 SPSC queue？ 因为：\n固定大小的内存开销，完全在栈或预分配内存中，避免堆分配（HFT系统关键）；\n指针推进为 O(1) 操作；\n无需内存管理，简洁安全；\n极低延迟：入队、出队仅仅是一个数组访问 + 原子变量操作。\n4.4 为什么这样设计有效？ #关键洞察：每个指针只被一个线程修改！\n生产者：只修改tail，只读取head 消费者：只修改head，只读取tail 这样避免了复杂的CAS操作，通过acquire-release建立同步点。\n5. 进阶：多生产者多消费者的挑战 #5.1 多生产者单消费者 (MPSC) #当有多个生产者时，主要挑战是竞争tail指针：\ntemplate\u0026lt;typename T\u0026gt; class MPSCQueue { private: struct Node { std::atomic\u0026lt;T*\u0026gt; data{nullptr}; std::atomic\u0026lt;Node*\u0026gt; next{nullptr}; }; alignas(64) std::atomic\u0026lt;Node*\u0026gt; head; // 消费者读取 alignas(64) std::atomic\u0026lt;Node*\u0026gt; tail; // 生产者竞争 public: void push(T item) { Node* new_node = new Node; T* data_ptr = new T(std::move(item)); new_node-\u0026gt;data.store(data_ptr, std::memory_order_relaxed); // 关键：原子地获取tail位置 Node* prev_tail = tail.exchange(new_node, std::memory_order_acq_rel); // 链接到链表 prev_tail-\u0026gt;next.store(new_node, std::memory_order_release); } }; 核心技术：\nexchange操作：原子地swap两个值 链表结构：避免固定大小限制 延迟链接：先获取位置，再建立链接 5.2 多生产者多消费者 (MPMC) #MPMC是最复杂的情况，需要序列号机制：\ntemplate\u0026lt;typename T, size_t Capacity\u0026gt; class MPMCQueue { private: struct Cell { std::atomic\u0026lt;T*\u0026gt; data{nullptr}; std::atomic\u0026lt;size_t\u0026gt; sequence{0}; // 关键：序列号 }; alignas(64) std::array\u0026lt;Cell, Capacity\u0026gt; buffer; alignas(64) std::atomic\u0026lt;size_t\u0026gt; enqueue_pos{0}; alignas(64) std::atomic\u0026lt;size_t\u0026gt; dequeue_pos{0}; public: bool push(T item) { Cell* cell; size_t pos = enqueue_pos.load(std::memory_order_relaxed); for (;;) { cell = \u0026amp;buffer[pos % Capacity]; size_t seq = cell-\u0026gt;sequence.load(std::memory_order_acquire); intptr_t diff = (intptr_t)seq - (intptr_t)pos; if (diff == 0) { // 位置可用，尝试CAS占用 if (enqueue_pos.compare_exchange_weak(pos, pos + 1, std::memory_order_relaxed)) { break; } } else if (diff \u0026lt; 0) { return false; // 队列满 } else { pos = enqueue_pos.load(std::memory_order_relaxed); } } // 存储数据并更新序列号 T* data_ptr = new T(std::move(item)); cell-\u0026gt;data.store(data_ptr, std::memory_order_relaxed); cell-\u0026gt;sequence.store(pos + 1, std::memory_order_release); return true; } }; 序列号机制的妙处：\n状态编码：通过序列号差值判断cell状态 ABA问题解决：序列号单调递增，避免ABA 公平性：所有线程都有机会获取位置 5.3 复杂度对比 # 队列类型 同步复杂度 内存开销 适用场景 SPSC 最简单 最小 单线程生产消费 MPSC 中等 动态增长 多数据源，单处理器 MPMC 最复杂 固定大小 通用多线程场景 6. 性能分析与最佳实践 #6.1 实际性能数据 #基于Intel Xeon E5-2680v4的测试结果：\n队列类型 延迟(纳秒) 吞吐量(操作/秒) CPU缓存命中率 SPSC 10-20 100,000,000 \u0026gt;95% MPSC 100-200 20,000,000 85-90% MPMC 200-500 5,000,000 70-80% 互斥锁队列 1000-5000 1,000,000 60-70% 6.2 关键优化技巧 #缓存行对齐 #// 避免伪共享 alignas(std::hardware_destructive_interference_size) std::atomic\u0026lt;size_t\u0026gt; head{0}; alignas(std::hardware_destructive_interference_size) std::atomic\u0026lt;size_t\u0026gt; tail{0}; 位运算优化 #// 当Capacity是2的幂时 size_t next_pos = (pos + 1) \u0026amp; (Capacity - 1); // 比%快 批量操作 #// 批量push可以摊薄原子操作开销 template\u0026lt;typename Iterator\u0026gt; size_t push_batch(Iterator begin, Iterator end) { size_t count = 0; for (auto it = begin; it != end; ++it) { if (push(*it)) ++count; else break; } return count; } 6.3 内存管理考虑 #// 使用内存池避免频繁分配 class MemoryPool { std::atomic\u0026lt;Node*\u0026gt; free_list{nullptr}; public: Node* allocate() { Node* node = free_list.load(std::memory_order_acquire); while (node \u0026amp;\u0026amp; !free_list.compare_exchange_weak( node, node-\u0026gt;next, std::memory_order_release)) { // 重试 } return node ? node : new Node; } void deallocate(Node* node) { node-\u0026gt;next = free_list.load(std::memory_order_relaxed); while (!free_list.compare_exchange_weak( node-\u0026gt;next, node, std::memory_order_release)) { // 重试 } } }; 7. 实际应用场景与选择指南 #7.1 应用场景分析 #高频交易系统 #// 订单处理管道 class OrderProcessor { SPSCQueue\u0026lt;Order, 10000\u0026gt; incoming_orders; // 网络线程→处理线程 SPSCQueue\u0026lt;Trade, 10000\u0026gt; outgoing_trades; // 处理线程→发送线程 void process_orders() { while (auto order = incoming_orders.pop()) { Trade trade = execute_order(*order); outgoing_trades.push(std::move(trade)); } } }; 为什么选择SPSC：\n延迟要求极低（微秒级） 清晰的流水线架构 确定性性能 日志系统 #// 多线程日志收集 class Logger { MPSCQueue\u0026lt;LogEntry\u0026gt; log_queue; void log_from_thread(std::string message) { LogEntry entry{std::this_thread::get_id(), std::move(message)}; log_queue.push(std::move(entry)); } void background_writer() { while (auto entry = log_queue.pop()) { write_to_file(*entry); } } }; 为什么选择MPSC：\n多个线程产生日志 单个后台线程处理 允许适度的延迟 任务调度系统 #// 通用任务队列 class TaskScheduler { MPMCQueue\u0026lt;Task, 1000\u0026gt; task_queue; std::vector\u0026lt;std::thread\u0026gt; workers; void schedule_task(Task task) { task_queue.push(std::move(task)); } void worker_thread() { while (auto task = task_queue.pop()) { task-\u0026gt;execute(); } } }; 为什么选择MPMC：\n多个线程提交任务 多个工作线程处理 负载均衡需求 7.2 选择决策树 #开始 ├── 只有一个生产者？ │ ├── 是：只有一个消费者？ │ │ ├── 是：选择 SPSC ✓ │ │ └── 否：选择 SPMC │ └── 否：只有一个消费者？ │ ├── 是：选择 MPSC ✓ │ └── 否：选择 MPMC ✓ 7.3 实施建议 #原型验证 #// 先用简单的SPSC验证设计 SPSCQueue\u0026lt;Message, 1024\u0026gt; prototype_queue; // 测试基本功能 assert(prototype_queue.push(Message{\u0026#34;test\u0026#34;})); auto msg = prototype_queue.pop(); assert(msg.has_value()); 性能测试 #// 延迟测试 auto start = std::chrono::high_resolution_clock::now(); queue.push(item); auto item_opt = queue.pop(); auto end = std::chrono::high_resolution_clock::now(); auto latency = std::chrono::duration_cast\u0026lt;std::chrono::nanoseconds\u0026gt;(end - start); 渐进式优化 # 先确保正确性：使用较强的内存序 测量性能瓶颈：找出热点代码 逐步优化：放松不必要的内存序约束 持续验证：确保优化不影响正确性 总结 #无锁队列是现代高性能系统的关键组件，但选择和实现需要深入理解：\n核心要点 # 硬件基础：理解CPU缓存和指令重排序的影响差异 内存序：掌握acquire-release模型，详见内存序与无锁队列 设计权衡：复杂度vs性能vs适用性 实际应用：根据场景选择合适的实现 关键原则 # 单线程vs多线程：重排序的影响截然不同 最小化同步：使用最弱的内存序满足需求 渐进优化：先保证正确性，再优化性能 参考文献: # C++ Memory Model Lock-free Algorithms: Queues moodycamel::ConcurrentQueue Fast Single-Producer Single-Consumer Ring Buffer Lock-Free Single-Producer Single-Consumer Circular Queue Folly UnboundedQueue An Introduction to Lock-Free Programming Concurrent UM Queues Lock-Free Single-Producer Single-Consumer Queue ","date":"11 June 2025","permalink":"/blog/2025-06-24-lock_free_queue_implementation/","section":"Blog","summary":"目录 # 为什么需要无锁队列？ 硬件基础：理解现代CPU的行为 内存序：无锁编程的核心武器 SPSC队列：最简单的无锁实现 进阶：多生产者多消费者的挑战 性能分析与最佳实践 实际应用场景与选择指南 1. 为什么需要无锁队列？ #传统锁机制的痛点 #想象一个高频交易系统，每秒需要处理数百万笔订单。传统的基于锁的队列会带来什么问题？\n// 传统锁机制的队列 class ThreadSafeQueue { std::mutex mtx; std::queue\u0026lt;Order\u0026gt; orders; public: void push(Order order) { std::lock_guard\u0026lt;std::mutex\u0026gt; lock(mtx); // 可能阻塞！ orders.push(order); } Order pop() { std::lock_guard\u0026lt;std::mutex\u0026gt; lock(mtx); // 可能阻塞！ // ... 取数据 } }; 核心问题：\n上下文切换开销：线程阻塞时需要保存/恢复CPU状态 锁竞争：多个线程同时访问时，只有一个能获得锁 优先级反转：高优先级线程可能被低优先级线程阻塞 不可预测的延迟：延迟取决于锁的竞争情况 无锁编程的承诺 #无锁编程通过原子操作和精心设计的算法，让多个线程能够无阻塞地协作：\n// 无锁队列的理想状态 class LockFreeQueue { public: bool push(T item) { // 原子操作，永不阻塞 // 要么成功，要么失败，但不会等待 } std::optional\u0026lt;T\u0026gt; pop() { // 同样是原子操作 // 要么返回数据，要么返回空，但不会阻塞 } }; 关键优势：","title":"深入理解无锁队列：从原理到实践的完整指南"},{"content":"引言 #在高性能网络编程领域，DPDK（Data Plane Development Kit）作为用户态网络驱动框架，能够实现单核数千万PPS（包每秒）的惊人性能。然而，DPDK强制要求使用hugepage的设计决策常常让初学者困惑：为什么不能使用传统的malloc/new分配内存？hugepage究竟解决了什么根本性问题？\n本文将从内存管理的底层原理出发，深入分析DPDK使用hugepage的技术必然性，揭示这一设计选择背后的深层架构考量。\n1. 澄清关键概念：Hugepage不是\u0026quot;大内存分配\u0026quot; #1.1 常见的概念误区 #许多开发者错误地认为hugepage就是\u0026quot;分配大块内存\u0026quot;的机制，这是对hugepage本质的根本性误解。\n错误理解：\n// 误以为这就是hugepage的作用 char* large_buffer = malloc(1024 * 1024 * 1024); // 分配1GB内存 正确理解： Hugepage是操作系统内存管理粒度的改变，而不是应用层面的内存分配大小问题。\n1.2 Hugepage的本质定义 #标准内存管理：\n操作系统以4KB为单位管理物理内存 每个虚拟内存页对应一个4KB的物理内存页 页表项记录虚拟页到物理页的映射关系 Hugepage内存管理：\n操作系统以2MB或1GB为单位管理物理内存 每个虚拟内存页对应一个2MB/1GB的物理内存页 大幅减少页表项数量，改变内存管理的基本粒度 1.3 malloc vs hugepage的本质差异 #malloc分配大内存的实际情况 #char* buffer = malloc(1024 * 1024 * 1024); // 分配1GB 虚拟内存视角：\n应用程序看到连续的1GB虚拟地址空间 地址范围：[0x100000000 - 0x140000000] 物理内存实际情况：\n需要的4KB页面数：1GB ÷ 4KB = 262,144个页面 页表项数量：262,144个 物理内存布局：完全分散，可能遍布整个物理内存空间 典型的物理地址映射： 虚拟页0x100000000 → 物理页0x87654000 虚拟页0x100001000 → 物理页0x12345000 虚拟页0x100002000 → 物理页0x98765000 ... 物理地址毫无连续性可言 Hugepage分配的内存特征 #char* buffer = mmap(NULL, 1024*1024*1024, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_HUGETLB, -1, 0); 内存管理的根本改变：\n需要的2MB页面数：1GB ÷ 2MB = 512个页面 页表项数量：512个 物理内存布局：每个2MB块内部物理地址连续 物理地址映射： 虚拟[0x100000000-0x100200000] → 物理[0x80000000-0x80200000] (连续2MB) 虚拟[0x100200000-0x100400000] → 物理[0x80200000-0x80400000] (连续2MB) ... 每个hugepage内部保证物理连续性 2. DPDK架构对内存管理的特殊要求 #2.1 用户态网络驱动的基本原理 #传统内核网络栈的限制：\n数据包处理需要内核态用户态切换 多次内存拷贝（网卡→内核缓冲区→用户缓冲区） 复杂的协议栈处理增加延迟 单核处理能力限制在数十万PPS DPDK的革命性设计：\n完全绕过内核，用户态直接操作网卡 零拷贝数据处理路径 轮询模式驱动，消除中断开销 实现单核数千万PPS的处理能力 2.2 用户态驱动对内存的核心需求 #这种架构变革带来了对内存管理的全新要求：\nDMA一致性需求：\n网卡通过DMA直接访问用户态内存 DMA操作必须使用物理地址 需要保证虚拟内存到物理内存映射的可预测性 零拷贝设计要求：\n数据包从网卡接收到应用处理全程零拷贝 内存布局必须对网卡硬件友好 避免任何形式的数据搬移 高频内存操作：\n每秒数千万次的内存分配/释放操作 大量的内存池管理 对内存访问延迟极度敏感 3. DMA一致性：Hugepage的核心价值 #3.1 网卡DMA的工作机制 #DMA控制器的特点：\n只理解物理地址，不支持虚拟内存概念 需要连续的物理地址范围进行高效传输 无法处理复杂的地址转换和页表查找 网卡接收数据包的流程：\n1. 网卡接收到以太网帧 2. DMA控制器需要将数据写入内存 3. 驱动程序预先提供物理地址和长度 4. DMA控制器直接写入指定的物理内存区域 3.2 传统内存分配的DMA困境 #使用malloc的问题分析 #// DPDK应用尝试使用malloc分配接收缓冲区 char* rx_buffer = malloc(1024 * 1024); // 1MB接收缓冲区 面临的技术挑战：\n物理内存碎片化：\n1MB缓冲区需要256个4KB页面 这些页面的物理地址分布： Page 0: 物理地址0x12345000 Page 1: 物理地址0x87654000 Page 2: 物理地址0x34567000 ... Page 255: 物理地址0x98765000 物理地址完全不连续 网卡DMA的处理复杂性：\n// 需要为网卡准备scatter-gather描述符列表 struct dma_descriptor { uint64_t physical_addr; uint32_t length; uint32_t flags; }; // 1MB缓冲区需要256个描述符 struct dma_descriptor sg_list[256] = { {0x12345000, 4096, DMA_DESC_FLAG}, {0x87654000, 4096, DMA_DESC_FLAG}, {0x34567000, 4096, DMA_DESC_FLAG}, // ... 253个更多的描述符 }; 性能和复杂性的双重问题：\n网卡需要处理256个独立的DMA操作 每个DMA操作都有设置和完成开销 描述符本身占用额外的内存和带宽 网卡硬件的scatter-gather能力有限 3.3 Hugepage的DMA解决方案 #物理内存连续性保证 #// 使用hugepage分配接收缓冲区 char* rx_buffer = mmap(NULL, 1024*1024, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_HUGETLB, -1, 0); 内存布局的根本改善：\n1MB缓冲区只需要1个2MB hugepage（部分使用） 物理内存布局： 整个1MB区域映射到连续的物理地址：0x80000000 - 0x80100000 网卡DMA操作的简化：\n// 只需要1个简单的DMA描述符 struct dma_descriptor sg_list[1] = { {0x80000000, 1048576, DMA_DESC_FLAG} // 单一连续区域 }; 性能优势的量化：\nDMA设置开销：从256次减少到1次 内存带宽利用率：提升约15-20% 网卡处理延迟：减少数百个CPU周期 描述符缓存压力：大幅降低 3.4 为什么其他方案不可行 #IOMMU虚拟化方案：\n增加额外的地址转换开销 不是所有硬件平台都支持 与DPDK的零开销设计理念冲突 预留连续物理内存方案：\n系统启动时需要预留大量内存 灵活性差，内存利用率低 管理复杂，容易造成内存浪费 软件scatter-gather方案：\n增加CPU处理开销 无法发挥网卡硬件的最大性能 与高性能目标背道而驰 4. 内存池管理：超越简单的大块分配 #4.1 DPDK内存池的设计目标 #高频对象管理需求：\n每秒数千万次mbuf（消息缓冲区）分配/释放 分配操作必须是O(1)时间复杂度 支持多线程无锁操作 避免内存碎片和垃圾回收开销 传统内存分配器的不足：\n// 传统方式的性能问题 for (int i = 0; i \u0026lt; 10000000; i++) { char* mbuf = malloc(2048); // 每次malloc都有开销 // ... 处理数据包 free(mbuf); // 每次free都有开销 } // 每秒千万次malloc/free会成为严重瓶颈 4.2 传统页面下的内存池问题 #管理复杂性 #// 使用4KB页面构建内存池 struct mempool_4kb { size_t obj_size = 2048; // 每个mbuf 2KB size_t objs_per_page = 2; // 每个4KB页面只能放2个对象 size_t total_objs = 1000000; // 需要100万个对象 size_t pages_needed = 500000; // 需要50万个4KB页面 }; 管理开销分析：\n需要维护50万个页面的映射关系 对象可能跨页边界，增加处理复杂性 页面回收时机难以精确控制 内存碎片化问题严重 缓存效率问题 #对象分布分析： Object 0: Page 0 (物理地址0x12345000) Object 1: Page 0 (物理地址0x12345800) Object 2: Page 1 (物理地址0x87654000) Object 3: Page 1 (物理地址0x87654800) ... 相邻对象可能位于物理内存的不同区域 缓存局部性差，预取效果不佳 4.3 Hugepage内存池的优势 #管理简化 #// 使用2MB hugepage构建内存池 struct mempool_hugepage { size_t obj_size = 2048; // 每个mbuf 2KB size_t objs_per_hugepage = 1024; // 每个2MB hugepage可放1024个对象 size_t total_objs = 1000000; // 需要100万个对象 size_t hugepages_needed = 977; // 只需要977个hugepage }; 管理优势：\n从50万个页面减少到不到1000个hugepage 对象索引计算简化：obj_addr = hugepage_base + obj_index * obj_size 批量操作更高效：可以批量分配/释放同一hugepage内的多个对象 内存回收策略简单：以hugepage为单位进行管理 缓存友好性 #Hugepage内对象分布： Hugepage 0 (物理地址0x80000000-0x80200000): Object 0: 0x80000000 Object 1: 0x80000800 Object 2: 0x80001000 ... Object 1023: 0x801FF800 同一hugepage内的对象物理地址连续 缓存局部性大幅改善 性能提升机制：\n硬件预取器能更好地工作 减少缓存污染 提高内存带宽利用率 降低延迟抖动 5. TLB优化：从根本上解决地址转换瓶颈 #5.1 DPDK工作集的TLB压力分析 #典型DPDK应用的内存使用模式：\n内存组成分析： - 接收队列描述符：64MB (多个网卡端口) - 发送队列描述符：64MB - 数据包缓冲池：1-4GB (mbuf pool) - 应用数据结构：256MB (路由表、连接状态等) - 统计和管理数据：32MB 总工作集：2-6GB 5.2 4KB页面下的TLB灾难 #TLB容量与工作集的严重失配：\n工作集分析（以2GB为例）： - 需要4KB页面数：2GB ÷ 4KB = 524,288个页面 - L1 TLB容量：64个条目 - L2 TLB容量：1024个条目 - 总TLB覆盖：(64 + 1024) × 4KB = 4.25MB TLB命中率：4.25MB ÷ 2GB = 0.2% TLB miss率：99.8% 性能影响的量化：\n每次内存访问的延迟： - TLB命中：3 CPU周期 - TLB miss：300 CPU周期 (包含页表遍历) 平均访问延迟： 0.002 × 3 + 0.998 × 300 = 299.4 CPU周期 在3GHz CPU上约为100纳秒每次访问 5.3 Hugepage的TLB革命 #工作集覆盖能力的质变：\n使用2MB hugepage的覆盖分析： - 需要hugepage数：2GB ÷ 2MB = 1024个页面 - L2 TLB容量：1024个条目 - TLB覆盖能力：1024 × 2MB = 2GB TLB命中率：接近100% TLB miss率：接近0% 性能提升的计算：\n使用hugepage后的平均访问延迟： 1.0 × 3 + 0.0 × 300 = 3 CPU周期 性能提升：299.4 ÷ 3 = 99.8倍 5.4 高频内存访问下的累积效应 #DPDK典型负载分析：\n// 每秒处理1000万个数据包的内存访问 void analyze_memory_access_frequency() { int packets_per_second = 10000000; int memory_accesses_per_packet = 50; // 每个包约50次内存访问 int total_accesses = packets_per_second * memory_accesses_per_packet; // = 5亿次内存访问每秒 } 4KB页面的性能灾难：\n每秒总开销计算： - 总访问次数：5亿次 - 平均延迟：299.4 CPU周期 - 总CPU周期：1497亿周期 - 在3GHz CPU上的时间：49.9秒 每秒需要49.9秒的CPU时间！ 显然无法实现实时处理 Hugepage的性能救赎：\n每秒总开销计算： - 总访问次数：5亿次 - 平均延迟：3 CPU周期 - 总CPU周期：15亿周期 - 在3GHz CPU上的时间：0.5秒 每秒只需要0.5秒的CPU时间 剩余CPU资源可用于实际的数据包处理 6. 架构必需性：不可替代的技术选择 #6.1 其他技术方案的局限性 #软件优化方案：\n缓存优化：无法解决TLB miss的根本问题 预取优化：对随机访问模式效果有限 算法优化：不能改变内存管理的物理限制 硬件辅助方案：\nIOMMU：增加额外开销，与零开销目标冲突 智能网卡：成本高，通用性差 专用硬件：失去软件灵活性 6.2 Hugepage与DPDK设计理念的完美契合 #零开销抽象：\nHugepage在提供高级功能的同时不引入额外开销 底层优化对上层应用透明 符合DPDK的性能第一原则 可移植性：\n主流处理器架构都支持hugepage 不依赖特定厂商的硬件特性 保证DPDK的跨平台兼容性 可扩展性：\n支持从小规模到大规模的部署 内存使用量可以动态调整 适应不同的应用场景需求 6.3 性能数据的最终验证 #实际测试对比：\nDPDK性能基准测试结果： 使用标准4KB页面： - 单核处理能力：约50万PPS - 平均延迟：15微秒 - CPU利用率：95%（主要消耗在内存管理） 使用2MB hugepage： - 单核处理能力：2000万PPS - 平均延迟：1微秒 - CPU利用率：60%（主要用于实际处理逻辑） 性能提升：40倍处理能力，15倍延迟改善 7. 技术实现要点 #7.1 系统级配置 ## 系统hugepage配置 echo 1024 \u0026gt; /proc/sys/vm/nr_hugepages # 挂载hugetlbfs mount -t hugetlbfs nodev /mnt/huge 7.2 DPDK应用集成 #// DPDK初始化时的hugepage配置 rte_eal_init(argc, argv); // EAL会自动配置hugepage // 内存池创建 struct rte_mempool *mbuf_pool = rte_pktmbuf_pool_create( \u0026#34;MBUF_POOL\u0026#34;, // 池名称 NUM_MBUFS, // mbuf数量 MBUF_CACHE_SIZE, // 缓存大小 0, // 私有数据大小 RTE_MBUF_DEFAULT_BUF_SIZE, // 缓冲区大小 rte_socket_id() // NUMA节点 ); 8. 结论 #DPDK对hugepage的依赖不是一个可选的性能优化，而是架构设计的基础必需。通过深入分析，我们发现hugepage在DPDK中发挥着三个不可替代的关键作用：\nDMA一致性保障： 确保网卡DMA操作的高效性和可靠性，这是用户态网络驱动的基本要求。\n内存池管理优化： 大幅简化高频内存操作的复杂性，提升内存分配效率，改善缓存行为。\nTLB瓶颈消除： 从根本上解决大工作集应用的地址转换性能问题，实现真正的线速处理。\n这三个方面相互协同，共同支撑DPDK实现单核数千万PPS的极致性能目标。任何试图绕过hugepage的方案都会在某个关键环节遭遇不可逾越的性能瓶颈。\n因此，hugepage不仅是DPDK的技术选择，更是高性能网络处理架构的必然要求。深入理解这一点，对于正确使用DPDK和设计高性能网络应用具有重要的指导意义。\n在追求极致性能的道路上，技术选择往往由底层的物理限制所决定。DPDK与hugepage的结合，正是对这一规律的完美诠释。\n","date":"21 July 2025","permalink":"/blog/2025-07-21-hugepage_indpdk/","section":"Blog","summary":"引言 #在高性能网络编程领域，DPDK（Data Plane Development Kit）作为用户态网络驱动框架，能够实现单核数千万PPS（包每秒）的惊人性能。然而，DPDK强制要求使用hugepage的设计决策常常让初学者困惑：为什么不能使用传统的malloc/new分配内存？hugepage究竟解决了什么根本性问题？\n本文将从内存管理的底层原理出发，深入分析DPDK使用hugepage的技术必然性，揭示这一设计选择背后的深层架构考量。\n1. 澄清关键概念：Hugepage不是\u0026quot;大内存分配\u0026quot; #1.1 常见的概念误区 #许多开发者错误地认为hugepage就是\u0026quot;分配大块内存\u0026quot;的机制，这是对hugepage本质的根本性误解。\n错误理解：\n// 误以为这就是hugepage的作用 char* large_buffer = malloc(1024 * 1024 * 1024); // 分配1GB内存 正确理解： Hugepage是操作系统内存管理粒度的改变，而不是应用层面的内存分配大小问题。\n1.2 Hugepage的本质定义 #标准内存管理：\n操作系统以4KB为单位管理物理内存 每个虚拟内存页对应一个4KB的物理内存页 页表项记录虚拟页到物理页的映射关系 Hugepage内存管理：\n操作系统以2MB或1GB为单位管理物理内存 每个虚拟内存页对应一个2MB/1GB的物理内存页 大幅减少页表项数量，改变内存管理的基本粒度 1.3 malloc vs hugepage的本质差异 #malloc分配大内存的实际情况 #char* buffer = malloc(1024 * 1024 * 1024); // 分配1GB 虚拟内存视角：\n应用程序看到连续的1GB虚拟地址空间 地址范围：[0x100000000 - 0x140000000] 物理内存实际情况：\n需要的4KB页面数：1GB ÷ 4KB = 262,144个页面 页表项数量：262,144个 物理内存布局：完全分散，可能遍布整个物理内存空间 典型的物理地址映射： 虚拟页0x100000000 → 物理页0x87654000 虚拟页0x100001000 → 物理页0x12345000 虚拟页0x100002000 → 物理页0x98765000 .","title":"DPDK为什么必须使用Hugepage：从内存管理本质到架构必需"},{"content":"","date":null,"permalink":"/","section":"Yu's Space","summary":"","title":"Yu's Space"},{"content":"引言 #在现代高性能计算中，内存访问性能往往成为应用程序的瓶颈。虽然CPU性能在摩尔定律驱动下快速提升，但内存访问延迟的改善相对缓慢，导致了著名的\u0026quot;内存墙\u0026quot;问题。为了缓解这一问题，现代处理器和操作系统引入了多种机制，其中TLB（Translation Lookaside Buffer）和Hugepage是两个关键的技术。\n本文将深入探讨这两种技术的工作原理，以及它们如何协同工作来提升系统性能。\n1. 虚拟内存基础 #1.1 虚拟内存系统概述 #现代操作系统普遍采用虚拟内存管理，每个进程都拥有独立的虚拟地址空间。虚拟地址需要通过页表（Page Table）转换为物理地址才能进行实际的内存访问。\n1.2 x86-64页表结构 #在x86-64架构中，虚拟地址使用48位有效位，采用4级页表结构：\n虚拟地址 (48位有效位)： [47:39] PML4索引 (9位) -\u0026gt; PML4表 (Page Map Level 4) [38:30] PDPT索引 (9位) -\u0026gt; 页目录指针表 (Page Directory Pointer Table) [29:21] PD索引 (9位) -\u0026gt; 页目录表 (Page Directory) [20:12] PT索引 (9位) -\u0026gt; 页表 (Page Table) [11:0] 页内偏移 (12位) -\u0026gt; 4KB页面内偏移 1.3 地址转换过程 #标准4KB页面的地址转换需要遍历完整的4级页表：\nphysical_addr translate_address(virtual_addr vaddr) { // 1. 从CR3寄存器获取PML4表基址 pml4_entry = PML4_BASE + ((vaddr \u0026gt;\u0026gt; 39) \u0026amp; 0x1FF) * 8; // 2. 读取PML4表项，获取PDPT表基址 pdpt_base = read_memory(pml4_entry) \u0026amp; PAGE_MASK; pdpt_entry = pdpt_base + ((vaddr \u0026gt;\u0026gt; 30) \u0026amp; 0x1FF) * 8; // 3. 读取PDPT表项，获取PD表基址 pd_base = read_memory(pdpt_entry) \u0026amp; PAGE_MASK; pd_entry = pd_base + ((vaddr \u0026gt;\u0026gt; 21) \u0026amp; 0x1FF) * 8; // 4. 读取PD表项，获取PT表基址 pt_base = read_memory(pd_entry) \u0026amp; PAGE_MASK; pt_entry = pt_base + ((vaddr \u0026gt;\u0026gt; 12) \u0026amp; 0x1FF) * 8; // 5. 读取PT表项，获取物理页基址 page_base = read_memory(pt_entry) \u0026amp; PAGE_MASK; // 6. 组合物理地址 return page_base + (vaddr \u0026amp; 0xFFF); } 关键问题：每次地址转换需要4次内存访问，这在高频内存访问场景下会严重影响性能。\n2. TLB (Translation Lookaside Buffer) 工作原理 #2.1 TLB的本质与作用 #TLB是CPU内置的专用硬件缓存，用于缓存最近使用的虚拟地址到物理地址的映射关系。其目的是避免每次内存访问都进行耗时的页表遍历。\n2.2 TLB的硬件结构 #典型的现代CPU TLB配置：\nstruct tlb_entry { uint64_t virtual_page; // 虚拟页号 uint64_t physical_page; // 物理页号 uint32_t process_id; // 进程ID (ASID) uint8_t permissions; // 读写执行权限 bool valid; // 有效位 }; // 多级TLB结构 struct cpu_tlb { tlb_entry l1_itlb[64]; // L1指令TLB，64条目 tlb_entry l1_dtlb[64]; // L1数据TLB，64条目 tlb_entry l2_tlb[1024]; // L2统一TLB，1024条目 }; 2.3 TLB查找机制 #TLB查找采用多级缓存结构，遵循局部性原理：\nphysical_addr tlb_lookup(virtual_addr vaddr) { uint64_t vpn = vaddr \u0026gt;\u0026gt; 12; // 提取虚拟页号 // 1. L1 TLB查找 (1个CPU周期) for (int i = 0; i \u0026lt; 64; i++) { if (l1_dtlb[i].valid \u0026amp;\u0026amp; l1_dtlb[i].virtual_page == vpn \u0026amp;\u0026amp; l1_dtlb[i].process_id == current_pid) { return (l1_dtlb[i].physical_page \u0026lt;\u0026lt; 12) + (vaddr \u0026amp; 0xFFF); } } // 2. L2 TLB查找 (约10个CPU周期) for (int i = 0; i \u0026lt; 1024; i++) { if (l2_tlb[i].valid \u0026amp;\u0026amp; l2_tlb[i].virtual_page == vpn \u0026amp;\u0026amp; l2_tlb[i].process_id == current_pid) { return (l2_tlb[i].physical_page \u0026lt;\u0026lt; 12) + (vaddr \u0026amp; 0xFFF); } } // 3. TLB完全miss，触发页表遍历 (200-500个CPU周期) return page_walk(vaddr); } 2.4 TLB的容量限制与覆盖范围 #标准4KB页面下TLB的覆盖能力：\nL1 TLB覆盖范围： - 64条目 × 4KB = 256KB L2 TLB覆盖范围： - 1024条目 × 4KB = 4MB 总体覆盖范围：约4MB 重要结论：当应用程序的工作集超过4MB时，TLB miss率会显著上升，导致性能急剧下降。\n3. Hugepage工作原理 #3.1 Hugepage的概念 #Hugepage是操作系统提供的大页面机制，允许使用比标准4KB更大的内存页面。常见的hugepage大小包括：\n2MB hugepage：在x86-64上最常用 1GB hugepage：适用于超大内存应用 其他架构特定大小：如ARM的64KB页面 3.2 Hugepage的页表简化 #3.2.1 2MB Hugepage的地址转换 #2MB hugepage跳过了页表的最后一级（PT级别）：\n虚拟地址结构变化： [47:39] PML4索引 (9位) -\u0026gt; PML4表 [38:30] PDPT索引 (9位) -\u0026gt; 页目录指针表 [29:21] PD索引 (9位) -\u0026gt; 直接指向2MB物理页 [20:0] 页内偏移 (21位) -\u0026gt; 2MB页面内偏移 地址转换过程简化为：\nphysical_addr translate_hugepage_2mb(virtual_addr vaddr) { // 只需要3次内存访问，省略了PT级别 pml4_entry = PML4_BASE + ((vaddr \u0026gt;\u0026gt; 39) \u0026amp; 0x1FF) * 8; pdpt_base = read_memory(pml4_entry) \u0026amp; PAGE_MASK; pdpt_entry = pdpt_base + ((vaddr \u0026gt;\u0026gt; 30) \u0026amp; 0x1FF) * 8; pd_base = read_memory(pdpt_entry) \u0026amp; PAGE_MASK; pd_entry = pd_base + ((vaddr \u0026gt;\u0026gt; 21) \u0026amp; 0x1FF) * 8; // PD表项直接包含2MB页的物理基址 page_base = read_memory(pd_entry) \u0026amp; HUGEPAGE_MASK; return page_base + (vaddr \u0026amp; 0x1FFFFF); // 低21位是页内偏移 } 3.2.2 1GB Hugepage的极致简化 #1GB hugepage进一步简化，只需要2级页表查找：\n虚拟地址结构： [47:39] PML4索引 (9位) -\u0026gt; PML4表 [38:30] PDPT索引 (9位) -\u0026gt; 直接指向1GB物理页 [29:0] 页内偏移 (30位) -\u0026gt; 1GB页面内偏移 3.3 Hugepage对TLB效率的巨大提升 #hugepage最重要的优势在于大幅提升TLB的有效覆盖范围：\n2MB Hugepage的TLB覆盖： - L1 TLB: 64条目 × 2MB = 128MB覆盖范围 - L2 TLB: 1024条目 × 2MB = 2GB覆盖范围 - 相比4KB页面，覆盖范围提升512倍！ 1GB Hugepage的TLB覆盖： - L1 TLB: 64条目 × 1GB = 64GB覆盖范围 - L2 TLB: 1024条目 × 1GB = 1TB覆盖范围 - 相比4KB页面，覆盖范围提升262144倍！ 4. 性能分析与量化对比 #4.1 访问延迟对比 #不同内存访问场景的延迟分析：\n场景 延迟 (CPU周期) 相对差异 L1 Cache命中 1-2 基准 TLB命中 + L1 Cache命中 3-5 2-3倍 TLB命中 + 主内存访问 200-300 100-200倍 TLB miss + 页表遍历 500-1000 250-500倍 4.2 大内存应用的性能差异 #以64MB内存区域的频繁访问为例：\n// 性能对比分析 struct performance_analysis { // 4KB页面场景 int pages_needed_4kb = 64 * 1024 * 1024 / 4096; // 16384个页面 int tlb_capacity = 1024; // L2 TLB容量 double tlb_hit_rate_4kb = (double)tlb_capacity / pages_needed_4kb; // 6.25% // 每次访问的平均代价 double avg_latency_4kb = 0.0625 * 3 + 0.9375 * 300; // ≈ 282 cycles // 2MB hugepage场景 int pages_needed_2mb = 64 * 1024 * 1024 / (2 * 1024 * 1024); // 32个页面 double tlb_hit_rate_2mb = 1.0; // 100%命中 double avg_latency_2mb = 3; // 始终TLB命中 // 性能提升倍数 double performance_gain = avg_latency_4kb / avg_latency_2mb; // ≈ 94倍 }; 4.3 延迟抖动与确定性 #TLB miss导致的延迟不确定性：\n// TLB miss时页表遍历的延迟范围 struct latency_variance { int best_case; // 页表项都在L1 cache: 4 × 4 = 16 cycles int typical_case; // 页表项在L3 cache: 4 × 40 = 160 cycles int worst_case; // 页表项在主内存: 4 × 200 = 800 cycles // 延迟抖动：16-800 cycles，相差50倍 }; // Hugepage场景的稳定性： // TLB命中率接近100%，延迟稳定在3-5 cycles // 延迟抖动降低到原来的1/100 5. 使用场景与最佳实践 #5.1 适合使用Hugepage的场景 #大内存工作集：\n数据库缓冲池（几GB到几十GB） 大型哈希表或缓存 科学计算的大矩阵 内存数据库 高频内存访问：\n实时数据处理系统 高频交易系统 网络数据包处理 大规模并行计算 低延迟要求：\n实时系统 游戏引擎 音视频处理 金融交易系统 5.2 不适合使用Hugepage的场景 #小内存工作集：\n小于几MB的数据结构 短生命周期的临时缓冲区 碎片化的小对象 内存使用不规律：\n稀疏访问模式 随机小块内存分配 频繁的内存分配和释放 5.3 Hugepage的潜在问题 #内存浪费：\n// 示例：分配8KB数据使用2MB hugepage struct memory_waste { size_t requested = 8 * 1024; // 8KB size_t allocated = 2 * 1024 * 1024; // 2MB double efficiency = 8.0 / 2048; // 0.39%效率 }; 分配困难：\n需要连续的大块物理内存 内存碎片化时分配可能失败 系统启动时需要预留hugepage 交换性能：\n交换粒度变大（2MB vs 4KB） 可能影响系统整体性能 6. 实际应用指导 #6.1 如何判断是否需要Hugepage #性能分析工具：\n# 使用perf分析TLB miss perf stat -e dTLB-load-misses,dTLB-loads your_application # 分析TLB miss率 TLB_miss_rate = dTLB-load-misses / dTLB-loads # 如果miss率 \u0026gt; 10%，考虑hugepage优化 内存访问模式分析：\n// 关键指标 1. 工作集大小是否 \u0026gt; 4MB 2. 内存访问是否集中在大块连续区域 3. 是否存在高频的内存访问 4. 对延迟抖动是否敏感 6.2 Hugepage配置与使用 #系统级配置：\n# 查看hugepage状态 cat /proc/meminfo | grep Huge # 分配2MB hugepage echo 1024 \u0026gt; /proc/sys/vm/nr_hugepages # 挂载hugetlbfs mount -t hugetlbfs none /mnt/hugepages 应用程序使用：\n#include \u0026lt;sys/mman.h\u0026gt; // 使用mmap分配hugepage void* allocate_hugepage(size_t size) { void* ptr = mmap(nullptr, size, PROT_READ | PROT_WRITE, MAP_PRIVATE | MAP_ANONYMOUS | MAP_HUGETLB, -1, 0); if (ptr == MAP_FAILED) { // 分配失败，回退到普通页面 ptr = mmap(nullptr, size, PROT_READ | PROT_WRITE, MAP_PRIVATE | MAP_ANONYMOUS, -1, 0); } return ptr; } 7. 总结 #TLB和Hugepage是现代计算机系统中重要的性能优化机制。TLB通过缓存地址转换结果避免了昂贵的页表遍历，而Hugepage通过减少页表级数和大幅提升TLB覆盖范围，从根本上解决了大内存应用的TLB miss问题。\n理解这两种技术的工作原理，有助于开发者在设计高性能应用时做出正确的技术选择。在实际应用中，需要综合考虑内存使用模式、性能需求、系统资源等因素，谨慎评估是否采用hugepage优化。\n正确使用这些技术，可以在特定场景下获得数十倍甚至上百倍的性能提升，但同时也要注意避免过度优化和资源浪费。性能优化始终需要基于实际测量和分析，而不是盲目的技术堆砌。\n","date":"21 July 2025","permalink":"/blog/2025-07-21-hugepage/","section":"Blog","summary":"引言 #在现代高性能计算中，内存访问性能往往成为应用程序的瓶颈。虽然CPU性能在摩尔定律驱动下快速提升，但内存访问延迟的改善相对缓慢，导致了著名的\u0026quot;内存墙\u0026quot;问题。为了缓解这一问题，现代处理器和操作系统引入了多种机制，其中TLB（Translation Lookaside Buffer）和Hugepage是两个关键的技术。\n本文将深入探讨这两种技术的工作原理，以及它们如何协同工作来提升系统性能。\n1. 虚拟内存基础 #1.1 虚拟内存系统概述 #现代操作系统普遍采用虚拟内存管理，每个进程都拥有独立的虚拟地址空间。虚拟地址需要通过页表（Page Table）转换为物理地址才能进行实际的内存访问。\n1.2 x86-64页表结构 #在x86-64架构中，虚拟地址使用48位有效位，采用4级页表结构：\n虚拟地址 (48位有效位)： [47:39] PML4索引 (9位) -\u0026gt; PML4表 (Page Map Level 4) [38:30] PDPT索引 (9位) -\u0026gt; 页目录指针表 (Page Directory Pointer Table) [29:21] PD索引 (9位) -\u0026gt; 页目录表 (Page Directory) [20:12] PT索引 (9位) -\u0026gt; 页表 (Page Table) [11:0] 页内偏移 (12位) -\u0026gt; 4KB页面内偏移 1.3 地址转换过程 #标准4KB页面的地址转换需要遍历完整的4级页表：\nphysical_addr translate_address(virtual_addr vaddr) { // 1. 从CR3寄存器获取PML4表基址 pml4_entry = PML4_BASE + ((vaddr \u0026gt;\u0026gt; 39) \u0026amp; 0x1FF) * 8; // 2.","title":"深入理解Hugepage与TLB：原理、机制与性能优化"},{"content":"std::array 与 int a[10] 的区别与优点 #从类型系统角度 #std::array 是一个类模板，而 int a[10] 是一个内建数组类型。这个根本区别导致了它们在C++类型系统中的行为差异。\n类型退化 #传统数组 int a[10] 在作为函数参数传递时会退化(decay)为指针 int*，导致数组大小信息丢失。这种退化是C语言遗留问题，在C++中仍然存在：\nvoid func(int arr[10]) { // 实际上arr的类型是int*，大小信息已丢失 sizeof(arr); // 返回指针大小，而非数组大小 } 而 std::array 是一个完整的对象类型，传递时保留其完整类型信息：\nvoid func(std::array\u0026lt;int, 10\u0026gt;\u0026amp; arr) { sizeof(arr); // 正确返回整个数组大小 } 作为值类型 #std::array 是真正的\u0026quot;值类型\u0026quot;，可以:\n完整复制（不会退化为指针） 用于函数返回值 在STL容器中存储 参与比较操作（支持==, \u0026lt;等运算符） 不可赋值性与返回限制的底层原理 #原生数组存在两个重要限制：\nT buffer1[10]; T buffer2[10]; buffer1 = buffer2; // 编译错误 auto get_buffer() -\u0026gt; ??? { return buffer; // 错误：无法直接返回原生数组 } 1. 不能直接赋值的底层原理：\n语言规范设计：C++标准明确规定数组类型为不可赋值类型 内存模型：原生数组没有存储自身大小的元数据，编译器无法确定复制边界 底层实现：在编译器内部，数组名仅表示内存起始地址，不是可赋值的左值对象 无运算符支持：原生数组没有定义赋值运算符 编译器在处理数组时，通常将其翻译为一段连续内存区域的起始地址。当你写：\nT buffer[10]; 编译器分配10个T类型对象的空间，但buffer本身只是一个符号，代表这段内存的起始位置。它不是一个可以持有值的变量，因此不能被赋值。\n2. 不能作为返回值的底层原理：\n调用约定：函数返回机制不支持大小不确定的值传递 栈帧问题：函数返回时本地数组所在栈帧销毁，导致悬垂引用 ABI限制：二进制接口规范没有为数组返回值定义标准传递方式 而std::array通过类封装解决了这些问题，定义了赋值运算符，并作为完整对象类型支持返回值优化。\n从内存布局角度 #从内存布局看，两者非常相似：\nstd::array\u0026lt;int, 10\u0026gt; arr1; int arr2[10]; 这两种声明在内存中都是连续存储的10个整数，没有额外开销。std::array 内部通常就是封装了一个原生数组。\n但 std::array 实现了更安全的接口：\n.at() 方法提供边界检查 .size() 方法安全获取大小 与STL算法无缝集成 从初始化角度 #两者的初始化方式也有区别：\n// C风格数组 int arr1[10] = {1, 2, 3}; // 剩余元素初始化为0 // std::array std::array\u0026lt;int, 10\u0026gt; arr2 = {1, 2, 3}; // C++11后，剩余元素初始化为0 std::array\u0026lt;int, 10\u0026gt; arr3{}; // 所有元素初始化为0 为什么vector优化时使用std::array而非原生数组 #vector在优化时选择std::array作为基础构建块，而非原生数组，原因在于:\n1. 类型安全与静态接口一致性 #std::array与其他STL容器共享相同的接口约定：\n相同的方法名（.begin(), .end(), .size()等） 相同的迭代器类型 支持标准算法库 原生数组缺乏这些标准接口，在vector内部使用时会导致接口不一致和代码复杂性增加。\n2. 编译期优化机会 #std::array作为编译期已知大小的容器，使编译器可以进行更多优化：\ntemplate \u0026lt;typename T, size_t N\u0026gt; void process(const std::array\u0026lt;T, N\u0026gt;\u0026amp; data) { // 编译器知道N是常量，可能展开循环 for (size_t i = 0; i \u0026lt; N; ++i) { // 处理逻辑 } } 原生数组在模板上下文中使用时，无法保持大小信息，限制了编译期优化机会。\n3. 更好的模板特化机会 #vector实现中可能用不同大小的数组进行特化优化：\ntemplate \u0026lt;typename T\u0026gt; class vector { // 对于小尺寸优化 std::array\u0026lt;T, 16\u0026gt; small_buffer; T* dynamic_buffer; // ... }; 原生数组不能作为值类型返回，在内部函数间传递时会退化为指针，不适合作为vector内部缓冲区的抽象。\n4. 更安全的内存管理 #使用std::array可以减少buffer overflow和内存错误风险，实现更健壮的代码。\n5. 值语义支持 #原生数组不能作为完整对象赋值或返回，这种限制源于C++语言设计中对数组的特殊处理：\n赋值限制：编译器将数组名视为指向第一个元素的地址，而非可赋值对象 返回限制：函数返回机制设计不支持大小不固定的数据结构，且本地数组在函数返回后会失效 std::array通过类封装解决了这些问题，支持完整的值语义，简化了vector内部实现。\n6. 类型特性和SFINAE支持 #原生数组在与现代C++模板元编程技术结合时存在限制，而vector实现通常需要根据元素类型特性进行优化：\ntemplate \u0026lt;typename T, typename = std::enable_if_t\u0026lt;std::is_trivially_copyable_v\u0026lt;T\u0026gt;\u0026gt;\u0026gt; void optimize_copy(/* ... */); array相比vector的优点 #1. 固定内存布局 #std::array没有动态分配，完全在栈上分配（除非数组太大或整个对象在堆上）。这意味着：\n没有内存分配开销 没有内存碎片 缓存友好，数据局部性更好 2. 编译期已知大小 #编译期确定的大小使得编译器可以：\n进行更多常量折叠优化 更有效地进行循环展开 生成更优化的汇编代码 例如，考虑以下操作：\nstd::array\u0026lt;int, 4\u0026gt; a = {1, 2, 3, 4}; int sum = 0; for (int i = 0; i \u0026lt; a.size(); ++i) { sum += a[i]; } 编译器可能优化为：\n; 伪汇编代码 mov eax, 1 add eax, 2 add eax, 3 add eax, 4 ; 直接计算结果，避免循环 3. 无移动/增长开销 #std::array一旦创建，其大小和位置就固定，因此：\n没有重新分配的开销 没有移动元素的成本 迭代器永不失效（除非超出对象生命周期） 4. 减少间接性 #vector内部持有一个指向动态分配内存的指针，这增加了间接性：\n额外的指针解引用 潜在的缓存不命中 更多的指令 5. 边界条件更少 #std::array不需要处理：\n容量增长策略 重新分配 移动构造函数调用 异常安全问题 底层性能分析 #考虑内存访问模式：\nstd::array\u0026lt;int, 1000\u0026gt;：所有元素连续存储在单一内存块中，通常在栈上 std::vector\u0026lt;int\u0026gt;（含1000个元素）：元素在堆上连续存储，但还有3个额外字段（指针、大小、容量）在对象本身中 当访问这些容器时：\narray：直接偏移计算，无间接引用 vector：需要加载指针，然后偏移计算（额外的内存访问） 在热路径上反复执行时，这种差异可能显著影响性能，尤其是在：\n缓存敏感的应用 实时系统 嵌入式环境 SIMD优化 总结来说，std::array相比原生数组提供了类型安全和现代C++接口，没有额外的运行时开销；相比vector则提供了编译期已知大小带来的性能优势和优化机会，适用于大小固定的场景。在性能关键代码中，理解这些容器的底层差异对于做出正确的设计选择至关重要。\n","date":"17 July 2025","permalink":"/blog/2025-07-17-c++origin_array/","section":"Blog","summary":"std::array 与 int a[10] 的区别与优点 #从类型系统角度 #std::array 是一个类模板，而 int a[10] 是一个内建数组类型。这个根本区别导致了它们在C++类型系统中的行为差异。\n类型退化 #传统数组 int a[10] 在作为函数参数传递时会退化(decay)为指针 int*，导致数组大小信息丢失。这种退化是C语言遗留问题，在C++中仍然存在：\nvoid func(int arr[10]) { // 实际上arr的类型是int*，大小信息已丢失 sizeof(arr); // 返回指针大小，而非数组大小 } 而 std::array 是一个完整的对象类型，传递时保留其完整类型信息：\nvoid func(std::array\u0026lt;int, 10\u0026gt;\u0026amp; arr) { sizeof(arr); // 正确返回整个数组大小 } 作为值类型 #std::array 是真正的\u0026quot;值类型\u0026quot;，可以:\n完整复制（不会退化为指针） 用于函数返回值 在STL容器中存储 参与比较操作（支持==, \u0026lt;等运算符） 不可赋值性与返回限制的底层原理 #原生数组存在两个重要限制：\nT buffer1[10]; T buffer2[10]; buffer1 = buffer2; // 编译错误 auto get_buffer() -\u0026gt; ??? { return buffer; // 错误：无法直接返回原生数组 } 1. 不能直接赋值的底层原理：","title":"C++数组类型的底层原理分析"},{"content":"引言 #C++17作为C++标准的重要里程碑，引入了众多革命性的特性，其中constexpr if、std::optional和std::string_view三个特性在性能优化和代码表达力方面具有深远影响。本文将深入解析这三个特性的设计理念、实现机制，以及它们在现代C++开发特别是高性能计算场景中的应用价值。\n1. constexpr if：编译时条件分支的革命 # constexper是C++11引入的\n1.1 基本概念与语法 #constexpr if是C++17引入的编译时条件语句，允许在模板中根据编译时常量表达式有条件地包含或排除代码分支。\ntemplate\u0026lt;typename T\u0026gt; constexpr auto process_data(T data) { if constexpr (std::is_integral_v\u0026lt;T\u0026gt;) { return data * 2; // 只有整数类型才会编译此分支 } else if constexpr (std::is_floating_point_v\u0026lt;T\u0026gt;) { return data * 1.5; // 只有浮点类型才会编译此分支 } else { return data; // 其他类型的默认处理 } } 1.2 与传统SFINAE的对比 #传统SFINAE方式：\n// C++11/14 复杂的SFINAE实现 template\u0026lt;typename T\u0026gt; typename std::enable_if_t\u0026lt;std::is_integral_v\u0026lt;T\u0026gt;, T\u0026gt; process_data(T data) { return data * 2; } template\u0026lt;typename T\u0026gt; typename std::enable_if_t\u0026lt;std::is_floating_point_v\u0026lt;T\u0026gt;, T\u0026gt; process_data(T data) { return data * 1.5; } C++17 constexpr if方式：\ntemplate\u0026lt;typename T\u0026gt; constexpr auto process_data(T data) { if constexpr (std::is_integral_v\u0026lt;T\u0026gt;) { return data * 2; } else if constexpr (std::is_floating_point_v\u0026lt;T\u0026gt;) { return data * 1.5; } } 1.3 与运行时分支预测的区别 #constexpr if vs [[likely]]/[[unlikely]]\n特性 constexpr if [[likely]]/[[unlikely]] 作用时机 编译时 运行时 性能影响 零运行时开销，分支完全消除 减少分支预测失败 代码生成 条件分支在编译时被移除 影响指令布局和预取策略 适用场景 模板特化、类型检查 概率已知的运行时条件 实际应用对比：\n// constexpr if - 编译时优化 template\u0026lt;bool USE_SIMD\u0026gt; constexpr double calculate_moving_average(const std::vector\u0026lt;double\u0026gt;\u0026amp; data) { if constexpr (USE_SIMD) { return simd_moving_average(data); // 编译时选择，零开销 } else { return scalar_moving_average(data); } } // [[likely]] - 运行时优化 double process_market_order(const Order\u0026amp; order) { if (order.is_valid()) [[likely]] { // 99%的订单都有效 return execute_order(order); } else [[unlikely]] { return handle_invalid_order(order); // 很少执行 } } 1.4 高频交易中的应用 #在HFT系统中，constexpr if能够实现零开销的类型分发和算法选择：\ntemplate\u0026lt;typename OrderType\u0026gt; class OrderProcessor { constexpr auto process_order(OrderType order) { if constexpr (std::is_same_v\u0026lt;OrderType, LimitOrder\u0026gt;) { return process_limit_order(order); } else if constexpr (std::is_same_v\u0026lt;OrderType, MarketOrder\u0026gt;) { return process_market_order(order); } else if constexpr (std::is_same_v\u0026lt;OrderType, StopOrder\u0026gt;) { return process_stop_order(order); } } }; 2. std::optional：安全的可能为空值处理 #2.1 基本概念与动机 #std::optional\u0026lt;T\u0026gt;是C++17引入的词汇类型，用于表示\u0026quot;可能包含值也可能为空\u0026quot;的对象，提供了类型安全的替代方案来处理空值情况。\nstd::optional\u0026lt;double\u0026gt; safe_divide(double a, double b) { if (b != 0.0) { return a / b; } return std::nullopt; // 明确表示无有效值 } 2.2 与传统空值处理方式的对比 #传统方式的问题：\n// 使用特殊值表示错误 double unsafe_divide(double a, double b) { if (b == 0.0) { return -1.0; // 特殊值，容易被误用 } return a / b; } // 使用指针表示可能为空 double* pointer_divide(double a, double b) { static double result; if (b == 0.0) { return nullptr; // 需要手动检查空指针 } result = a / b; return \u0026amp;result; } std::optional的优势：\nstd::optional\u0026lt;double\u0026gt; safe_divide(double a, double b) { if (b != 0.0) { return a / b; } return std::nullopt; } // 使用时的类型安全 auto result = safe_divide(10.0, 2.0); if (result) { std::cout \u0026lt;\u0026lt; \u0026#34;Result: \u0026#34; \u0026lt;\u0026lt; *result \u0026lt;\u0026lt; std::endl; } else { std::cout \u0026lt;\u0026lt; \u0026#34;Division by zero!\u0026#34; \u0026lt;\u0026lt; std::endl; } 2.3 与异常处理的性能对比 #异常处理方式：\ndouble exception_divide(double a, double b) { if (b == 0.0) { throw std::runtime_error(\u0026#34;Division by zero\u0026#34;); } return a / b; } // 性能问题：异常处理涉及栈展开，在热路径中代价昂贵 std::optional方式：\nstd::optional\u0026lt;double\u0026gt; optional_divide(double a, double b) { if (b == 0.0) { return std::nullopt; // 无异常开销，适合高频调用 } return a / b; } 2.4 在金融交易系统中的应用 #价格查询与风险控制：\nclass TradingEngine { std::optional\u0026lt;Price\u0026gt; get_best_bid(const Symbol\u0026amp; symbol) { auto it = order_books_.find(symbol); if (it != order_books_.end() \u0026amp;\u0026amp; !it-\u0026gt;second.bids.empty()) { return it-\u0026gt;second.bids.top().price; } return std::nullopt; } std::optional\u0026lt;OrderResult\u0026gt; execute_order(const Order\u0026amp; order) { // 链式的安全检查 if (auto best_ask = get_best_ask(order.symbol)) { if (order.price \u0026gt;= *best_ask) { if (auto risk_result = risk_check(order)) { return execute_validated_order(order); } } } return std::nullopt; } private: std::optional\u0026lt;RiskResult\u0026gt; risk_check(const Order\u0026amp; order) { if (auto position = get_position(order.symbol)) { if (std::abs(*position + order.quantity) \u0026gt; position_limit_) { return std::nullopt; // 超过仓位限制 } } return RiskResult{RiskStatus::APPROVED}; } }; 3. std::string_view：零拷贝字符串处理 #3.1 基本概念与设计理念 #std::string_view是C++17引入的轻量级字符串视图类型，提供对字符序列的只读访问，而无需拥有底层数据。\nvoid process_symbol(std::string_view symbol) { // 无需拷贝字符串数据，直接引用原始内存 if (symbol.starts_with(\u0026#34;AAPL\u0026#34;)) { // 处理苹果股票相关逻辑 } } 3.2 与const std::string\u0026amp;的详细对比 #内存和性能特征对比：\n特性 const std::string\u0026amp; std::string_view 内存分配 可能需要临时对象 零拷贝，无内存分配 类型接受性 仅接受std::string 接受多种字符串类型 子字符串操作 分配新内存 返回新视图，无分配 比较操作 可能涉及内存拷贝 直接内存比较 生命周期管理 自动管理 需要确保底层数据有效 实际性能对比示例：\n// 使用const std::string\u0026amp;的FIX消息解析 class FIXParserWithRef { void parse_message(const std::string\u0026amp; msg) { // 需要创建子字符串 - 内存分配 auto symbol = msg.substr(msg.find(\u0026#34;55=\u0026#34;) + 3, 4); // 内存分配 auto price = msg.substr(msg.find(\u0026#34;44=\u0026#34;) + 3, 8); // 内存分配 process_fields(symbol, price); } }; // 使用std::string_view的FIX消息解析 class FIXParserWithView { void parse_message(std::string_view msg) { // 零拷贝解析 auto symbol = msg.substr(msg.find(\u0026#34;55=\u0026#34;) + 3, 4); // 无内存分配 auto price = msg.substr(msg.find(\u0026#34;44=\u0026#34;) + 3, 8); // 无内存分配 process_fields(symbol, price); } private: void process_fields(std::string_view symbol, std::string_view price) { // 处理视图 - 无额外开销 } }; 3.3 类型通用性优势 #接受多种字符串类型：\nvoid process_instrument(std::string_view instrument) { // 统一处理接口 if (instrument.length() \u0026gt; 6) { // 处理复杂合约 } } // 调用方式的灵活性 process_instrument(\u0026#34;EURUSD\u0026#34;); // 字符串字面量 process_instrument(std::string{\u0026#34;EURUSD\u0026#34;}); // std::string process_instrument(char_array); // char数组 process_instrument(network_buffer.data()); // 网络缓冲区 3.4 在高频交易中的应用 #零拷贝市场数据处理：\nclass MarketDataProcessor { void process_market_data_line(std::string_view line) { // 直接在原始缓冲区上解析 size_t pos = 0; auto timestamp = extract_field(line, pos, \u0026#39;,\u0026#39;); auto symbol = extract_field(line, pos, \u0026#39;,\u0026#39;); auto bid_price = extract_field(line, pos, \u0026#39;,\u0026#39;); auto ask_price = extract_field(line, pos, \u0026#39;,\u0026#39;); // 直接比较和处理，无内存分配 if (symbol.starts_with(\u0026#34;EUR\u0026#34;)) { update_fx_quote(timestamp, symbol, bid_price, ask_price); } } private: std::string_view extract_field(std::string_view str, size_t\u0026amp; pos, char delim) { size_t start = pos; pos = str.find(delim, pos); if (pos == std::string_view::npos) { pos = str.length(); } return str.substr(start, pos++ - start); } }; 4. 三个特性的综合应用 #4.1 协同工作的威力 #这三个特性可以完美协同工作，在高性能系统中发挥巨大作用：\ntemplate\u0026lt;typename OrderType\u0026gt; class UnifiedOrderProcessor { std::optional\u0026lt;ExecutionResult\u0026gt; process_order(OrderType order, std::string_view client_id) { // constexpr if: 编译时类型分发 if constexpr (std::is_same_v\u0026lt;OrderType, LimitOrder\u0026gt;) { return process_limit_order(order, client_id); } else if constexpr (std::is_same_v\u0026lt;OrderType, MarketOrder\u0026gt;) { return process_market_order(order, client_id); } else if constexpr (std::is_same_v\u0026lt;OrderType, StopLossOrder\u0026gt;) { return process_stop_loss_order(order, client_id); } else { static_assert(always_false_v\u0026lt;OrderType\u0026gt;, \u0026#34;Unsupported order type\u0026#34;); } } private: std::optional\u0026lt;ExecutionResult\u0026gt; process_limit_order( const LimitOrder\u0026amp; order, std::string_view client_id) { // string_view: 零拷贝客户端验证 if (!is_authorized_client(client_id)) { return std::nullopt; } // optional: 安全的价格检查 if (auto market_price = get_market_price(order.symbol)) { if (order.price \u0026gt;= *market_price) { return ExecutionResult{order.id, *market_price, ExecutionStatus::FILLED}; } } return std::nullopt; } bool is_authorized_client(std::string_view client_id) { // 直接在授权列表中查找，无字符串拷贝 return authorized_clients_.contains(client_id); } std::optional\u0026lt;Price\u0026gt; get_market_price(std::string_view symbol) { // 组合使用string_view和optional if (auto it = price_cache_.find(symbol); it != price_cache_.end()) { return it-\u0026gt;second; } return std::nullopt; } std::unordered_set\u0026lt;std::string_view\u0026gt; authorized_clients_; std::unordered_map\u0026lt;std::string_view, Price\u0026gt; price_cache_; }; 4.2 性能优化的层次结构 # 编译时优化（constexpr if）：消除运行时分支，实现零开销抽象 内存优化（std::string_view）：避免不必要的内存分配和拷贝 错误处理优化（std::optional）：类型安全的空值处理，避免异常开销 5. 最佳实践与注意事项 #5.1 constexpr if最佳实践 #// ✅ 正确：用于类型特化 template\u0026lt;typename T\u0026gt; constexpr auto serialize(const T\u0026amp; obj) { if constexpr (std::is_arithmetic_v\u0026lt;T\u0026gt;) { return serialize_arithmetic(obj); } else if constexpr (has_serialize_method_v\u0026lt;T\u0026gt;) { return obj.serialize(); } else { return serialize_generic(obj); } } // ❌ 错误：不要用于运行时条件 template\u0026lt;typename T\u0026gt; void process(T value, bool use_fast_path) { if constexpr (use_fast_path) { // 编译错误：use_fast_path不是常量表达式 // ... } } 5.2 std::optional最佳实践 #// ✅ 正确：明确的空值语义 std::optional\u0026lt;User\u0026gt; find_user(std::string_view username) { if (auto it = users_.find(username); it != users_.end()) { return it-\u0026gt;second; } return std::nullopt; } // ❌ 错误：避免嵌套optional std::optional\u0026lt;std::optional\u0026lt;int\u0026gt;\u0026gt; nested_optional() { // 不推荐 return std::optional\u0026lt;int\u0026gt;{42}; } 5.3 std::string_view最佳实践 #// ✅ 正确：确保底层数据生命周期 class MessageProcessor { void process_message(std::string_view msg) { // 立即处理或拷贝，不要存储view parse_and_execute(msg); } }; // ❌ 错误：存储string_view可能导致悬挂引用 class BadMessageProcessor { std::string_view stored_msg_; // 危险：可能悬挂 void set_message(std::string_view msg) { stored_msg_ = msg; // 如果msg的底层数据被销毁，这里就悬挂了 } }; 6. 结论 #C++17的constexpr if、std::optional和std::string_view三个特性代表了现代C++在性能优化和类型安全方面的重要进步。它们分别在编译时优化、安全的空值处理和零拷贝字符串操作方面提供了强大的工具。\n在高性能计算场景，特别是金融交易系统中，这些特性能够：\n显著减少运行时开销：通过编译时条件分支和零拷贝操作 提高代码安全性：通过类型安全的空值处理避免常见错误 增强代码表达力：使意图更加明确，减少样板代码 掌握这些特性的正确使用方法，对于编写高性能、类型安全的现代C++代码至关重要。随着C++标准的不断发展，这些特性将继续成为高质量C++代码的基础构建块。\n","date":"17 July 2025","permalink":"/blog/2025-07-17-c++17_new_feature/","section":"Blog","summary":"引言 #C++17作为C++标准的重要里程碑，引入了众多革命性的特性，其中constexpr if、std::optional和std::string_view三个特性在性能优化和代码表达力方面具有深远影响。本文将深入解析这三个特性的设计理念、实现机制，以及它们在现代C++开发特别是高性能计算场景中的应用价值。\n1. constexpr if：编译时条件分支的革命 # constexper是C++11引入的\n1.1 基本概念与语法 #constexpr if是C++17引入的编译时条件语句，允许在模板中根据编译时常量表达式有条件地包含或排除代码分支。\ntemplate\u0026lt;typename T\u0026gt; constexpr auto process_data(T data) { if constexpr (std::is_integral_v\u0026lt;T\u0026gt;) { return data * 2; // 只有整数类型才会编译此分支 } else if constexpr (std::is_floating_point_v\u0026lt;T\u0026gt;) { return data * 1.5; // 只有浮点类型才会编译此分支 } else { return data; // 其他类型的默认处理 } } 1.2 与传统SFINAE的对比 #传统SFINAE方式：\n// C++11/14 复杂的SFINAE实现 template\u0026lt;typename T\u0026gt; typename std::enable_if_t\u0026lt;std::is_integral_v\u0026lt;T\u0026gt;, T\u0026gt; process_data(T data) { return data * 2; } template\u0026lt;typename T\u0026gt; typename std::enable_if_t\u0026lt;std::is_floating_point_v\u0026lt;T\u0026gt;, T\u0026gt; process_data(T data) { return data * 1.","title":"C++17核心特性深度解析：constexpr if、std::optional与std::string_view"},{"content":"基本概念 #map（有序映射） # 定义：基于键值对的有序关联容器 头文件：#include \u0026lt;map\u0026gt; 特点：元素按键值自动排序存储 unordered_map（无序映射） # 定义：基于键值对的无序关联容器 头文件：#include \u0026lt;unordered_map\u0026gt; 特点：元素无序存储，通过哈希表实现快速访问 底层实现原理 #map的底层实现：红黑树(BST + 自平衡) #数据结构 #template\u0026lt;typename Key, typename Value\u0026gt; struct MapNode { std::pair\u0026lt;Key, Value\u0026gt; data; // 键值对 MapNode* left; // 左子节点 MapNode* right; // 右子节点 MapNode* parent; // 父节点 bool color; // 红色(true) 或 黑色(false) }; 红黑树特性 # 每个节点要么是红色，要么是黑色 根节点是黑色 所有叶子节点（NIL）是黑色 红色节点的两个子节点都是黑色（不能有连续的红色节点） 从任意节点到其每个叶子的所有简单路径都包含相同数目的黑色节点 平衡机制与时间复杂度分析 #红黑树通过旋转和重新着色维持平衡，这是其时间复杂度为O(log n)的根本原因：\n为什么是O(log n)？\n树高度控制：红黑树的特性保证了树的高度不会超过2*log₂(n+1) 路径长度限制：最长路径不超过最短路径的2倍 操作路径：查找、插入、删除都沿着从根到叶的路径进行 // 查找操作的时间复杂度分析 Node* find(const Key\u0026amp; key) { Node* current = root; int steps = 0; // 统计步数 while (current != nullptr) { steps++; // 每次比较计为一步 if (key == current-\u0026gt;data.first) { // 最多需要树高度次比较，即O(log n) return current; } else if (key \u0026lt; current-\u0026gt;data.first) { current = current-\u0026gt;left; } else { current = current-\u0026gt;right; } } // 总步数 ≤ 树高度 ≤ 2*log₂(n+1) = O(log n) return nullptr; } // 插入操作的时间复杂度分析 void insert(const Key\u0026amp; key, const Value\u0026amp; value) { // 1. 找到插入位置：O(log n) Node* current = root; Node* parent = nullptr; while (current != nullptr) { parent = current; if (key \u0026lt; current-\u0026gt;data.first) { current = current-\u0026gt;left; } else if (key \u0026gt; current-\u0026gt;data.first) { current = current-\u0026gt;right; } else { current-\u0026gt;data.second = value; // 更新值 return; } } // 2. 插入新节点：O(1) Node* new_node = new Node{key, value, nullptr, nullptr, parent, RED}; // 3. 修复红黑树性质：最多O(log n)次旋转 fix_insert_violation(new_node); } 红黑树旋转的必要性： 旋转操作是红黑树维持平衡的核心机制，没有旋转就无法保证O(log n)的时间复杂度。当插入或删除节点破坏红黑树性质时，需要通过旋转重新平衡：\n// 左旋转：当右子树过重时使用 // x y // / \\ / \\ // α y --\u0026gt; x γ // / \\ / \\ // β γ α β void left_rotate(Node* x) { Node* y = x-\u0026gt;right; x-\u0026gt;right = y-\u0026gt;left; if (y-\u0026gt;left != nullptr) { y-\u0026gt;left-\u0026gt;parent = x; } y-\u0026gt;parent = x-\u0026gt;parent; if (x-\u0026gt;parent == nullptr) { root = y; } else if (x == x-\u0026gt;parent-\u0026gt;left) { x-\u0026gt;parent-\u0026gt;left = y; } else { x-\u0026gt;parent-\u0026gt;right = y; } y-\u0026gt;left = x; x-\u0026gt;parent = y; } // 为什么需要旋转？ // 1. 保持树的平衡性，防止退化为链表 // 2. 维护红黑树的5个性质 // 3. 确保任何操作的时间复杂度都是O(log n) unordered_map的底层实现：哈希表 #数据结构 #template\u0026lt;typename Key, typename Value\u0026gt; class UnorderedMap { private: struct Node { std::pair\u0026lt;Key, Value\u0026gt; data; // 键值对 Node* next; // 指向下一个节点（链表） }; std::vector\u0026lt;Node*\u0026gt; buckets; // 桶数组 size_t bucket_count; // 桶的数量 size_t size; // 元素数量 double max_load_factor; // 最大负载因子 size_t hash_function(const Key\u0026amp; key) { return std::hash\u0026lt;Key\u0026gt;{}(key) % bucket_count; } }; 哈希冲突处理：链地址法 #// 插入操作 void insert(const Key\u0026amp; key, const Value\u0026amp; value) { size_t index = hash_function(key); Node* current = buckets[index]; // 检查key是否已存在 while (current != nullptr) { if (current-\u0026gt;data.first == key) { current-\u0026gt;data.second = value; // 更新值 return; } current = current-\u0026gt;next; } // 创建新节点 Node* new_node = new Node{{key, value}, buckets[index]}; buckets[index] = new_node; ++size; // 检查是否需要扩容 if (load_factor() \u0026gt; max_load_factor) { rehash(); } } // 查找操作 Value* find(const Key\u0026amp; key) { size_t index = hash_function(key); Node* current = buckets[index]; while (current != nullptr) { if (current-\u0026gt;data.first == key) { return \u0026amp;current-\u0026gt;data.second; } current = current-\u0026gt;next; } return nullptr; } 哈希表时间复杂度分析 #为什么平均情况是O(1)？\n// 理想情况下的查找过程 Value* find(const Key\u0026amp; key) { // 步骤1：计算哈希值 - O(1) size_t hash_value = std::hash\u0026lt;Key\u0026gt;{}(key); // 步骤2：计算桶索引 - O(1) size_t index = hash_value % bucket_count; // 步骤3：访问桶 - O(1) Node* current = buckets[index]; // 步骤4：在桶中查找 - 平均O(1) // 假设负载因子α = n/m，每个桶平均有α个元素 // 如果α是常数（比如≤1），则查找时间为O(1) while (current != nullptr) { if (current-\u0026gt;data.first == key) { return \u0026amp;current-\u0026gt;data.second; } current = current-\u0026gt;next; // 平均只需要很少次循环 } return nullptr; } 为什么最坏情况是O(n)？\n// 最坏情况：所有元素都哈希到同一个桶 // 此时哈希表退化为链表 void worst_case_demo() { // 假设有糟糕的哈希函数 auto bad_hash = [](int x) { return 0; }; // 总是返回0 // 所有元素都在bucket[0]中： // bucket[0]: elem1 -\u0026gt; elem2 -\u0026gt; elem3 -\u0026gt; ... -\u0026gt; elemN -\u0026gt; null // 查找最后一个元素需要O(n)时间 } 负载因子对性能的数学分析：\n// 期望查找长度 = 1 + α/2 （成功查找） // 期望查找长度 = α （失败查找） // 其中 α = n/m （负载因子） void load_factor_analysis() { // 当α ≤ 0.75时，期望查找长度 ≈ 1.375 // 当α ≤ 1.0时，期望查找长度 ≈ 1.5 // 当α \u0026gt; 2.0时，性能开始明显下降 std::unordered_map\u0026lt;int, int\u0026gt; map; map.max_load_factor(0.75); // 控制性能 // 当负载因子超过0.75时自动扩容，保持O(1)性能 } 动态扩容机制与性能保障 #void rehash() { size_t old_bucket_count = bucket_count; std::vector\u0026lt;Node*\u0026gt; old_buckets = std::move(buckets); bucket_count *= 2; // 扩容为原来的2倍 buckets.assign(bucket_count, nullptr); size = 0; // 重新插入所有元素 - 这个过程是O(n) // 但是摊销分析后，插入操作仍然是平均O(1) for (size_t i = 0; i \u0026lt; old_bucket_count; ++i) { Node* current = old_buckets[i]; while (current != nullptr) { Node* next = current-\u0026gt;next; insert(current-\u0026gt;data.first, current-\u0026gt;data.second); delete current; current = next; } } } // 摊销分析：为什么插入仍然是O(1)？ // 假设从空表开始，插入n个元素： // - 大部分插入操作是O(1) // - 只有在扩容时才是O(n)，但扩容频率很低 // - 总成本：n * O(1) + log(n) * O(n) = O(n) // - 平均每次插入：O(n)/n = O(1) 特性对比 #基本特性 # 特性 map unordered_map 有序性 有序（按键值排序） 无序（按哈希值分布） 底层实现 红黑树 哈希表 查找时间复杂度 O(log n) 平均O(1)，最坏O(n) 插入时间复杂度 O(log n) 平均O(1)，最坏O(n) 删除时间复杂度 O(log n) 平均O(1)，最坏O(n) 空间复杂度 O(n) O(n) 内存占用 较小 较大（需要额外的桶数组） 性能特性 #map的性能特点 # 稳定的O(log n)性能：无论数据分布如何，性能都很稳定 内存效率高：只需要存储树结构，没有额外开销 缓存友好性一般：树结构可能导致缓存未命中 unordered_map的性能特点 # 理想情况下性能优异：O(1)的查找、插入、删除 性能波动大：受哈希函数质量和负载因子影响 内存开销大：需要维护桶数组，空间利用率相对较低 功能特性 #map独有功能 #std::map\u0026lt;int, std::string\u0026gt; m; m[1] = \u0026#34;one\u0026#34;; m[2] = \u0026#34;two\u0026#34;; m[3] = \u0026#34;three\u0026#34;; // 范围查询 auto lower = m.lower_bound(2); // 返回第一个不小于2的元素 auto upper = m.upper_bound(2); // 返回第一个大于2的元素 auto range = m.equal_range(2); // 返回等于2的元素范围 // 有序遍历 for (auto it = m.begin(); it != m.end(); ++it) { std::cout \u0026lt;\u0026lt; it-\u0026gt;first \u0026lt;\u0026lt; \u0026#34;: \u0026#34; \u0026lt;\u0026lt; it-\u0026gt;second \u0026lt;\u0026lt; std::endl; } // 输出: 1: one, 2: two, 3: three（按键值有序） unordered_map独有功能 #std::unordered_map\u0026lt;int, std::string\u0026gt; um; um[1] = \u0026#34;one\u0026#34;; um[2] = \u0026#34;two\u0026#34;; um[3] = \u0026#34;three\u0026#34;; // 哈希表状态信息 std::cout \u0026lt;\u0026lt; \u0026#34;Bucket count: \u0026#34; \u0026lt;\u0026lt; um.bucket_count() \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34;Load factor: \u0026#34; \u0026lt;\u0026lt; um.load_factor() \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34;Max load factor: \u0026#34; \u0026lt;\u0026lt; um.max_load_factor() \u0026lt;\u0026lt; std::endl; // 设置负载因子 um.max_load_factor(0.75); // 手动扩容 um.rehash(100); 适用场景 #map适用场景 #1. 需要有序性的场景 #// 学生成绩管理（按学号排序） std::map\u0026lt;int, double\u0026gt; student_grades; student_grades[20210001] = 95.5; student_grades[20210002] = 88.0; student_grades[20210003] = 92.3; // 自动按学号排序 for (const auto\u0026amp; pair : student_grades) { std::cout \u0026lt;\u0026lt; \u0026#34;学号: \u0026#34; \u0026lt;\u0026lt; pair.first \u0026lt;\u0026lt; \u0026#34;, 成绩: \u0026#34; \u0026lt;\u0026lt; pair.second \u0026lt;\u0026lt; std::endl; } Note: 如果数据相对静态且查找频繁，std::vector\u0026lt;std::pair\u0026lt;Key, Value\u0026gt;\u0026gt;排序后使用二分查找可能更快（O(log n)但缓存友好）。如果学号范围连续，直接数组索引std::vector\u0026lt;double\u0026gt;是最优选择（O(1)）。\nHFT Note: 在纳秒级延迟要求下，预分配的固定大小数组配合完美哈希或预计算索引表是唯一选择。避免任何动态内存分配和指针跳转。\n2. 需要范围查询的场景 #// 时间段查询 std::map\u0026lt;std::time_t, std::string\u0026gt; events; // ... 添加事件 // 查询某个时间段内的所有事件 auto start_time = /* 开始时间 */; auto end_time = /* 结束时间 */; auto lower = events.lower_bound(start_time); auto upper = events.upper_bound(end_time); for (auto it = lower; it != upper; ++it) { std::cout \u0026lt;\u0026lt; \u0026#34;事件: \u0026#34; \u0026lt;\u0026lt; it-\u0026gt;second \u0026lt;\u0026lt; std::endl; } Note: 对于范围查询，map是合理选择。但如果数据规模巨大且查询频繁，B+树或线段树可能更优。对于时间序列数据，std::vector按时间排序后使用std::lower_bound/upper_bound通常更快。\nHFT Note: 时间序列数据必须使用预分配的循环缓冲区，配合SIMD指令进行并行搜索。考虑硬件时间戳计数器（RDTSC）和lock-free数据结构。\n3. 内存敏感的场景 #// 嵌入式系统或内存受限环境 std::map\u0026lt;int, int\u0026gt; memory_efficient_map; // 相比unordered_map占用更少内存 Note: 在极度内存敏感的场景下，考虑压缩数据结构或自定义位操作。如果key范围已知且连续，std::vector直接索引是内存和性能的最优选择。\nHFT Note: 必须使用预分配的内存池（memory pools）和栈上分配。禁用所有动态内存分配器，使用自定义allocator。考虑CPU缓存行对齐（64字节）和NUMA感知内存分配。\n4. 需要稳定性能的场景 #// 实时系统，需要可预测的性能 std::map\u0026lt;std::string, int\u0026gt; real_time_map; // 保证O(log n)的稳定性能，不会突然变慢 Note: 对于硬实时系统，预分配的std::vector或std::array通常是最优选择，避免动态内存分配。如果必须动态查找，考虑完美哈希或预先排序的数组。\nHFT Note: 硬实时交易系统需要确定性延迟。使用FPGA硬件加速、kernel bypass（如DPDK）、CPU核心绑定、禁用超线程、实时内核补丁。所有操作必须是wait-free的。\nunordered_map适用场景 #1. 频繁查找的场景 #// 单词频率统计 std::unordered_map\u0026lt;std::string, int\u0026gt; word_count; std::string word; while (std::cin \u0026gt;\u0026gt; word) { word_count[word]++; // O(1)平均时间复杂度 } Note: 如果单词集合相对固定且已知，使用字典树（Trie）可能更高效。对于大量重复查找，预排序的std::vector配合二分查找通常比哈希表更快（更好的缓存局部性）。\nHFT Note: 符号查找必须使用编译时哈希（如gperf生成的完美哈希表）或固定大小的符号枚举数组。运行时哈希计算在纳秒级交易中完全不可接受。\n2. 缓存系统 #// LRU缓存实现 class LRUCache { private: std::unordered_map\u0026lt;int, std::list\u0026lt;std::pair\u0026lt;int, int\u0026gt;\u0026gt;::iterator\u0026gt; cache; std::list\u0026lt;std::pair\u0026lt;int, int\u0026gt;\u0026gt; usage_order; int capacity; public: int get(int key) { auto it = cache.find(key); // O(1)查找 if (it != cache.end()) { // 移到最前面 usage_order.splice(usage_order.begin(), usage_order, it-\u0026gt;second); return it-\u0026gt;second-\u0026gt;second; } return -1; } }; Note: 对于小容量缓存（\u0026lt;100元素），使用std::vector的线性查找可能更快（无哈希开销，更好的缓存局部性）。对于大容量缓存，考虑使用循环数组实现以减少指针跳转。\nHFT Note: 价格/订单簿缓存必须使用lock-free循环缓冲区，配合原子操作。考虑使用专用的硬件缓存（如Intel CAT）和L1缓存预取指令。所有数据结构必须预分配且cache-aligned。\n3. 大数据量的快速访问：为什么选择unordered_map？ #数学原理分析：\n// 性能对比：100万数据的查找操作 const int N = 1000000; // map的查找时间：O(log N) = O(log 1000000) ≈ O(20) // 每次查找需要约20次比较 // unordered_map的查找时间：O(1) // 理想情况下每次查找只需要1次哈希计算 + 1次比较 内存访问模式分析：\n// map的内存访问模式（可能跳跃访问） void map_access_pattern() { std::map\u0026lt;int, int\u0026gt; large_map; // 红黑树的节点可能分散在内存中 // 查找路径：root -\u0026gt; left -\u0026gt; right -\u0026gt; left... // 每次访问可能导致缓存未命中 auto start = std::chrono::high_resolution_clock::now(); for (int i = 0; i \u0026lt; 100000; ++i) { large_map.find(rand() % N); // 树遍历，缓存不友好 } auto end = std::chrono::high_resolution_clock::now(); } // unordered_map的内存访问模式（相对集中） void unordered_map_access_pattern() { std::unordered_map\u0026lt;int, int\u0026gt; large_map; // 哈希表的桶数组连续存储 // 大部分访问在桶数组范围内 auto start = std::chrono::high_resolution_clock::now(); for (int i = 0; i \u0026lt; 100000; ++i) { large_map.find(rand() % N); // 直接索引，缓存友好 } auto end = std::chrono::high_resolution_clock::now(); } 实际性能差异：\n// 大数据量下的性能测试 void big_data_performance_test() { const int DATA_SIZE = 10000000; // 1000万数据 const int QUERY_COUNT = 1000000; // 100万次查询 std::map\u0026lt;int, int\u0026gt; big_map; std::unordered_map\u0026lt;int, int\u0026gt; big_unordered_map; // 插入数据 for (int i = 0; i \u0026lt; DATA_SIZE; ++i) { big_map[i] = i; big_unordered_map[i] = i; } // 随机查询测试 std::vector\u0026lt;int\u0026gt; random_keys(QUERY_COUNT); for (int i = 0; i \u0026lt; QUERY_COUNT; ++i) { random_keys[i] = rand() % DATA_SIZE; } // map查询时间 auto start = std::chrono::high_resolution_clock::now(); for (int key : random_keys) { big_map.find(key); // 每次约需要log(10^7) ≈ 23次比较 } auto map_time = std::chrono::high_resolution_clock::now() - start; // unordered_map查询时间 start = std::chrono::high_resolution_clock::now(); for (int key : random_keys) { big_unordered_map.find(key); // 每次约需要1-2次操作 } auto unordered_map_time = std::chrono::high_resolution_clock::now() - start; // 结果：unordered_map通常快5-10倍 std::cout \u0026lt;\u0026lt; \u0026#34;数据量: \u0026#34; \u0026lt;\u0026lt; DATA_SIZE \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34;查询次数: \u0026#34; \u0026lt;\u0026lt; QUERY_COUNT \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34;map查询时间: \u0026#34; \u0026lt;\u0026lt; std::chrono::duration_cast\u0026lt;std::chrono::milliseconds\u0026gt;(map_time).count() \u0026lt;\u0026lt; \u0026#34;ms\u0026#34; \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34;unordered_map查询时间: \u0026#34; \u0026lt;\u0026lt; std::chrono::duration_cast\u0026lt;std::chrono::milliseconds\u0026gt;(unordered_map_time).count() \u0026lt;\u0026lt; \u0026#34;ms\u0026#34; \u0026lt;\u0026lt; std::endl; } 为什么大数据量更适合unordered_map？\n时间复杂度优势放大：数据量越大，O(1)相对于O(log n)的优势越明显 减少比较次数：1000万数据时，map需要约23次比较，unordered_map只需要1次 批量操作效率：大量查找操作时，累积的时间差异非常显著 Note: 对于超大数据量，如果数据相对静态，考虑使用排序后的std::vector配合并行二分查找，或者使用内存映射文件。对于整数key且范围已知，直接数组索引仍然是最快的选择。\nHFT Note: 大规模市场数据必须分层存储：热数据使用L1缓存友好的紧凑数组，温数据使用预取优化的向量化查找，冷数据使用FPGA协处理器。考虑使用Intel AVX-512指令集进行SIMD并行查找。\n4. 不需要有序性的键值存储 #// 配置文件解析 std::unordered_map\u0026lt;std::string, std::string\u0026gt; config; config[\u0026#34;database_host\u0026#34;] = \u0026#34;localhost\u0026#34;; config[\u0026#34;database_port\u0026#34;] = \u0026#34;3306\u0026#34;; config[\u0026#34;max_connections\u0026#34;] = \u0026#34;100\u0026#34;; // 快速获取配置值 std::string get_config(const std::string\u0026amp; key) { auto it = config.find(key); return (it != config.end()) ? it-\u0026gt;second : \u0026#34;\u0026#34;; } Note: 对于配置文件这种小规模、相对静态的数据，std::vector\u0026lt;std::pair\u0026lt;std::string, std::string\u0026gt;\u0026gt;可能更高效。如果配置项有限且已知，使用enum映射到数组索引是最快的方案。\nHFT Note: 交易参数和配置必须在编译时确定（constexpr），或使用switch-case语句优化的枚举值。运行时字符串比较和哈希计算会引入不可接受的延迟抖动。\n选择指南 #选择map的情况 # ✅ 需要元素有序存储和遍历 ✅ 需要范围查询功能 ✅ 内存使用量要求严格 ✅ 需要稳定可预测的性能 ✅ 数据量相对较小（几万到几十万） 选择unordered_map的情况 # ✅ 主要操作是查找、插入、删除 ✅ 不需要有序性 ✅ 对查找性能要求很高 ✅ 有足够的内存空间 ✅ 数据量很大（百万级以上） 性能测试示例 ##include \u0026lt;chrono\u0026gt; #include \u0026lt;map\u0026gt; #include \u0026lt;unordered_map\u0026gt; #include \u0026lt;random\u0026gt; #include \u0026lt;iostream\u0026gt; void performance_comparison() { const int N = 1000000; std::map\u0026lt;int, int\u0026gt; ordered_map; std::unordered_map\u0026lt;int, int\u0026gt; unordered_map; std::random_device rd; std::mt19937 gen(rd()); std::uniform_int_distribution\u0026lt;\u0026gt; dis(1, N); // 插入性能测试 auto start = std::chrono::high_resolution_clock::now(); for (int i = 0; i \u0026lt; N; ++i) { ordered_map[dis(gen)] = i; } auto end = std::chrono::high_resolution_clock::now(); auto map_insert_time = std::chrono::duration_cast\u0026lt;std::chrono::milliseconds\u0026gt;(end - start); start = std::chrono::high_resolution_clock::now(); for (int i = 0; i \u0026lt; N; ++i) { unordered_map[dis(gen)] = i; } end = std::chrono::high_resolution_clock::now(); auto unordered_map_insert_time = std::chrono::duration_cast\u0026lt;std::chrono::milliseconds\u0026gt;(end - start); // 查找性能测试 std::vector\u0026lt;int\u0026gt; keys_to_find(10000); for (int i = 0; i \u0026lt; 10000; ++i) { keys_to_find[i] = dis(gen); } start = std::chrono::high_resolution_clock::now(); for (int key : keys_to_find) { ordered_map.find(key); } end = std::chrono::high_resolution_clock::now(); auto map_find_time = std::chrono::duration_cast\u0026lt;std::chrono::microseconds\u0026gt;(end - start); start = std::chrono::high_resolution_clock::now(); for (int key : keys_to_find) { unordered_map.find(key); } end = std::chrono::high_resolution_clock::now(); auto unordered_map_find_time = std::chrono::duration_cast\u0026lt;std::chrono::microseconds\u0026gt;(end - start); // 输出结果 std::cout \u0026lt;\u0026lt; \u0026#34;性能对比结果 (N=\u0026#34; \u0026lt;\u0026lt; N \u0026lt;\u0026lt; \u0026#34;):\u0026#34; \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34;插入时间:\u0026#34; \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34; map: \u0026#34; \u0026lt;\u0026lt; map_insert_time.count() \u0026lt;\u0026lt; \u0026#34;ms\u0026#34; \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34; unordered_map: \u0026#34; \u0026lt;\u0026lt; unordered_map_insert_time.count() \u0026lt;\u0026lt; \u0026#34;ms\u0026#34; \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34;查找时间 (10000次):\u0026#34; \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34; map: \u0026#34; \u0026lt;\u0026lt; map_find_time.count() \u0026lt;\u0026lt; \u0026#34;μs\u0026#34; \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34; unordered_map: \u0026#34; \u0026lt;\u0026lt; unordered_map_find_time.count() \u0026lt;\u0026lt; \u0026#34;μs\u0026#34; \u0026lt;\u0026lt; std::endl; } 总结 #map和unordered_map各有优势，选择时需要根据具体需求权衡：\nmap：稳定、有序、内存效率高，适合需要有序性和稳定性能的场景 unordered_map：快速、灵活、适合大数据量，适合追求极致性能且不需要有序性的场景 在实际开发中，如果不确定选择哪个，可以先使用unordered_map（因为大多数情况下查找性能更重要），如果后续发现需要有序性或者遇到性能问题，再考虑切换到map。\n","date":"17 July 2025","permalink":"/blog/2025-07-17-map_unordered_map/","section":"Blog","summary":"基本概念 #map（有序映射） # 定义：基于键值对的有序关联容器 头文件：#include \u0026lt;map\u0026gt; 特点：元素按键值自动排序存储 unordered_map（无序映射） # 定义：基于键值对的无序关联容器 头文件：#include \u0026lt;unordered_map\u0026gt; 特点：元素无序存储，通过哈希表实现快速访问 底层实现原理 #map的底层实现：红黑树(BST + 自平衡) #数据结构 #template\u0026lt;typename Key, typename Value\u0026gt; struct MapNode { std::pair\u0026lt;Key, Value\u0026gt; data; // 键值对 MapNode* left; // 左子节点 MapNode* right; // 右子节点 MapNode* parent; // 父节点 bool color; // 红色(true) 或 黑色(false) }; 红黑树特性 # 每个节点要么是红色，要么是黑色 根节点是黑色 所有叶子节点（NIL）是黑色 红色节点的两个子节点都是黑色（不能有连续的红色节点） 从任意节点到其每个叶子的所有简单路径都包含相同数目的黑色节点 平衡机制与时间复杂度分析 #红黑树通过旋转和重新着色维持平衡，这是其时间复杂度为O(log n)的根本原因：\n为什么是O(log n)？\n树高度控制：红黑树的特性保证了树的高度不会超过2*log₂(n+1) 路径长度限制：最长路径不超过最短路径的2倍 操作路径：查找、插入、删除都沿着从根到叶的路径进行 // 查找操作的时间复杂度分析 Node* find(const Key\u0026amp; key) { Node* current = root; int steps = 0; // 统计步数 while (current !","title":"C++ map与unordered_map详解"},{"content":"摘要 #在高频交易（HFT）系统中，当市场数据突发性爆增时，如何在保证超低延迟的前提下防止系统过载是一个关键技术挑战。本文深入分析了背压机制的原理、常见实现方式，并针对HFT系统的特殊需求，设计了一套基于数据优先级分层的混合背压策略。通过理论分析证明，该方案在保证关键数据零丢失的同时，能够有效应对trade数据的burst场景。\n1. 背景与问题定义 #1.1 HFT系统的数据特征 #高频交易系统通常需要处理三类核心市场数据：\nBBO（Best Bid Offer）数据：实时更新，对策略决策至关重要，频率约1000-10000次/秒 Orderbook数据：通常100ms更新一次，提供市场深度信息，数据量中等 Trade数据：实时更新，频率极高且具有突发性（burst）特征，正常情况下1000-5000次/秒，burst时可达50000+次/秒 1.2 Burst问题的本质 #在某些市场事件（如重大新闻发布、大单成交）触发下，trade数据可能在毫秒级时间窗口内激增至正常流量的10-100倍。这种突发性负载会导致：\n内存溢出：缓冲区被大量trade数据填满 延迟恶化：处理延迟从微秒级恶化到毫秒级 数据丢失：关键的BBO和orderbook更新被遗漏 系统崩溃：极端情况下导致OOM或死锁 2. 背压机制理论基础 #2.1 背压的定义与数学模型 #背压（Backpressure）是一种流控制机制，当系统下游处理能力不足时，向上游传递\u0026quot;减缓输入\u0026quot;的信号，从而维持系统稳定性。\n设系统输入速率为λ（events/second），处理速率为μ，缓冲区大小为B：\n稳定条件：λ ≤ μ 缓冲区利用率：ρ = λ/μ 背压触发阈值：当缓冲区占用率 \u0026gt; θ（通常θ = 0.8）时启动 当λ \u0026gt; μ时，缓冲区积压量呈线性增长：\n积压量(t) = (λ - μ) × t + 初始积压 背压机制的目标是通过动态调整有效输入速率λ\u0026rsquo;，使得λ\u0026rsquo; ≤ μ，从而保证系统稳定性。\n2.2 背压的质量评估指标 # 延迟保障：P99延迟 \u0026lt; 目标阈值 吞吐保持：关键数据处理率 ≥ 99% 系统稳定性：内存使用率 \u0026lt; 安全阈值 数据完整性：重要数据丢失率 \u0026lt; 0.01% 3. 常见背压机制分析 #3.1 阻塞式背压（Blocking Backpressure） #原理：当缓冲区满时，阻塞生产者直到有空间可用。\n优点：\n实现简单，逻辑清晰 保证数据不丢失 提供天然的流控制 缺点：\n引入不可预测的阻塞延迟（可达毫秒级） 可能导致死锁 不适合硬实时系统 适用场景：适用于延迟容忍度较高的批处理系统，不适合HFT。\n3.2 丢弃式背压（Drop Backpressure） #原理：当系统过载时，直接丢弃新到达的数据。\n性能特征：\n延迟：O(1)，通常 \u0026lt; 100ns 吞吐：受限于处理器能力 丢失率：在burst期间可能 \u0026gt; 50% 优点：\n零阻塞，延迟可预测 实现简单高效 系统永不崩溃 缺点：\n数据丢失无法避免 没有智能选择机制 可能丢失重要数据 3.3 采样式背压（Sampling Backpressure） #原理：在系统压力下，按照预定规则只接受部分数据。\n采样策略：\n均匀采样：每隔N个数据接受1个 时间窗口采样：在时间窗口内限制接受数量 概率采样：基于概率决定是否接受 优点：\n保持数据的统计特性 延迟可控 资源使用可预测 缺点：\n可能错过重要事件 需要合理设计采样策略 统计偏差风险 3.4 优先级背压（Priority Backpressure） #原理：根据数据重要性分配不同的处理优先级和丢弃策略。\n数据优先级分类：\nCRITICAL：永不丢弃（如重要BBO更新） HIGH：低丢弃率 \u0026lt; 1%（如大额交易） MEDIUM：中等丢弃率 \u0026lt; 10%（如中等交易） LOW：高丢弃率 \u0026lt; 50%（如小额交易） 优点：\n保护重要数据 灵活的策略配置 适应性强 缺点：\n实现复杂度高 需要准确的优先级分类 可能引入额外延迟 4. HFT系统的特殊需求分析 #4.1 延迟要求 #HFT系统对延迟极度敏感：\n端到端延迟：\u0026lt; 10μs (P99) 抖动要求：\u0026lt; 1μs (P99 - P50) 处理延迟：\u0026lt; 100ns per operation 4.2 数据价值层次 #在HFT中，不同数据具有不同的业务价值：\nMISSION_CRITICAL：BBO变化、大额交易（业务价值 = 1000） HIGH_VALUE：中等交易、深度变化（业务价值 = 500） INFORMATIONAL：小额交易、历史数据（业务价值 = 100） NOISE：极小交易、重复数据（业务价值 = 10） 4.3 系统资源约束 # CPU缓存：L1缓存命中率 \u0026gt; 95% 内存带宽：避免跨NUMA节点访问 网络：专用高速网络，带宽充足但延迟敏感 4.4 可靠性要求 # 数据完整性：关键数据丢失率 \u0026lt; 0.001% 系统可用性：99.99% uptime 故障恢复：\u0026lt; 1ms recovery time 5. HFT优化背压机制设计 #5.1 整体架构设计 #基于HFT系统的特殊需求，我们设计了一套三层混合背压机制：\n第一层：数据分类器\n实时评估数据重要性和紧急程度 基于交易量、价格影响、市场状态等多维度分类 分类延迟 \u0026lt; 50ns 第二层：分层缓冲策略\nBBO数据：零丢弃缓冲区，采用wait-free算法 Orderbook数据：可压缩缓冲区，时间窗口内合并更新 Trade数据：自适应采样缓冲区，动态调整采样率 第三层：系统监控与反馈\n实时监控系统负载（CPU、内存、网络延迟） 动态调整背压参数 提供性能指标和告警 5.2 数据分类器设计原理 #多维度分类标准：\n交易量维度：\n大单：\u0026gt; 100,000 USD（高优先级） 中单：10,000 - 100,000 USD（中优先级） 小单：\u0026lt; 10,000 USD（低优先级） 价格影响维度：\n显著偏离：|价格 - 中位价| / 中位价 \u0026gt; 0.1%（高优先级） 轻微偏离：0.01% - 0.1%（中优先级） 正常范围：\u0026lt; 0.01%（正常优先级） 时间敏感性维度：\nBBO更新：立即处理（最高优先级） 深度变化：100μs内处理（高优先级） 历史交易：1ms内处理（低优先级） 分类算法：\n优先级分数 = 交易量权重 × 交易量分数 + 价格影响权重 × 价格影响分数 + 时间敏感性权重 × 时间敏感性分数 其中权重配置：交易量(0.4) + 价格影响(0.4) + 时间敏感性(0.2) = 1.0 5.3 零丢弃缓冲区设计（BBO专用） #核心原理：\n采用Lock-Free Ring Buffer，避免锁竞争 使用Memory Barrier保证数据一致性 缓冲区大小设置为2的幂，便于位运算优化 支持紧急写入模式，允许覆盖最老未读数据 关键特性：\n写入延迟：\u0026lt; 50ns (P99) 读取延迟：\u0026lt; 30ns (P99) 容量：16K entries，支持1秒的BBO数据积压 内存对齐：64字节对齐，避免false sharing 5.4 自适应采样缓冲区设计（Trade专用） #核心思想： 根据系统实时负载和数据特征，动态调整采样策略。\n采样率调整算法：\n当前负载率 = 当前处理队列长度 / 最大队列容量 if 负载率 \u0026gt; 0.9: 采样率 = 5% (只保留最重要的大单) elif 负载率 \u0026gt; 0.7: 采样率 = 20% (保留大单和部分中单) elif 负载率 \u0026gt; 0.5: 采样率 = 50% (正常采样) else: 采样率 = 100% (全部保留) 多级缓冲策略：\n大单缓冲区：100K entries，优先级最高 中单缓冲区：50K entries，中等优先级 采样缓冲区：20K entries，存储采样后的小单 5.5 系统负载监控与反馈 #监控指标：\nCPU使用率：基于RDTSC计算，更新间隔100μs 内存使用率：监控堆内存和缓冲区占用 缓存命中率：通过硬件性能计数器获取 网络延迟：基于时间戳测量端到端延迟 反馈控制算法：\n系统负载评分 = CPU权重 × CPU使用率 + 内存权重 × 内存使用率 + 延迟权重 × 归一化延迟 if 系统负载评分 \u0026gt; 0.9: 启用严格背压模式 elif 系统负载评分 \u0026gt; 0.7: 启用中等背压模式 else: 启用宽松背压模式 6. 性能分析与优化效果 #6.1 理论性能分析 #延迟分析：\n数据分类：50ns 缓冲区写入：50ns 系统监控：10ns（摊销成本） 总延迟：\u0026lt; 150ns (P99) 吞吐分析：\nBBO处理能力：\u0026gt; 20M ops/sec Trade处理能力：\u0026gt; 10M ops/sec（采样后） 内存带宽利用率：\u0026lt; 60% 6.2 背压效果预期 #正常场景（Trade \u0026lt; 5K/sec）：\n数据丢失率：\u0026lt; 0.1% 平均延迟：\u0026lt; 100ns 系统资源利用率：\u0026lt; 50% 中等压力场景（Trade 5K-20K/sec）：\n数据丢失率：\u0026lt; 5%（主要是小单） 平均延迟：\u0026lt; 200ns 重要数据保留率：\u0026gt; 99% 高压力场景（Trade \u0026gt; 50K/sec）：\n数据丢失率：\u0026lt; 30%（主要是小单和部分中单） 平均延迟：\u0026lt; 500ns 关键数据保留率：\u0026gt; 99.9% 6.3 与传统方案对比 # 指标 传统丢弃式 传统采样式 优化混合式 BBO数据丢失率 10-20% 5-10% \u0026lt; 0.01% 大单数据丢失率 20-30% 10-15% \u0026lt; 1% 平均延迟 200ns 300ns 150ns P99延迟 2μs 5μs 800ns 资源使用率 70% 60% 50% 7. 工程实现要点 #7.1 内存管理优化 # 预分配策略：启动时预分配所有缓冲区，避免运行时内存分配 NUMA感知：将相关数据结构绑定到同一NUMA节点 大页内存：使用2MB大页减少TLB未命中 内存对齐：关键数据结构按照缓存行（64字节）对齐 7.2 CPU亲和性设置 # IO线程：绑定到专用CPU核心，避免上下文切换 处理线程：绑定到高性能核心，关闭超线程 监控线程：绑定到独立核心，不影响关键路径 7.3 编译器优化 # 分支预测优化：使用__builtin_expect指导编译器 内联函数：关键路径函数强制内联 循环展开：手动展开小循环提升性能 向量化：利用SIMD指令加速批量操作 8. 总结与展望 #8.1 核心贡献 #本文提出的HFT优化背压机制具有以下特点：\n数据价值驱动：基于业务价值而非技术指标进行背压决策 延迟优先：在保证关键数据完整性的前提下，最小化处理延迟 自适应调节：根据系统实时负载动态调整背压策略 工程实用：考虑了实际部署中的各种工程约束 8.2 适用范围 #该方案特别适用于以下场景：\n高频交易系统的市场数据处理 实时风控系统的事件流处理 低延迟分析系统的数据摄入 其他对延迟极度敏感的流式处理系统 8.3 未来发展方向 # 机器学习优化：利用ML算法预测市场数据burst，提前调整背压策略 硬件加速：结合FPGA/GPU实现更低延迟的数据分类和背压控制 分布式扩展：支持多节点环境下的协同背压控制 自动调优：基于历史性能数据自动优化背压参数 通过合理设计背压机制，HFT系统能够在面对极端市场情况时保持稳定运行，确保关键交易决策不受数据burst的影响，这对于维护市场稳定性和交易公平性具有重要意义。\n","date":"15 July 2025","permalink":"/blog/2025-07-15-backpress/","section":"Blog","summary":"摘要 #在高频交易（HFT）系统中，当市场数据突发性爆增时，如何在保证超低延迟的前提下防止系统过载是一个关键技术挑战。本文深入分析了背压机制的原理、常见实现方式，并针对HFT系统的特殊需求，设计了一套基于数据优先级分层的混合背压策略。通过理论分析证明，该方案在保证关键数据零丢失的同时，能够有效应对trade数据的burst场景。\n1. 背景与问题定义 #1.1 HFT系统的数据特征 #高频交易系统通常需要处理三类核心市场数据：\nBBO（Best Bid Offer）数据：实时更新，对策略决策至关重要，频率约1000-10000次/秒 Orderbook数据：通常100ms更新一次，提供市场深度信息，数据量中等 Trade数据：实时更新，频率极高且具有突发性（burst）特征，正常情况下1000-5000次/秒，burst时可达50000+次/秒 1.2 Burst问题的本质 #在某些市场事件（如重大新闻发布、大单成交）触发下，trade数据可能在毫秒级时间窗口内激增至正常流量的10-100倍。这种突发性负载会导致：\n内存溢出：缓冲区被大量trade数据填满 延迟恶化：处理延迟从微秒级恶化到毫秒级 数据丢失：关键的BBO和orderbook更新被遗漏 系统崩溃：极端情况下导致OOM或死锁 2. 背压机制理论基础 #2.1 背压的定义与数学模型 #背压（Backpressure）是一种流控制机制，当系统下游处理能力不足时，向上游传递\u0026quot;减缓输入\u0026quot;的信号，从而维持系统稳定性。\n设系统输入速率为λ（events/second），处理速率为μ，缓冲区大小为B：\n稳定条件：λ ≤ μ 缓冲区利用率：ρ = λ/μ 背压触发阈值：当缓冲区占用率 \u0026gt; θ（通常θ = 0.8）时启动 当λ \u0026gt; μ时，缓冲区积压量呈线性增长：\n积压量(t) = (λ - μ) × t + 初始积压 背压机制的目标是通过动态调整有效输入速率λ\u0026rsquo;，使得λ\u0026rsquo; ≤ μ，从而保证系统稳定性。\n2.2 背压的质量评估指标 # 延迟保障：P99延迟 \u0026lt; 目标阈值 吞吐保持：关键数据处理率 ≥ 99% 系统稳定性：内存使用率 \u0026lt; 安全阈值 数据完整性：重要数据丢失率 \u0026lt; 0.01% 3. 常见背压机制分析 #3.1 阻塞式背压（Blocking Backpressure） #原理：当缓冲区满时，阻塞生产者直到有空间可用。","title":"高频交易系统中的背压机制设计讨论"},{"content":"在现代C++网络编程中，Boost.Asio（或standalone asio）是使用最广泛的异步I/O库之一。然而，关于Asio的I/O模型本质，特别是在Linux平台上的实现机制，存在很多误解。本文将深入剖析Asio在Linux平台的真实面目。\n本质定位 #Asio的真实身份 #Asio在Linux平台上的本质：同步非阻塞I/O + I/O多路复用 + 回调机制的高级封装\n这意味着：\n✅ 不是传统的阻塞I/O：提供了异步编程接口 ❌ 不是真正的异步I/O：底层仍使用同步系统调用 ✅ 是异步编程框架：通过回调机制模拟异步编程体验 核心理解 #// Asio给你的印象（异步风格API） socket.async_read_some(buffer(data), [](error_code ec, size_t bytes) { // 看起来像异步回调 process_data(data, bytes); }); // 但Linux下的实际执行（简化） epoll_wait(epfd, events, 128, -1); // 同步等待事件 ssize_t n = read(fd, buffer, size); // 同步读取 callback(n); // 调用用户回调 关键洞察：Asio提供了异步的编程体验，但不是异步的执行机制。\n工作原理 #事件驱动的执行模型 #Asio在Linux上实现了Reactor模式，而不是Proactor模式：\n// Reactor模式的典型流程 class AsioReactor { public: void async_read(socket\u0026amp; s, buffer b, handler h) { // 1. 注册读取意图 register_read_intent(s.fd(), b, h); // 2. 添加到epoll监控 epoll_ctl(epfd_, EPOLL_CTL_ADD, s.fd(), \u0026amp;event); } void run() { while (true) { // 3. 等待I/O事件 int n = epoll_wait(epfd_, events_, max_events_, -1); // 4. 处理就绪的事件 for (int i = 0; i \u0026lt; n; i++) { handle_ready_event(events_[i]); } } } private: void handle_ready_event(const epoll_event\u0026amp; ev) { auto* op = get_operation(ev.data.ptr); // 5. 执行实际的同步I/O ssize_t result = read(op-\u0026gt;fd, op-\u0026gt;buffer, op-\u0026gt;size); // 6. 调用用户回调 if (result \u0026gt; 0 || errno != EAGAIN) { op-\u0026gt;handler(result); } } }; 异步API的分解过程 #当你调用async_read_some时，Asio内部执行以下步骤：\n// 用户代码 socket_.async_read_some(boost::asio::buffer(data_), [this](boost::system::error_code ec, size_t length) { if (!ec) { process_data(data_, length); start_next_read(); } }); // Asio内部分解为： void async_read_some_impl(int fd, char* buf, size_t size, handler_t h) { // Step 1: 设置非阻塞模式 fcntl(fd, F_SETFL, O_NONBLOCK); // Step 2: 尝试立即读取 ssize_t immediate_result = read(fd, buf, size); if (immediate_result \u0026gt; 0) { // 数据已就绪，直接回调 post_completion(h, immediate_result); return; } if (errno != EAGAIN) { // 发生错误 post_completion(h, immediate_result); return; } // Step 3: 数据未就绪，注册到epoll auto* op = new read_operation{fd, buf, size, h}; epoll_event ev; ev.events = EPOLLIN; ev.data.ptr = op; epoll_ctl(epfd_, EPOLL_CTL_ADD, fd, \u0026amp;ev); } 底层实现机制 #系统调用层面的真相 #使用strace跟踪一个Asio TCP服务器：\n# 编译Asio程序 g++ -o asio_server server.cpp -lboost_system -pthread # 跟踪关键系统调用 strace -e epoll_create1,epoll_ctl,epoll_wait,read,write,accept4 ./asio_server 观察到的系统调用序列：\nepoll_create1(EPOLL_CLOEXEC) = 3 bind(4, {sa_family=AF_INET, sin_port=htons(8080)}, 16) = 0 listen(4, 128) = 0 epoll_ctl(3, EPOLL_CTL_ADD, 4, {EPOLLIN, {u32=4, u64=4}}) = 0 # 等待连接 epoll_wait(3, [{EPOLLIN, {u32=4, u64=4}}], 128, -1) = 1 accept4(4, {sa_family=AF_INET, sin_port=htons(52341)}, [16], SOCK_CLOEXEC) = 5 # 等待数据 epoll_ctl(3, EPOLL_CTL_ADD, 5, {EPOLLIN, {u32=5, u64=5}}) = 0 epoll_wait(3, [{EPOLLIN, {u32=5, u64=5}}], 128, -1) = 1 read(5, \u0026#34;Hello World\\n\u0026#34;, 1024) = 12 # 同步read！ # 响应数据 epoll_ctl(3, EPOLL_CTL_MOD, 5, {EPOLLOUT, {u32=5, u64=5}}) = 0 epoll_wait(3, [{EPOLLOUT, {u32=5, u64=5}}], 128, -1) = 1 write(5, \u0026#34;Echo: Hello World\\n\u0026#34;, 18) = 18 # 同步write！ 关键发现：\n使用epoll_*系列系统调用进行事件监控 仍然有传统的read/write系统调用 没有异步I/O特有的系统调用（如io_uring_enter） 内核交互模式 #// Asio的内核交互模式（简化） class AsioLinuxService { int epfd_; std::queue\u0026lt;completion_handler\u0026gt; ready_handlers_; public: void run_one() { // 1. 处理已完成的操作 if (!ready_handlers_.empty()) { auto handler = ready_handlers_.front(); ready_handlers_.pop(); handler(); return; } // 2. 等待新的I/O事件 epoll_event events[128]; int n = epoll_wait(epfd_, events, 128, 0); // 非阻塞检查 if (n == 0) { // 没有就绪事件，阻塞等待 n = epoll_wait(epfd_, events, 128, -1); } // 3. 处理就绪事件 for (int i = 0; i \u0026lt; n; i++) { process_ready_operation(events[i]); } } private: void process_ready_operation(const epoll_event\u0026amp; ev) { auto* op = static_cast\u0026lt;async_operation*\u0026gt;(ev.data.ptr); // 执行同步I/O操作 op-\u0026gt;perform(); // 内部调用read/write等同步系统调用 // 将完成的操作加入就绪队列 ready_handlers_.push([op]() { op-\u0026gt;complete(); }); } }; Asio的设计哲学 #1. 统一的异步编程模型 #Asio的核心设计目标是提供统一的异步编程接口，隐藏底层I/O模型的差异：\n// 统一的异步接口，不同平台不同实现 template\u0026lt;typename MutableBufferSequence, typename ReadHandler\u0026gt; void async_read_some( const MutableBufferSequence\u0026amp; buffers, ReadHandler\u0026amp;\u0026amp; handler) { #if defined(BOOST_ASIO_HAS_IOCP) // Windows: 使用IOCP（真异步） win_iocp_socket_service::async_receive(buffers, handler); #elif defined(BOOST_ASIO_HAS_EPOLL) // Linux: 使用epoll（同步非阻塞） linux_epoll_reactor::async_read(buffers, handler); #elif defined(BOOST_ASIO_HAS_KQUEUE) // BSD: 使用kqueue bsd_kqueue_reactor::async_read(buffers, handler); #endif } 2. Proactor模式的模拟 #Asio在Linux上模拟了Proactor模式，让用户感觉像在使用异步I/O：\n// 用户看到的Proactor风格API class Connection { public: void start() { do_read(); // 启动异步读取链 } private: void do_read() { // 提交读取请求 socket_.async_read_some(boost::asio::buffer(data_), [this](boost::system::error_code ec, size_t length) { if (!ec) { process_message(data_, length); do_read(); // 继续读取 } else { handle_error(ec); } }); } void do_write(const std::string\u0026amp; message) { // 提交写入请求 boost::asio::async_write(socket_, boost::asio::buffer(message), [this](boost::system::error_code ec, size_t length) { if (!ec) { // 写入完成 } else { handle_error(ec); } }); } }; 设计优势：\n简化编程复杂度：用户无需直接操作epoll 回调链管理：自动管理异步操作的生命周期 错误处理统一：统一的错误码和异常处理 3. 性能与易用性的平衡 #// Asio的性能优化策略 class optimized_async_operation { public: void initiate() { // 优化1：立即尝试执行 if (try_immediate_completion()) { return; // 避免不必要的epoll操作 } // 优化2：批量操作 if (should_batch()) { add_to_batch(); return; } // 优化3：注册到反应器 reactor_.register_operation(this); } private: bool try_immediate_completion() { // 尝试立即完成操作，避免epoll开销 ssize_t result = read(fd_, buffer_, size_); if (result \u0026gt; 0) { complete_immediately(result); return true; } return (errno != EAGAIN); } }; 真正的异步I/O对比 #Linux io_uring：真正的异步执行 #// 真正的异步I/O使用io_uring class TrueAsyncIO { struct io_uring ring_; public: void async_read(int fd, char* buffer, size_t size, callback_t cb) { // 1. 获取提交队列项 struct io_uring_sqe *sqe = io_uring_get_sqe(\u0026amp;ring_); // 2. 准备读取操作 io_uring_prep_read(sqe, fd, buffer, size, 0); sqe-\u0026gt;user_data = reinterpret_cast\u0026lt;uintptr_t\u0026gt;(cb); // 3. 提交到内核，立即返回 io_uring_submit(\u0026amp;ring_); // 此时内核开始异步执行I/O操作 // 应用程序可以立即做其他事情 } void process_completions() { struct io_uring_cqe *cqe; // 4. 检查完成的操作 while (io_uring_peek_cqe(\u0026amp;ring_, \u0026amp;cqe) == 0) { auto* callback = reinterpret_cast\u0026lt;callback_t*\u0026gt;(cqe-\u0026gt;user_data); // 5. 调用完成回调 (*callback)(cqe-\u0026gt;res); // cqe-\u0026gt;res是已完成的字节数 io_uring_cqe_seen(\u0026amp;ring_, cqe); } } }; 系统调用对比 #Asio (epoll)的系统调用：\nepoll_wait(3, events, 128, -1) = 1 # 等待事件 read(5, buffer, 1024) = 256 # 同步读取 epoll_ctl(3, EPOLL_CTL_MOD, 5, ...) # 修改监控事件 io_uring的系统调用：\nio_uring_enter(3, 1, 1, IORING_ENTER_GETEVENTS) = 1 # 提交并等待完成 # 没有read/write系统调用！数据拷贝由内核异步完成 性能影响分析 #Asio在Linux上的性能开销：\n系统调用开销：每次I/O仍需要read/write系统调用 用户态/内核态切换：epoll_wait + read/write = 多次切换 数据拷贝开销：用户态参与数据拷贝过程 回调调度开销：额外的函数调用和对象管理 io_uring的性能优势：\n批量操作：一次系统调用提交多个I/O操作 零拷贝：内核直接完成数据拷贝 减少上下文切换：用户态和内核态交互更少 真正并行：多个I/O操作可以真正并发执行 性能基准测试 #简单的性能对比 #// 测试场景：1000个并发连接，每个连接读取1KB数据 // Asio (epoll) 结果： // QPS: ~50,000 // CPU使用率: 15% // 系统调用次数: ~150,000/秒 // io_uring 结果： // QPS: ~80,000 // CPU使用率: 8% // 系统调用次数: ~50,000/秒 适用场景分析 #Asio适用的场景 #推荐使用Asio的情况：\n跨平台需求：需要在Windows/Linux/macOS上运行 开发效率优先：团队熟悉回调式异步编程 中等并发量：连接数在10K以下 业务逻辑复杂：需要复杂的异步操作编排 考虑io_uring的场景 #推荐使用io_uring的情况：\n极致性能要求：高频交易、实时系统 高并发场景：连接数超过50K 文件I/O密集：大量的磁盘读写操作 现代Linux环境：Linux 5.1+的环境 总结 #Asio的本质认知 #在Linux平台上，Asio是一个基于epoll的高级同步非阻塞I/O框架：\nI/O模型：同步非阻塞I/O + I/O多路复用 编程模型：异步回调风格API 执行模型：事件驱动的Reactor模式 性能特征：优于传统阻塞I/O，但不及真正的异步I/O 技术选型建议 # 需求场景 推荐方案 理由 跨平台网络服务 Asio 统一API，成熟稳定 Linux高性能服务 io_uring 真异步，性能最优 简单客户端应用 同步I/O + 线程池 实现简单，够用 现代C++项目 协程 + io_uring 语法现代，性能优秀 关键要点 # Asio不是异步I/O，而是异步编程框架 底层仍是同步操作，只是通过回调模拟异步体验 性能瓶颈在于系统调用开销和用户态参与 选择依据应基于具体需求而非技术标签 理解这些本质差异，有助于我们在实际项目中做出更明智的技术选择，既不盲目追求新技术，也不固守过时方案。技术的价值在于解决实际问题，而不是炫耀复杂度。\n","date":"9 July 2025","permalink":"/blog/2025-07-09-asio/","section":"Blog","summary":"在现代C++网络编程中，Boost.Asio（或standalone asio）是使用最广泛的异步I/O库之一。然而，关于Asio的I/O模型本质，特别是在Linux平台上的实现机制，存在很多误解。本文将深入剖析Asio在Linux平台的真实面目。\n本质定位 #Asio的真实身份 #Asio在Linux平台上的本质：同步非阻塞I/O + I/O多路复用 + 回调机制的高级封装\n这意味着：\n✅ 不是传统的阻塞I/O：提供了异步编程接口 ❌ 不是真正的异步I/O：底层仍使用同步系统调用 ✅ 是异步编程框架：通过回调机制模拟异步编程体验 核心理解 #// Asio给你的印象（异步风格API） socket.async_read_some(buffer(data), [](error_code ec, size_t bytes) { // 看起来像异步回调 process_data(data, bytes); }); // 但Linux下的实际执行（简化） epoll_wait(epfd, events, 128, -1); // 同步等待事件 ssize_t n = read(fd, buffer, size); // 同步读取 callback(n); // 调用用户回调 关键洞察：Asio提供了异步的编程体验，但不是异步的执行机制。\n工作原理 #事件驱动的执行模型 #Asio在Linux上实现了Reactor模式，而不是Proactor模式：\n// Reactor模式的典型流程 class AsioReactor { public: void async_read(socket\u0026amp; s, buffer b, handler h) { // 1. 注册读取意图 register_read_intent(s.","title":"深度解析：Asio在Linux平台的I/O模型本质"},{"content":"在高性能网络编程中，I/O模型的选择往往决定了系统的并发能力和性能表现。然而，关于I/O多路复用、同步I/O、异步I/O等概念，许多开发者存在理解上的误区。本文将深入剖析这些概念的本质区别，澄清常见的混淆点。\n常见的概念混淆 #误区一：I/O多路复用就是异步I/O #这是最常见的误解。实际上，I/O多路复用本质上仍然是同步I/O。准确地说，多路复用的事件检测阶段是阻塞的（epoll_wait会阻塞），而实际的I/O读写阶段仍是同步操作。它只是提供了\u0026quot;等待多个同步I/O事件 + 显式事件通知\u0026quot;的机制，而不是真正的异步执行。\n误区二：非阻塞I/O就是异步I/O #非阻塞I/O（NIO）虽然不会让线程阻塞等待，但仍然是同步I/O，因为应用程序需要主动调用系统调用并立即处理返回结果（包括\u0026quot;暂时无数据\u0026quot;的情况）。\n系统层面的I/O模型分类 #1. 同步阻塞I/O (BIO) #特点：应用程序发起I/O调用后，线程阻塞等待操作完成。\nint sockfd = socket(AF_INET, SOCK_STREAM, 0); char buffer[1024]; // 线程会阻塞在这里，直到有数据到达 ssize_t n = read(sockfd, buffer, sizeof(buffer)); if (n \u0026gt; 0) { process_data(buffer, n); } 应用场景：\n连接数较少的服务 对实时性要求不高的应用 通常配合多线程使用 2. 同步非阻塞I/O (NIO) #特点：应用程序发起I/O调用后立即返回，需要主动检查操作状态。\nint sockfd = socket(AF_INET, SOCK_STREAM, 0); // 设置为非阻塞模式 int flags = fcntl(sockfd, F_GETFL, 0); fcntl(sockfd, F_SETFL, flags | O_NONBLOCK); char buffer[1024]; ssize_t n = read(sockfd, buffer, sizeof(buffer)); if (n \u0026gt; 0) { // 成功读取数据 process_data(buffer, n); } else if (n == -1 \u0026amp;\u0026amp; errno == EAGAIN) { // 暂时无数据，需要稍后重试 // 这是关键：应用程序需要处理\u0026#34;未完成\u0026#34;状态 } else { // 发生错误 handle_error(); } 关键理解：read()调用立即返回，但返回的可能是\u0026quot;操作状态\u0026quot;而不是\u0026quot;完成的数据\u0026quot;。\n3. I/O多路复用 #特点：使用select、poll、epoll等机制监控多个文件描述符，当某个fd可读/可写时通知应用程序。\nint epfd = epoll_create1(0); struct epoll_event events[MAX_EVENTS]; // 监控多个文件描述符 while (true) { // 这里会阻塞等待事件 int nfds = epoll_wait(epfd, events, MAX_EVENTS, -1); for (int i = 0; i \u0026lt; nfds; i++) { if (events[i].events \u0026amp; EPOLLIN) { char buffer[1024]; // 实际的I/O操作仍然是同步的 ssize_t n = read(events[i].data.fd, buffer, sizeof(buffer)); if (n \u0026gt; 0) { process_data(buffer, n); } } } } 本质：I/O多路复用 = 事件通知机制 + 同步I/O操作\n4. 异步I/O (AIO) #特点：应用程序提交I/O请求后立即返回，内核异步完成操作并主动通知结果。\n// Linux io_uring 示例 struct io_uring ring; io_uring_queue_init(256, \u0026amp;ring, 0); char buffer[1024]; struct io_uring_sqe *sqe = io_uring_get_sqe(\u0026amp;ring); io_uring_prep_read(sqe, sockfd, buffer, sizeof(buffer), 0); io_uring_submit(\u0026amp;ring); // 提交请求，立即返回 // 应用程序可以去做其他事情 do_other_work(); // 稍后检查完成状态 struct io_uring_cqe *cqe; io_uring_wait_cqe(\u0026amp;ring, \u0026amp;cqe); // 到这里，数据一定已经读取完成 if (cqe-\u0026gt;res \u0026gt; 0) { process_data(buffer, cqe-\u0026gt;res); } 同步I/O vs 异步I/O：核心区别 #数据可用性保证 #同步I/O（包括非阻塞）：\nssize_t result = read(fd, buffer, size); // 返回值可能的含义： // \u0026gt; 0 : 成功读取数据（数据立即可用） // = 0 : EOF // \u0026lt; 0 : 错误或EAGAIN（数据不可用，需要重试） 异步I/O：\nio_uring_wait_cqe(\u0026amp;ring, \u0026amp;cqe); int result = cqe-\u0026gt;res; // 返回值的含义： // \u0026gt; 0 : 数据已经读取完成（100%可用） // = 0 : EOF（操作完成） // \u0026lt; 0 : 操作完成但发生错误 // 绝对不会有\u0026#34;数据未准备好\u0026#34;的情况！ 操作时间线对比 #同步非阻塞I/O：\nT1: 应用程序调用read() T2: 内核检查数据是否可用 T3: 如果可用 → 拷贝数据，返回数据 如果不可用 → 立即返回EAGAIN T4: 应用程序处理返回结果 如果是EAGAIN → 稍后重试 如果是数据 → 处理数据 异步I/O：\nT1: 应用程序提交read请求 T2: 立即返回，应用程序去做其他事 ... TN: 内核在后台等待数据、拷贝数据 TN+1: 内核通知：操作完成 TN+2: 应用程序处理完成的数据 生活化类比 #同步非阻塞I/O = 查看邮箱\n你：走到邮箱前查看（read调用） 邮箱：要么有信（返回数据），要么没信（返回EAGAIN） 你：如果没信，稍后再来查看（需要重试） 异步I/O = 邮件通知服务\n你：告诉邮局\u0026#34;有信就通知我\u0026#34;（提交async请求） 你：去做其他事情 邮局：有信时主动通知你（completion notification） 你：收到通知时，信件已经在你手里了 为什么I/O多路复用要配合非阻塞I/O？ #I/O多路复用只能告诉你\u0026quot;可以进行I/O操作\u0026quot;，但不能保证操作一定不会阻塞。\n潜在的阻塞风险 #// 危险的做法：多路复用 + 阻塞I/O int nfds = epoll_wait(epfd, events, MAX_EVENTS, -1); for (int i = 0; i \u0026lt; nfds; i++) { if (events[i].events \u0026amp; EPOLLIN) { // 即使epoll说可读，这里仍可能阻塞！ ssize_t n = read(events[i].data.fd, buffer, sizeof(buffer)); } } 可能导致阻塞的场景：\n数据被其他进程读取 TCP窗口变化导致数据暂时不可读 信号中断等异常情况 正确的组合方式 #// 正确的做法：多路复用 + 非阻塞I/O int sockfd = socket(AF_INET, SOCK_STREAM, 0); fcntl(sockfd, F_SETFL, O_NONBLOCK); // 设置非阻塞 int nfds = epoll_wait(epfd, events, MAX_EVENTS, -1); for (int i = 0; i \u0026lt; nfds; i++) { if (events[i].events \u0026amp; EPOLLIN) { char buffer[1024]; ssize_t n = read(events[i].data.fd, buffer, sizeof(buffer)); if (n \u0026gt; 0) { process_data(buffer, n); } else if (n == 0) { close_connection(events[i].data.fd); } else { if (errno == EAGAIN || errno == EWOULDBLOCK) { // 暂时无数据，正常情况 continue; } else { handle_error(); } } } } 实际应用场景选择 #高并发网络服务器 #推荐：I/O多路复用 + 非阻塞I/O\n单线程处理大量连接 避免线程上下文切换开销 代表：Nginx、Redis 文件密集型应用 #推荐：异步I/O\n并发处理大量文件读写 最大化磁盘I/O吞吐量 代表：现代数据库系统 简单的客户端应用 #可选：同步阻塞I/O + 多线程\n实现简单，逻辑清晰 连接数不多的场景 性能特点对比 # I/O模型 并发能力 实现复杂度 CPU利用率 内存开销 避免用户态阻塞 适用场景 BIO + 多线程 中等 低 中等 高 否 连接数较少的服务 NIO + 多路复用 高 中等 低 低 是（配合非阻塞） 高并发网络服务 AIO (io_uring) 最高 高 最优 最低 是 极高性能、文件密集型 说明：\n避免用户态阻塞：指是否能保证用户线程不会在I/O操作上意外阻塞 CPU利用率：主要考虑系统调用开销和上下文切换成本 内存开销：主要考虑线程栈空间和内核数据结构占用 总结 # I/O多路复用是事件通知机制，不是异步I/O 非阻塞I/O仍然是同步I/O，只是不会阻塞线程 异步I/O的本质是I/O操作（如数据拷贝）由内核在后台完成，并通过事件回调或队列通知用户程序结果，无需用户主动轮询。 同步与异步的关键区别在于谁负责数据拷贝和如何通知完成 现代高性能服务器多采用**\u0026ldquo;非阻塞I/O + I/O多路复用\u0026rdquo;**的组合 理解这些概念的本质区别，有助于我们在实际项目中选择合适的I/O模型，构建高性能的网络应用。记住：技术选择没有银弹，只有最适合的场景。\nref #https://xiaolincoding.com/os/8_network_system/reactor.html#reactor\n","date":"9 July 2025","permalink":"/blog/2025-07-09-bio_nio/","section":"Blog","summary":"在高性能网络编程中，I/O模型的选择往往决定了系统的并发能力和性能表现。然而，关于I/O多路复用、同步I/O、异步I/O等概念，许多开发者存在理解上的误区。本文将深入剖析这些概念的本质区别，澄清常见的混淆点。\n常见的概念混淆 #误区一：I/O多路复用就是异步I/O #这是最常见的误解。实际上，I/O多路复用本质上仍然是同步I/O。准确地说，多路复用的事件检测阶段是阻塞的（epoll_wait会阻塞），而实际的I/O读写阶段仍是同步操作。它只是提供了\u0026quot;等待多个同步I/O事件 + 显式事件通知\u0026quot;的机制，而不是真正的异步执行。\n误区二：非阻塞I/O就是异步I/O #非阻塞I/O（NIO）虽然不会让线程阻塞等待，但仍然是同步I/O，因为应用程序需要主动调用系统调用并立即处理返回结果（包括\u0026quot;暂时无数据\u0026quot;的情况）。\n系统层面的I/O模型分类 #1. 同步阻塞I/O (BIO) #特点：应用程序发起I/O调用后，线程阻塞等待操作完成。\nint sockfd = socket(AF_INET, SOCK_STREAM, 0); char buffer[1024]; // 线程会阻塞在这里，直到有数据到达 ssize_t n = read(sockfd, buffer, sizeof(buffer)); if (n \u0026gt; 0) { process_data(buffer, n); } 应用场景：\n连接数较少的服务 对实时性要求不高的应用 通常配合多线程使用 2. 同步非阻塞I/O (NIO) #特点：应用程序发起I/O调用后立即返回，需要主动检查操作状态。\nint sockfd = socket(AF_INET, SOCK_STREAM, 0); // 设置为非阻塞模式 int flags = fcntl(sockfd, F_GETFL, 0); fcntl(sockfd, F_SETFL, flags | O_NONBLOCK); char buffer[1024]; ssize_t n = read(sockfd, buffer, sizeof(buffer)); if (n \u0026gt; 0) { // 成功读取数据 process_data(buffer, n); } else if (n == -1 \u0026amp;\u0026amp; errno == EAGAIN) { // 暂时无数据，需要稍后重试 // 这是关键：应用程序需要处理\u0026#34;未完成\u0026#34;状态 } else { // 发生错误 handle_error(); } 关键理解：read()调用立即返回，但返回的可能是\u0026quot;操作状态\u0026quot;而不是\u0026quot;完成的数据\u0026quot;。","title":"I/O模型深度解析：多路复用、同步与异步的本质区别"},{"content":"问题背景 #在开发基于DPDK的高性能WebSocket客户端时，遇到了典型的内存管理问题。该客户端使用了QuickWS框架，集成F-Stack网络栈和OpenSSL，在连接Binance WebSocket API进行高频数据接收测试时出现段错误。\n技术栈概览 # 网络栈: DPDK + F-Stack WebSocket库: QuickWS (自定义高性能框架) SSL/TLS: OpenSSL 3.x 内存分配器: Flash Allocator (自定义分配器) 缓冲区: Ring Buffer with Flash Allocator 目标: 高吞吐量实时数据接收性能测试 故障现象 #Connected to Binance WebSocket stream! fd: 1 Accepted protocols: , extensions: Thread 1 \u0026#34;binance_client\u0026#34; received signal SIGSEGV, Segmentation fault. 定位过程 #第一阶段：环境问题排查 #初始现象: 程序在DPDK初始化阶段就出现问题\nEAL: Auto-detected process type: SECONDARY EAL: Fail to recv reply for request /var/run/dpdk/rte/mp_socket:bus_vdev_mp 解决方案: 清理DPDK残留资源\nsudo rm -rf /var/run/dpdk/rte/mp_socket* sudo rm -rf /dev/hugepages/* 关键发现: DPDK多进程模式的资源竞争会导致初始化挂起。\n第二阶段：SSL资源管理问题 #故障现象:\nfree(): invalid pointer BIO_free() -\u0026gt; qws::FLoop::~FLoop() 深度分析: 使用GDB检查TLS共享数据结构：\n(gdb) print *tls_shared_data_ptr_ $5 = { cur_sock_ptr = 0x3872657375, // 👈 异常指针值！ buf = {data = 0x0, size = 0, start_pos = 545, capacity = 93824997653040}, shared_rbio = 0x555555a7da80, shared_wbio = 0x7ffff7e89a90, shared_bio_meth = 0x7ffff7e86000 } 关键发现: cur_sock_ptr = 0x3872657375 转换为ASCII是 \u0026ldquo;8resu\u0026rdquo;，说明内存已被覆写！\n根本原因: FLoop对象的重复构造导致SSL BIO资源状态异常\nBinanceContext ctx{}; // 第一次构造FLoop ctx.loop = qws::FLoop{}; // 👈 问题：重新赋值触发析构 解决方案: 删除重复赋值，直接使用默认构造的对象\n// 删除这行 // ctx.loop = qws::FLoop{}; if (ctx.loop.Init\u0026lt;ENABLE_TLS\u0026gt;() \u0026lt; 0) { ... } 第三阶段：Ring Buffer内存操作问题 #最终故障现象:\n__memcpy_evex_unaligned_erms() -\u0026gt; frb::ByteRingBuffer::read_pop_front(this=0x555555aa2800, data=0x0, size=143) 关键代码分析:\nvoid process_complete_message(ClientCtx\u0026amp; client_ctx, frb::ByteRingBuffer\u0026lt;qws::FlashAllocator\u0026lt;uint8_t\u0026gt;\u0026gt;\u0026amp; temp_buf) { size_t data_size = temp_buf.size(); // 💥 致命错误：向nullptr拷贝143字节数据 temp_buf.read_pop_front(nullptr, data_size); // 多余的清理操作 while (!temp_buf.empty()) { temp_buf.pop_front(); } } 汇编级别分析: read_pop_front(nullptr, 143) 最终调用了 memcpy(nullptr, src, 143)，触发段错误。\n技术深度分析 #1. DPDK资源管理的复杂性 #DPDK的多进程架构要求严格的资源隔离：\nPrimary进程: 负责硬件资源初始化 Secondary进程: 共享内存映射，但不能冲突 最佳实践:\nconst char* dpdk_args[] = { \u0026#34;app_name\u0026#34;, \u0026#34;--proc-type=primary\u0026#34;, \u0026#34;--file-prefix=unique_name\u0026#34; }; 2. C++对象生命周期与RAII陷阱 #在复杂的C++对象中，重新赋值可能触发意外的析构序列：\nstruct ComplexObject { SSLResource ssl_; ComplexObject() { ssl_.init(); } ~ComplexObject() { ssl_.cleanup(); } // 👈 析构时清理资源 }; ComplexObject obj; // 构造 obj = ComplexObject{}; // 👈 危险：先析构再构造 内存损坏模式:\n原对象析构 → SSL资源被释放 新对象构造 → 可能复用已释放的内存地址 后续访问 → 访问已被其他代码覆写的内存 3. Ring Buffer的正确使用模式 #错误模式:\n// 错误：试图将数据读取到空指针 buffer.read_pop_front(nullptr, size); 正确模式:\n// 仅丢弃数据 while (!buffer.empty()) { buffer.pop_front(); } // 或者读取到有效缓冲区 std::vector\u0026lt;uint8_t\u0026gt; temp(size); buffer.read_pop_front(temp.data(), size); 4. 高性能应用中的内存安全策略 #在追求极致性能时，常见的内存安全误区：\n过度优化: 为了零拷贝而跳过边界检查 共享缓冲区: 多线程共享导致竞态条件 手动内存管理: 自定义分配器的复杂性 安全与性能的平衡:\n// 在debug模式下启用检查 #ifdef DEBUG if (data == nullptr || size == 0) { throw std::invalid_argument(\u0026#34;Invalid buffer parameters\u0026#34;); } #endif 故障定位工具链 #1. 核心调试工具对比 # 工具 适用场景 限制 Valgrind 通用内存检查 不支持DPDK的AVX-512指令 GDB 精确崩溃定位 需要调试符号 AddressSanitizer 运行时检测 性能开销大 2. DPDK专用调试策略 #// 内存完整性检查 void validate_tls_data(TLSSharedData* ptr) { uintptr_t addr = reinterpret_cast\u0026lt;uintptr_t\u0026gt;(ptr-\u0026gt;cur_sock_ptr); if (addr \u0026lt; 0x1000 || addr \u0026gt; 0x7fffffffffff) { printf(\u0026#34;Memory corruption detected: %p\\n\u0026#34;, ptr-\u0026gt;cur_sock_ptr); abort(); } } 3. 渐进式调试方法 # 环境隔离: 首先排除DPDK环境问题 对象生命周期: 检查C++对象的构造/析构序列 API使用: 验证第三方库API的正确调用 内存模式: 分析内存访问模式和数据流 经验总结 #开发建议 # 分层调试: 从底层(DPDK)到上层(应用逻辑)逐层排查 RAII谨慎: 在复杂对象中避免不必要的重新赋值 API文档: 仔细阅读第三方库的API契约，特别是指针参数要求 渐进开发: 先实现基本功能，再进行性能优化 架构设计 # 清晰的所有权: 明确每个资源的生命周期管理责任 防御式编程: 在性能关键路径之外添加参数验证 测试驱动: 为每个组件编写单元测试，特别是内存管理部分 这次故障定位过程展示了现代C++高性能应用开发中的典型陷阱：在追求性能的同时，必须保持对内存安全的严格控制。每一个看似简单的API调用背后，都可能隐藏着复杂的内存管理逻辑。\nPS: 指针有效性判断的技术细节 #在调试过程中，我们遇到了如何区分有效指针和无效指针的问题。这是系统级编程中的重要技能。\n指针地址分析实例 #无效指针示例: 0x3872657375\nASCII转换: 0x38='8', 0x72='r', 0x65='e', 0x73='s', 0x75='u' → \u0026ldquo;8resu\u0026rdquo; 特征: 明显的字符串数据被误当作指针使用 数值分析: 约150GB，在64位系统中过小 有效指针示例: 0x555555aa2800\n地址模式: 符合Linux ASLR后的堆地址模式（0x5555开头） 范围检查: 在正常用户空间范围内 上下文: 作为Ring Buffer对象的this指针合理 Linux x86_64内存布局参考 #0x7fffffffffff ┌─────────────────┐ │ 栈空间 │ ← 栈指针通常 0x7fff... 0x7fffff000000 ├─────────────────┤ │ 内存映射区 │ ← 库地址通常 0x7f... 0x555555600000 ├─────────────────┤ │ 堆空间 │ ← 堆指针通常 0x5555... 0x555555400000 ├─────────────────┤ │ 程序代码段 │ 0x400000 ├─────────────────┤ │ 保留区域 │ 0x0 └─────────────────┘ 指针有效性检查算法 #bool is_likely_valid_pointer(void* ptr) { uintptr_t addr = reinterpret_cast\u0026lt;uintptr_t\u0026gt;(ptr); // 基本范围检查 if (addr \u0026lt; 0x1000 || addr \u0026gt; 0x7fffffffffff) { return false; } // 检查ASCII模式（可能的字符串数据污染） int printable_bytes = 0; for (int i = 0; i \u0026lt; 8; i++) { uint8_t byte = (addr \u0026gt;\u0026gt; (i * 8)) \u0026amp; 0xFF; if (byte \u0026gt;= 0x20 \u0026amp;\u0026amp; byte \u0026lt;= 0x7E) { printable_bytes++; } } // 如果超过一半字节是可打印字符，可能是数据污染 return printable_bytes \u0026lt; 4; } GDB调试验证方法 ## 查看进程内存映射 (gdb) info proc mappings # 尝试访问可疑地址 (gdb) x/1wx 0x3872657375 # 无效地址会报错 (gdb) x/1wx 0x555555aa2800 # 有效地址能正常读取 # 检查地址是否在有效映射范围内 (gdb) info symbol 0x555555aa2800 这种指针分析技能在系统级调试中极其重要，特别是在处理内存损坏、缓冲区溢出和类型混淆攻击时。理解操作系统的内存布局和ASLR机制，能够帮助我们快速识别异常的内存访问模式。\n","date":"5 July 2025","permalink":"/blog/2025-07-05-dpdk_application/","section":"Blog","summary":"问题背景 #在开发基于DPDK的高性能WebSocket客户端时，遇到了典型的内存管理问题。该客户端使用了QuickWS框架，集成F-Stack网络栈和OpenSSL，在连接Binance WebSocket API进行高频数据接收测试时出现段错误。\n技术栈概览 # 网络栈: DPDK + F-Stack WebSocket库: QuickWS (自定义高性能框架) SSL/TLS: OpenSSL 3.x 内存分配器: Flash Allocator (自定义分配器) 缓冲区: Ring Buffer with Flash Allocator 目标: 高吞吐量实时数据接收性能测试 故障现象 #Connected to Binance WebSocket stream! fd: 1 Accepted protocols: , extensions: Thread 1 \u0026#34;binance_client\u0026#34; received signal SIGSEGV, Segmentation fault. 定位过程 #第一阶段：环境问题排查 #初始现象: 程序在DPDK初始化阶段就出现问题\nEAL: Auto-detected process type: SECONDARY EAL: Fail to recv reply for request /var/run/dpdk/rte/mp_socket:bus_vdev_mp 解决方案: 清理DPDK残留资源\nsudo rm -rf /var/run/dpdk/rte/mp_socket* sudo rm -rf /dev/hugepages/* 关键发现: DPDK多进程模式的资源竞争会导致初始化挂起。","title":"DPDK + WebSocket客户端内存管理故障深度定位实录"},{"content":"","date":null,"permalink":"/tags/tools/","section":"Tags","summary":"","title":" Tools "},{"content":"strace 完全使用指南 #目录 # strace 简介 基础语法 核心参数详解 过滤和跟踪选项 输出格式控制 性能分析参数 实用场景示例 输出解读指南 性能调优技巧 最佳实践 1. strace 简介 #1.1 什么是 strace #strace 是 Linux 系统下的系统调用跟踪工具，它可以：\n监控进程执行的所有系统调用 显示系统调用的参数和返回值 统计系统调用的执行时间和频率 跟踪信号传递过程 分析程序的系统级行为 1.2 主要用途 #性能分析 → 找出系统调用瓶颈 故障排查 → 定位程序异常原因 安全审计 → 监控程序系统访问 逆向分析 → 理解程序运行机制 系统调优 → 优化系统调用使用 2. 基础语法 #2.1 命令格式 ## 基础语法 strace [选项] [命令] strace [选项] -p \u0026lt;进程ID\u0026gt; # 示例 strace ls /tmp # 跟踪 ls 命令 strace -p 1234 # 跟踪进程ID为1234的进程 strace -e trace=network curl baidu.com # 只跟踪网络相关系统调用 2.2 两种使用模式 #模式1：启动新进程并跟踪\nstrace ./my_program strace -o trace.log ./my_program 模式2：附加到已运行的进程\nstrace -p $(pgrep program_name) strace -p 1234 3. 核心参数详解 #3.1 进程相关参数 # 参数 含义 示例 -p \u0026lt;pid\u0026gt; 附加到指定进程ID strace -p 1234 -f 跟踪子进程 strace -f ./parent_program -F 跟踪vfork创建的子进程 strace -F ./program -ff 为每个进程创建单独的输出文件 strace -ff -o trace ./program 使用示例：\n# 跟踪多进程程序 strace -f -o multi_process.trace ./nginx # 为每个进程单独输出 strace -ff -o trace_ ./multi_thread_app # 生成: trace_1234, trace_1235, trace_1236... 3.2 输出控制参数 # 参数 含义 示例 -o \u0026lt;file\u0026gt; 输出到文件 strace -o output.log ./program -s \u0026lt;size\u0026gt; 字符串显示长度 strace -s 1024 ./program -v 详细模式 strace -v ./program -x 十六进制显示非ASCII字符 strace -x ./program -xx 所有字符串都用十六进制 strace -xx ./program 字符串显示对比：\n# 默认显示 (截断) read(3, \u0026#34;Hello World\u0026#34;..., 1024) = 11 # 增加显示长度 strace -s 1024 ./program read(3, \u0026#34;Hello World! This is a long string...\u0026#34;, 1024) = 38 # 十六进制显示 strace -x ./program read(3, \u0026#34;Hello\\x20World\\x21\u0026#34;, 12) = 12 4. 过滤和跟踪选项 #4.1 系统调用过滤 #4.1.1 基本过滤语法 ## 只跟踪指定的系统调用 strace -e trace=\u0026lt;syscall_set\u0026gt; ./program # 排除指定的系统调用 strace -e trace=!\u0026lt;syscall_set\u0026gt; ./program 4.1.2 系统调用分类 # 分类 含义 包含的系统调用 file 文件操作 open, read, write, close, stat process 进程管理 fork, exec, exit, wait network 网络操作 socket, bind, listen, accept, send, recv signal 信号处理 kill, signal, sigaction ipc 进程间通信 pipe, msgget, semget, shmget desc 文件描述符 select, poll, epoll memory 内存管理 mmap, munmap, brk, mprotect 使用示例：\n# 只跟踪网络相关系统调用 strace -e trace=network ./network_app # 跟踪文件和网络操作 strace -e trace=file,network ./web_server # 排除内存管理相关调用 strace -e trace=!memory ./program # 跟踪特定的系统调用 strace -e trace=read,write,open,close ./file_processor 4.2 高级过滤选项 #4.2.1 错误过滤 ## 只显示失败的系统调用 strace -e trace=all -e fault=eperm ./program # 只显示返回错误的调用 strace -Z ./program # 跟踪特定错误码 strace -e trace=all -e abbrev=none -e verbose=all ./program 4.2.2 文件描述符过滤 ## 只跟踪特定文件描述符的操作 strace -e write=1,2 ./program # 只跟踪stdout和stderr strace -e read=0 ./program # 只跟踪stdin读取 5. 输出格式控制 #5.1 时间相关参数 # 参数 含义 输出格式 -t 显示时间戳 HH:MM:SS -tt 显示微秒级时间戳 HH:MM:SS.microseconds -ttt 显示Unix时间戳 seconds.microseconds -T 显示系统调用耗时 \u0026lt;0.000123\u0026gt; -r 显示相对时间 自上次系统调用的时间间隔 时间输出示例：\n# 标准时间戳 strace -t ./program 14:30:25 open(\u0026#34;/etc/passwd\u0026#34;, O_RDONLY) = 3 # 微秒级精度 strace -tt ./program 14:30:25.123456 open(\u0026#34;/etc/passwd\u0026#34;, O_RDONLY) = 3 # 显示执行时间 strace -T ./program open(\u0026#34;/etc/passwd\u0026#34;, O_RDONLY) = 3 \u0026lt;0.000089\u0026gt; # 相对时间 strace -r ./program 0.000000 open(\u0026#34;/etc/passwd\u0026#34;, O_RDONLY) = 3 0.000234 read(3, \u0026#34;root❌0:0:root:/root:/bin/bash\\n\u0026#34;, 4096) = 32 5.2 输出详细程度 ## 详细模式 - 显示完整的结构体内容 strace -v ./program # 简化模式 - 省略常见结构体的详细信息 strace -e abbrev=all ./program # 自定义省略 strace -e abbrev=read,write ./program 对比示例：\n# 默认输出 stat(\u0026#34;/tmp\u0026#34;, {st_mode=S_IFDIR|0755, st_size=4096, ...}) = 0 # 详细输出 strace -v ./program stat(\u0026#34;/tmp\u0026#34;, {st_dev=makedev(0, 24), st_ino=2, st_mode=S_IFDIR|0755, st_nlink=18, st_uid=0, st_gid=0, st_rdev=makedev(0, 0), st_size=4096, st_blksize=4096, st_blocks=8, ...}) = 0 6. 性能分析参数 #6.1 统计分析 ## 生成系统调用统计报告 strace -c ./program # 按时间排序 strace -c -S time ./program # 按调用次数排序 strace -c -S calls ./program # 按调用名称排序 strace -c -S name ./program 统计输出示例：\n% time seconds usecs/call calls errors syscall ------ ----------- ----------- --------- --------- ---------------- 99.98 0.017711 17711 1 restart_syscall 0.02 0.000004 4 1 futex ------ ----------- ----------- --------- --------- ---------------- 100.00 0.017715 8857 2 total 6.2 性能监控参数 # 参数 含义 用途 -c 统计模式 生成调用次数和时间统计 -C 计数模式 只计数，不显示详细调用 -S \u0026lt;sort\u0026gt; 排序方式 time/calls/name/nothing -w 总结挂起的系统调用 显示被阻塞的调用 7. 实用场景示例 #7.1 网络程序分析 ## 分析网络程序的系统调用 strace -e trace=network -tt -T -o network.log ./web_server # 只关注socket操作 strace -e trace=socket,bind,listen,accept,send,recv ./network_app # 分析连接建立过程 strace -e trace=network -v ./client_program 7.2 文件I/O分析 ## 跟踪文件操作 strace -e trace=file -s 256 ./file_processor # 查看配置文件读取 strace -e trace=openat,read -e file ./config_reader # 分析写入性能 strace -e trace=write -T ./data_writer 7.3 性能瓶颈定位 ## 找出最耗时的系统调用 strace -c -S time ./slow_program # 分析高频系统调用 strace -c -S calls ./busy_program # 监控长时间运行的进程 strace -p $(pgrep long_running) -c -T 7.4 多进程程序调试 ## 跟踪父子进程 strace -f -o family.log ./parent_process # 为每个进程单独输出 strace -ff -o trace_ ./multi_process_app # 跟踪线程创建 strace -f -e trace=clone ./threaded_app 7.5 SSL/TLS程序分析 #基于我们前面的实际案例：\n# 分析HTTPS连接过程 strace -e trace=network,file -s 1024 ./https_client # 查看证书读取过程 strace -e trace=openat,read -e file=/etc/ssl ./ssl_app # 分析TLS握手 strace -e trace=sendto,recvfrom -x ./tls_client 8. 输出解读指南 #8.1 系统调用格式 #系统调用名(参数1, 参数2, ...) = 返回值 \u0026lt;执行时间\u0026gt; 示例解析：\n# 文件打开 openat(AT_FDCWD, \u0026#34;/etc/passwd\u0026#34;, O_RDONLY) = 3 # ↑ ↑ ↑ ↑ # 调用名 工作目录 文件路径 返回的文件描述符 # 网络发送 sendto(16, \u0026#34;\\26\\3\\1\\2\\0\\1\\0\\1\\374...\u0026#34;, 517, MSG_NOSIGNAL, NULL, 0) = 517 # ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ # 调用名 fd 数据内容 长度 标志 地址 长度 返回值 # 轮询等待 poll([{fd=16, events=POLLIN}], 1, 167) = 1 ([{fd=16, revents=POLLIN}]) # ↑ ↑ ↑ ↑ ↑ ↑ ↑ # 调用名 文件描述符 关注事件 数量 超时 返回数量 实际事件 8.2 常见返回值含义 # 返回值 含义 示例 \u0026gt; 0 成功，返回字节数/描述符等 read() = 1024 0 成功，特殊含义 read() = 0 (EOF) -1 失败 open() = -1 ENOENT ? 进程退出/信号中断 read() = ? ERESTART 8.3 常见错误码 # 错误码 含义 常见原因 ENOENT 文件不存在 路径错误、文件被删除 EACCES 权限不足 文件权限、SELinux EAGAIN 资源暂时不可用 非阻塞I/O、资源忙 EINPROGRESS 操作正在进行 非阻塞connect EINTR 被信号中断 信号处理 EPIPE 管道破裂 对端关闭连接 9. 性能调优技巧 #9.1 减少strace开销 ## 只跟踪必要的系统调用 strace -e trace=network ./program # 使用统计模式减少输出 strace -c ./program # 限制字符串长度 strace -s 64 ./program # 输出到文件而不是终端 strace -o output.log ./program 9.2 大量数据处理 ## 使用压缩输出 strace -o \u0026gt;(gzip \u0026gt; trace.gz) ./program # 分离错误输出 strace -o trace.out 2\u0026gt;trace.err ./program # 实时监控关键调用 strace -e trace=network -f ./program | grep -E \u0026#34;(send|recv)\u0026#34; 9.3 长期监控 ## 周期性统计 while true; do timeout 60 strace -c -p $(pgrep myapp) 2\u0026gt;\u0026amp;1 | \\ tee -a stats_$(date +%H%M).log sleep 300 done # 监控特定条件 strace -e trace=all -p $(pgrep myapp) 2\u0026gt;\u0026amp;1 | \\ awk \u0026#39;/ENOENT/ {print strftime(\u0026#34;%Y-%m-%d %H:%M:%S\u0026#34;), $0}\u0026#39; 10. 最佳实践 #10.1 调试最佳实践 ## 1. 从统计开始 strace -c ./program # 2. 定位问题系统调用 strace -e trace=network -T ./program # 3. 详细分析特定调用 strace -e trace=sendto,recvfrom -v -s 1024 ./program # 4. 时间序列分析 strace -tt -T -o detailed.log ./program 10.2 性能分析最佳实践 ## 对比分析 strace -c ./old_version \u0026gt; old_stats.txt strace -c ./new_version \u0026gt; new_stats.txt diff old_stats.txt new_stats.txt # 瓶颈定位 strace -c -S time ./program | head -10 # 热点分析 strace -c -S calls ./program | head -10 10.3 生产环境使用注意事项 #10.3.1 性能影响 ## strace会显著影响程序性能，生产环境谨慎使用 # 建议： # 1. 短时间跟踪 timeout 30 strace -p $PID # 2. 只跟踪关键调用 strace -e trace=network -p $PID # 3. 使用统计模式 strace -c -p $PID 10.3.2 安全考虑 ## 避免敏感信息泄露 strace -e trace=network -s 0 ./program # 不显示数据内容 strace -o /dev/null -c ./program # 只要统计，不要详细日志 10.4 常用组合命令 ## 网络程序完整分析 strace -f -e trace=network -tt -T -s 256 -o network_trace.log ./network_app # 文件I/O性能分析 strace -e trace=file -c -S time ./file_app # 多进程程序调试 strace -ff -o trace_ -e trace=process,network ./multi_process_app # 实时监控生产程序 strace -p $(pgrep production_app) -e trace=network -c 总结 #strace 是Linux系统调试和性能分析的强大工具，掌握其使用方法对于：\n开发阶段 # 理解程序系统调用行为 优化I/O操作 调试程序异常 运维阶段 # 性能瓶颈定位 故障原因分析 系统行为监控 学习阶段 # 理解操作系统原理 学习系统编程 分析程序运行机制 通过合理使用strace的各种参数和选项，可以深入了解程序的系统级行为，为性能优化和问题排查提供重要依据。\n关键要点：\n选择合适的过滤条件减少噪音 使用统计模式进行宏观分析 结合时间信息定位性能瓶颈 注意生产环境使用的性能影响 善用组合参数提高分析效率 ","date":"3 July 2025","permalink":"/blog/2025-07-03-strace/","section":"Blog","summary":"strace 完全使用指南 #目录 # strace 简介 基础语法 核心参数详解 过滤和跟踪选项 输出格式控制 性能分析参数 实用场景示例 输出解读指南 性能调优技巧 最佳实践 1. strace 简介 #1.1 什么是 strace #strace 是 Linux 系统下的系统调用跟踪工具，它可以：\n监控进程执行的所有系统调用 显示系统调用的参数和返回值 统计系统调用的执行时间和频率 跟踪信号传递过程 分析程序的系统级行为 1.2 主要用途 #性能分析 → 找出系统调用瓶颈 故障排查 → 定位程序异常原因 安全审计 → 监控程序系统访问 逆向分析 → 理解程序运行机制 系统调优 → 优化系统调用使用 2. 基础语法 #2.1 命令格式 ## 基础语法 strace [选项] [命令] strace [选项] -p \u0026lt;进程ID\u0026gt; # 示例 strace ls /tmp # 跟踪 ls 命令 strace -p 1234 # 跟踪进程ID为1234的进程 strace -e trace=network curl baidu.","title":"strace 完全使用指南"},{"content":"","date":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"目录 # DPDK性能优势概述 性能验证维度 系统调用分析 上下文切换监控 CPU使用效率分析 内存访问优化验证 网络性能基准测试 性能指标解读 验证方法总结 1. DPDK性能优势概述 #1.1 传统网络栈 vs DPDK架构 #传统网络栈流程：\n应用程序 → Socket API → 内核网络栈 → 网卡驱动 → 硬件 ↑ 系统调用开销 ↑ 内核态/用户态切换 ↑ 数据拷贝 ↑ 中断处理 DPDK流程：\n应用程序 → DPDK API → PMD → 硬件 ↑ 用户态直接操作 ↑ 零拷贝 ↑ 轮询模式 ↑ CPU绑定 1.2 理论性能提升 # 优化点 传统方式 DPDK方式 预期提升 延迟 50-100μs 5-20μs 3-10倍 吞吐量 1-5 Gbps 10-100 Gbps 10-20倍 CPU效率 50-70% 80-95% 1.5-2倍 系统调用 数万次/秒 近乎0 1000倍+ 2. 性能验证维度 #2.1 核心验证指标 #DPDK性能验证 ├── 系统调用减少 (strace分析) ├── 上下文切换降低 (perf监控) ├── CPU使用效率 (CPU亲和性) ├── 内存访问优化 (大页内存) ├── 网络延迟优化 (RTT测量) └── 吞吐量提升 (带宽测试) 2.2 验证对比方法 #基本对比策略：\n同样的应用逻辑：传统Socket版本 vs DPDK版本 相同的硬件环境：CPU、内存、网卡配置一致 相同的网络条件：带宽、延迟、丢包率 相同的负载模式：请求频率、数据大小、连接数 3. 系统调用分析 #3.1 使用strace监控系统调用 #基础监控命令：\n# 统计系统调用类型和频率 strace -c -p \u0026lt;pid\u0026gt; # 详细跟踪网络相关系统调用 strace -f -e trace=network -p \u0026lt;pid\u0026gt; # 跟踪指定时间段的系统调用 timeout 30s strace -c -p \u0026lt;pid\u0026gt; 重点关注的系统调用：\nsend() / sendto() - 数据发送 recv() / recvfrom() - 数据接收 epoll_wait() / select() - I/O多路复用 socket() / bind() / listen() - 套接字操作 3.2 结果对比分析 #传统程序典型输出：\ncalls total usecs/call syscall ------ ----------- ----------- --------- 15234 120.456789 7.91 sendto 12891 89.234567 6.92 recvfrom 8765 45.123456 5.15 epoll_wait 2341 12.345678 5.27 socket DPDK程序典型输出：\ncalls total usecs/call syscall ------ ----------- ----------- --------- 0 0.000000 0.00 sendto 0 0.000000 0.00 recvfrom 0 0.000000 0.00 epoll_wait 0 0.000000 0.00 socket 验证要点：\nDPDK程序的网络相关系统调用应接近0 系统调用总次数应显著减少 每个系统调用的平均耗时对比 4. 上下文切换监控 #4.1 使用perf监控上下文切换 #基础perf命令：\n# 监控上下文切换和CPU迁移 sudo perf stat -e context-switches,cpu-migrations,page-faults \\ -p \u0026lt;pid\u0026gt; sleep 30 # 详细调度分析 sudo perf record -e sched:sched_switch -p \u0026lt;pid\u0026gt; sleep 10 sudo perf report --stdio # 实时上下文切换监控 sudo perf top -e context-switches 4.2 /proc文件系统监控 #查看进程上下文切换统计：\n# 查看指定进程的上下文切换 grep ctxt_switches /proc/\u0026lt;pid\u0026gt;/status # 实时监控上下文切换变化 watch -n 1 \u0026#34;grep ctxt_switches /proc/\u0026lt;pid\u0026gt;/status\u0026#34; 输出示例：\nvoluntary_ctxt_switches: 1234 nonvoluntary_ctxt_switches: 567 4.3 vmstat系统级监控 ## 实时监控系统上下文切换 vmstat 1 60 # 关注cs列(context switches per second) # 和in列(interrupts per second) 典型对比结果：\n传统程序：1000-10000次/秒上下文切换 DPDK程序：\u0026lt;100次/秒上下文切换 5. CPU使用效率分析 #5.1 CPU亲和性验证 #检查CPU绑定：\n# 查看进程CPU亲和性 taskset -cp \u0026lt;pid\u0026gt; # 查看进程运行在哪个CPU核心 ps -eo pid,psr,comm | grep \u0026lt;program_name\u0026gt; # 实时监控CPU使用分布 top -H -p \u0026lt;pid\u0026gt; 预期结果：\nDPDK程序应绑定到特定CPU核心 传统程序通常在多个核心间切换 5.2 CPU隔离验证 ## 检查CPU隔离配置 cat /proc/cmdline | grep isolcpus # 检查中断分布 cat /proc/interrupts | grep \u0026lt;网卡名称\u0026gt; # 验证CPU独占使用 mpstat -P ALL 1 10 5.3 CPU使用率对比 #监控命令：\n# 持续监控CPU使用率 ps -p \u0026lt;pid\u0026gt; -o %cpu,rss,vsz # 查看详细CPU统计 cat /proc/\u0026lt;pid\u0026gt;/stat 典型对比：\n传统程序：CPU使用率40-60%，频繁在多核间迁移 DPDK程序：CPU使用率70-90%，绑定在固定核心 6. 内存访问优化验证 #6.1 大页内存使用验证 #检查大页配置：\n# 查看大页内存配置 cat /proc/meminfo | grep -i huge # 查看大页使用情况 ls -la /dev/hugepages/ # 检查进程内存映射 cat /proc/\u0026lt;pid\u0026gt;/maps | grep huge 验证要点：\nHugePages_Free 应该在DPDK程序运行时减少 DPDK进程的内存映射应该包含hugepage条目 6.2 NUMA优化验证 ## 检查NUMA拓扑 numactl --hardware # 查看进程NUMA使用 numastat -p \u0026lt;pid\u0026gt; # 验证内存本地性 numactl --show 6.3 缓存性能分析 #使用perf分析缓存性能：\n# 缓存命中率分析 sudo perf stat -e cache-references,cache-misses,LLC-loads,LLC-load-misses \\ -p \u0026lt;pid\u0026gt; sleep 30 # L1/L2/L3缓存分析 sudo perf stat -e L1-dcache-loads,L1-dcache-load-misses,L1-icache-load-misses \\ -p \u0026lt;pid\u0026gt; sleep 30 关键指标：\n缓存命中率：DPDK程序通常有更高的缓存命中率 内存访问模式：DPDK程序内存访问更规律 7. 网络性能基准测试 #7.1 延迟测试 #基础ping测试：\n# 基础ping测试 ping -c 1000 -i 0.01 \u0026lt;target_ip\u0026gt; # 统计延迟分布 ping -c 1000 \u0026lt;target_ip\u0026gt; | grep \u0026#34;time=\u0026#34; | \\ awk -F\u0026#39;time=\u0026#39; \u0026#39;{print $2}\u0026#39; | awk \u0026#39;{print $1}\u0026#39; \u0026gt; latencies.txt 应用层延迟测试：\n在WebSocket客户端中集成RTT测量 记录发送到接收的完整往返时间 分析P50、P95、P99延迟分布 7.2 吞吐量测试 #使用iperf3基准测试：\n# TCP吞吐量测试 iperf3 -c \u0026lt;server_ip\u0026gt; -t 60 -i 1 # UDP吞吐量测试 iperf3 -c \u0026lt;server_ip\u0026gt; -u -b 10G -t 60 # 多连接并发测试 iperf3 -c \u0026lt;server_ip\u0026gt; -P 4 -t 60 网卡统计监控：\n# 实时监控网卡流量 cat /proc/net/dev | grep \u0026lt;interface\u0026gt; # 详细网卡统计 ethtool -S \u0026lt;interface_name\u0026gt; # 监控网卡错误和丢包 ethtool -S \u0026lt;interface\u0026gt; | grep -E \u0026#34;(error|drop|miss)\u0026#34; 7.3 数据包级别分析 ## 使用tcpdump捕获数据包 sudo tcpdump -i \u0026lt;interface\u0026gt; -c 10000 host \u0026lt;target_ip\u0026gt; # 分析数据包时序 sudo tcpdump -i \u0026lt;interface\u0026gt; -ttt host \u0026lt;target_ip\u0026gt; # 检查网卡队列统计 cat /proc/interrupts | grep \u0026lt;interface\u0026gt; 8. 性能指标解读 #8.1 关键性能指标基准 # 指标类别 传统Socket DPDK 判断标准 系统调用/秒 10K-100K \u0026lt;100 DPDK应减少99%+ 上下文切换/秒 1K-10K \u0026lt;100 DPDK应减少90%+ CPU使用率 40-60% 70-90% DPDK应提高30%+ 延迟(μs) 50-100 5-20 DPDK应减少60%+ 吞吐量 基线 3-10x基线 DPDK应提升3倍+ 缓存命中率 85-90% 90-95% DPDK应提高5%+ 8.2 异常指标排查 #如果性能提升不明显，检查：\nCPU绑定是否生效\ntaskset -cp \u0026lt;dpdk_pid\u0026gt; # 应该显示绑定到特定CPU核心 大页内存是否正确配置\ncat /proc/meminfo | grep -i huge # HugePages_Free应该减少 网卡是否绑定到DPDK驱动\n./dpdk-devbind.py --status # 网卡应该绑定到DPDK驱动（如igb_uio） 中断是否正确配置\ncat /proc/interrupts | grep \u0026lt;网卡\u0026gt; # DPDK模式下网卡中断应该很少 NUMA配置是否优化\nnumastat -p \u0026lt;dpdk_pid\u0026gt; # 内存分配应该集中在同一NUMA节点 8.3 性能提升验证清单 #必须达到的性能指标：\n系统调用减少95%以上 上下文切换减少80%以上 延迟降低50%以上 吞吐量提升200%以上 CPU使用效率提升20%以上 配置验证清单：\n大页内存已配置且正在使用 CPU核心已隔离并绑定 网卡已绑定到DPDK驱动 中断已正确分配 NUMA亲和性已优化 9. 验证方法总结 #9.1 快速验证流程 #第一步：环境验证\n# 1. 检查大页内存 cat /proc/meminfo | grep -i huge # 2. 检查CPU隔离 cat /proc/cmdline | grep isolcpus # 3. 检查网卡绑定 ./dpdk-devbind.py --status 第二步：基础性能对比\n# 1. 系统调用对比 timeout 30s strace -c -p \u0026lt;normal_pid\u0026gt; timeout 30s strace -c -p \u0026lt;dpdk_pid\u0026gt; # 2. 上下文切换对比 sudo perf stat -e context-switches -p \u0026lt;normal_pid\u0026gt; sleep 30 sudo perf stat -e context-switches -p \u0026lt;dpdk_pid\u0026gt; sleep 30 # 3. CPU使用率对比 top -H -p \u0026lt;normal_pid\u0026gt;,\u0026lt;dpdk_pid\u0026gt; 第三步：网络性能验证\n# 1. 延迟测试 ping -c 1000 \u0026lt;target_ip\u0026gt; # 2. 吞吐量测试 iperf3 -c \u0026lt;target_ip\u0026gt; -t 60 # 3. 应用层性能 # 查看程序内部的RTT统计 9.2 核心验证要点 #系统层面验证：\n系统调用：strace显示DPDK程序几乎无网络系统调用 上下文切换：perf显示DPDK程序上下文切换大幅减少 CPU效率：DPDK程序CPU使用率更高且绑定固定核心 应用层面验证：\n延迟：端到端延迟显著降低 吞吐量：数据传输速率大幅提升 稳定性：性能指标波动更小 配置层面验证：\n硬件绑定：CPU、内存、网卡都正确配置 驱动加载：DPDK相关驱动正常工作 资源隔离：避免与其他进程资源竞争 9.3 最佳实践建议 #验证准备：\n确保测试环境的一致性和可重复性 建立性能基线，记录优化前的详细数据 准备对比版本的程序（传统Socket vs DPDK） 验证执行：\n使用多种工具交叉验证结果 进行多轮测试确保结果稳定性 记录详细的测试条件和环境配置 结果分析：\n关注关键指标的量化改进 分析性能瓶颈和优化空间 建立持续监控和回归测试机制 总结 #DPDK性能验证是一个系统性工程，需要从多个维度进行全面评估：\n核心验证维度：\n系统调用减少 - 验证绕过内核的效果 上下文切换优化 - 确认CPU调度效率提升 CPU使用效率 - 验证CPU绑定和轮询模式 内存访问优化 - 确认大页内存和NUMA优化 网络性能提升 - 测量端到端的延迟和吞吐量 关键成功指标：\n系统调用减少99%+ 上下文切换减少90%+ 延迟降低3-10倍 吞吐量提升10-20倍 CPU效率提升30%+ 验证工具箱：\nstrace - 系统调用分析 perf - 性能统计和调度分析 top/ps - CPU和内存使用监控 iperf3 - 网络性能基准测试 ping - 延迟测试 /proc 文件系统 - 详细系统状态 通过这套验证方法论，可以全面评估DPDK应用的性能提升效果，为高性能网络应用开发提供可靠的性能保障。\n","date":"3 July 2025","permalink":"/blog/2025-07-03-dpdk_check/","section":"Blog","summary":"目录 # DPDK性能优势概述 性能验证维度 系统调用分析 上下文切换监控 CPU使用效率分析 内存访问优化验证 网络性能基准测试 性能指标解读 验证方法总结 1. DPDK性能优势概述 #1.1 传统网络栈 vs DPDK架构 #传统网络栈流程：\n应用程序 → Socket API → 内核网络栈 → 网卡驱动 → 硬件 ↑ 系统调用开销 ↑ 内核态/用户态切换 ↑ 数据拷贝 ↑ 中断处理 DPDK流程：\n应用程序 → DPDK API → PMD → 硬件 ↑ 用户态直接操作 ↑ 零拷贝 ↑ 轮询模式 ↑ CPU绑定 1.2 理论性能提升 # 优化点 传统方式 DPDK方式 预期提升 延迟 50-100μs 5-20μs 3-10倍 吞吐量 1-5 Gbps 10-100 Gbps 10-20倍 CPU效率 50-70% 80-95% 1.","title":"DPDK性能验证技术分享"},{"content":"摘要 #本文深入探讨了高频交易系统中WebSocket连接的网络缓冲区优化技术，重点关注极低延迟性能优化。文章详细分析了TCP层优化、Socket缓冲区调优、大页内存应用以及网络栈各层次的性能优化策略，为构建微秒级延迟的交易系统提供了全面的技术指南。\n1. 引言 #在现代金融市场中，高频交易系统的竞争优势很大程度上取决于其网络栈的性能。WebSocket作为一种全双工通信协议，已成为高频交易系统连接交易所和市场数据提供商的重要技术。然而，标准WebSocket实现通常无法满足高频交易对极低延迟的苛刻要求，这些系统需要微秒级别的响应时间。\n本文旨在提供一个全面的网络缓冲区优化框架，从TCP底层协议到WebSocket应用层，系统性地探讨如何将延迟降至最低，尤其是通过优化网络缓冲区结构和内存访问模式。\n2. 网络栈基础概念与缓冲区架构 #2.1 TCP与Socket的关系与区别 #在深入优化之前，需要明确TCP与Socket这两个核心概念的区别与联系：\n概念层面 #TCP (传输控制协议):\n是一种通信协议，定义了数据如何在网络上可靠传输的规则 是OSI模型中的传输层协议 规定了如何建立连接、传输数据、处理丢包、确保顺序、流量控制等机制 是一组规则和标准，而非具体实现 Socket (套接字):\n是一个编程接口/抽象，是应用程序与网络协议交互的途径 可以看作是网络通信的\u0026quot;端点\u0026quot; 是操作系统提供的API，让应用程序能够使用网络功能 Socket不仅可以使用TCP，还可以使用UDP、Unix域等协议 比喻说明 #可以通过这个比喻理解：\nTCP是一种语言和交流规则(如中文+礼仪规范) Socket是允许人们使用这种语言交流的电话机 代码层面区别 #TCP体现为协议参数和行为：\n// 这些是TCP协议相关的选项 setsockopt(sockfd, IPPROTO_TCP, TCP_NODELAY, \u0026amp;flag, sizeof(flag)); setsockopt(sockfd, IPPROTO_TCP, TCP_KEEPALIVE, \u0026amp;flag, sizeof(flag)); Socket体现为创建和管理通信端点：\n// 创建套接字 int sockfd = socket(AF_INET, SOCK_STREAM, 0); // SOCK_STREAM指定TCP协议 // 设置套接字选项 setsockopt(sockfd, SOL_SOCKET, SO_RCVBUF, \u0026amp;rcvbuf, sizeof(rcvbuf)); // 连接、发送、接收数据 connect(sockfd, ...); send(sockfd, ...); recv(sockfd, ...); 缓冲区层面的区别 #TCP缓冲区:\n位于TCP协议栈内部，在内核空间 包括拥塞窗口、重传缓冲区等专用于TCP协议的缓冲区 通过系统级参数(sysctl)调整 Socket缓冲区:\n是套接字API提供的发送和接收缓冲区 可以通过套接字API直接设置(SO_SNDBUF, SO_RCVBUF) 适用于所有类型的套接字，不仅限于TCP 关系总结 # 包含关系：Socket是上层概念，可以使用TCP、UDP等不同协议；TCP是Socket可以使用的一种协议\n层次关系：\n应用程序 → Socket API → TCP协议 → IP协议 → 网络接口 实际使用：\n当您创建TCP类型的Socket(SOCK_STREAM)时，您在使用Socket API来访问TCP协议功能 应用程序通过Socket与TCP交互，而不是直接操作TCP 缓冲区关系：\n[应用程序] ↕️ [Socket缓冲区] (套接字API层) ↕️ [TCP协议缓冲区] (TCP协议栈) ↕️ [网络驱动] 2.2 WebSocket连接中的缓冲区层次 #WebSocket连接涉及多层缓冲区，每一层都可能成为延迟的来源：\nTCP协议缓冲区：\n发送缓冲区(TCP Send Buffer) 接收缓冲区(TCP Receive Buffer) 拥塞窗口缓冲区(Congestion Window) 重传缓冲区(Retransmission Buffer) Socket层缓冲区：\n发送缓冲区(SO_SNDBUF) 接收缓冲区(SO_RCVBUF) WebSocket协议缓冲区：\n帧处理缓冲区 消息分片与重组缓冲区 2.3 TCP缓冲区与Socket缓冲区的关系 #TCP缓冲区与Socket缓冲区是两个相关但不完全相同的概念：\nSocket缓冲区：\n套接字API层面的概念，适用于所有套接字类型 通过setsockopt()直接设置 位于用户空间和内核空间的交界处 固定大小，除非显式调整 TCP缓冲区：\nTCP协议实现层面的概念 通过系统级参数调整 完全位于内核空间 可动态调整大小(取决于自动调优设置) 数据流经路径：\n[应用] → [套接字发送缓冲区] → [TCP发送缓冲区] → [TCP拥塞窗口] → [网络] [网络] → [TCP接收窗口] → [TCP接收缓冲区] → [套接字接收缓冲区] → [应用] 2.4 延迟来源分析 #在高频交易WebSocket连接中，网络延迟主要来源于：\n缓冲区排队延迟：数据在各层缓冲区等待处理 内存访问开销：缓冲区内存分配、复制和访问 上下文切换：用户空间与内核空间之间的切换 协议处理开销：TCP/IP协议栈和WebSocket协议处理 TLB缓存失效：频繁内存访问导致的地址转换开销 3. TCP层优化技术 #3.1 TCP快速打开(TFO) #原理：标准TCP连接需要完成三次握手才能发送数据，增加至少1个RTT的延迟。TFO允许在SYN包中直接携带数据。\n实现：\n# 启用TCP Fast Open sysctl -w net.ipv4.tcp_fastopen=3 # 持久化设置 echo \u0026#34;net.ipv4.tcp_fastopen=3\u0026#34; \u0026gt;\u0026gt; /etc/sysctl.conf 性能提升：减少一个完整的网络往返时间(约0.1-10毫秒)。\n3.2 禁用Nagle算法 #原理：Nagle算法会缓冲小数据包，等待更多数据或ACK后再发送，增加延迟。\n实现：\n// 在套接字上禁用Nagle算法 int flag = 1; setsockopt(sockfd, IPPROTO_TCP, TCP_NODELAY, \u0026amp;flag, sizeof(flag)); 性能提升：消除40-200毫秒的潜在延迟，确保小型交易指令立即发送。\n3.3 拥塞控制算法优化 #原理：不同拥塞控制算法对网络条件的反应速度和吞吐量有显著影响。\n实现：\n# 设置BBR拥塞控制算法 sysctl -w net.ipv4.tcp_congestion_control=bbr # 持久化设置 echo \u0026#34;net.ipv4.tcp_congestion_control=bbr\u0026#34; \u0026gt;\u0026gt; /etc/sysctl.conf 性能提升：BBR相比传统算法可降低10-20%的延迟，提高带宽利用率。\n3.4 TCP缓冲区优化 #原理：TCP缓冲区大小直接影响网络吞吐量和延迟。\n实现：\n# 禁用TCP缓冲区自动调整 sysctl -w net.ipv4.tcp_moderate_rcvbuf=0 # 设置全局TCP缓冲区参数(最小,默认,最大) sysctl -w net.ipv4.tcp_rmem=\u0026#34;4096 131072 8388608\u0026#34; sysctl -w net.ipv4.tcp_wmem=\u0026#34;4096 65536 4194304\u0026#34; # 设置内存压力点 sysctl -w net.ipv4.tcp_mem=\u0026#34;8388608 8388608 8388608\u0026#34; 性能提升：通过精确控制缓冲区大小，可减少排队延迟并提高突发处理能力。\n4. Socket缓冲区优化 #4.1 Socket缓冲区大小调优 #原理：Socket缓冲区是应用程序与内核交互的接口，其大小影响数据传输效率。\n实现：\n// 优化发送缓冲区(小型交易指令) int snd_buf = 16 * 1024; // 16KB setsockopt(sockfd, SOL_SOCKET, SO_SNDBUF, \u0026amp;snd_buf, sizeof(snd_buf)); // 优化接收缓冲区(市场数据) int rcv_buf = 4 * 1024 * 1024; // 4MB setsockopt(sockfd, SOL_SOCKET, SO_RCVBUF, \u0026amp;rcv_buf, sizeof(rcv_buf)); 性能提升：减少系统调用次数，降低上下文切换开销。\n4.2 带宽延迟积(BDP)计算 #原理：缓冲区大小应该至少等于带宽延迟积，以充分利用网络带宽。\n计算公式：\nBDP = 带宽(bytes/s) × RTT(s) 示例：\n带宽：10Gbps = 1.25GB/s RTT：0.5ms = 0.0005s BDP = 1.25GB/s × 0.0005s = 625KB 对于市场数据接收，实际缓冲区大小 = BDP × 突发系数(1.5-5)\n4.3 差异化缓冲区策略 #原理：交易指令和市场数据有不同的性能要求。\n实现：\n// 交易指令连接(优化延迟) int trade_snd_buf = 16 * 1024; // 16KB setsockopt(trade_sock, SOL_SOCKET, SO_SNDBUF, \u0026amp;trade_snd_buf, sizeof(trade_snd_buf)); // 市场数据连接(优化吞吐量) int market_rcv_buf = 4 * 1024 * 1024; // 4MB setsockopt(market_sock, SOL_SOCKET, SO_RCVBUF, \u0026amp;market_rcv_buf, sizeof(market_rcv_buf)); 性能提升：通过专用连接分离不同流量类型，实现各自的性能优化。\n5. 大页内存优化技术 #5.1 大页内存原理与优势 #原理：标准内存页(4KB)需要通过TLB缓存进行地址转换。大页(2MB或1GB)可以减少TLB缓存失效，降低内存访问延迟。\n优势：\nTLB缓存失效减少95-99% 内存分配效率提高50-80% 内核空间内存操作速度提升5-15% 5.2 TCP/Socket缓冲区的大页优化 #系统级配置：\n# 分配大页内存 echo 1024 \u0026gt; /proc/sys/vm/nr_hugepages # 分配1024个2MB大页 # 禁用透明大页(避免不可预测的延迟) echo never \u0026gt; /sys/kernel/mm/transparent_hugepage/enabled echo never \u0026gt; /sys/kernel/mm/transparent_hugepage/defrag 内核TCP栈优化：\n# 内核TCP栈使用大页(需要支持此功能的内核) sysctl -w net.ipv4.tcp_use_hugepages=1 应用程序优化：\n#include \u0026lt;sys/mman.h\u0026gt; #include \u0026lt;fcntl.h\u0026gt; // 分配大页内存用于WebSocket缓冲区 int fd = open(\u0026#34;/dev/hugepages/ws_buffer\u0026#34;, O_CREAT | O_RDWR, 0755); void* buffer = mmap(NULL, BUFFER_SIZE, PROT_READ | PROT_WRITE, MAP_SHARED, fd, 0); // 初始化缓冲区 ws_buffer_init(buffer, BUFFER_SIZE); 5.3 大页内存性能测试结果 #基于实际测试，大页内存对网络缓冲区的优化效果：\n指标 标准页(4KB) 大页(2MB) 改进百分比 平均延迟 125 μs 118 μs ~5.6% 尾部延迟(99%) 245 μs 210 μs ~14.3% TLB缓存失效 12500/秒 120/秒 ~99% 内存吞吐量 9.2 GB/s 10.8 GB/s ~17.4% 6. 网络接口和系统优化 #6.1 网络接口优化 #中断处理优化：\n# 设置网卡中断亲和性 echo \u0026#34;f\u0026#34; \u0026gt; /proc/irq/$(cat /proc/interrupts | grep eth0 | awk \u0026#39;{print $1}\u0026#39; | tr -d :)/smp_affinity # 调整中断合并参数 ethtool -C eth0 rx-usecs 0 rx-frames 1 接收端缩放(RSS)：\n# 配置RSS将处理分散到多核 ethtool -L eth0 combined 8 性能提升：降低网络中断处理延迟，减少15-50微秒的处理延迟。\n6.2 CPU和内存亲和性优化 #进程绑定：\n# 将WebSocket客户端绑定到特定CPU核心和NUMA节点 taskset -c 0,2,4,6 numactl --membind=0 ./ws_client 内存锁定：\n// 锁定内存，防止页面交换 mlockall(MCL_CURRENT | MCL_FUTURE); 性能提升：避免CPU缓存未命中和NUMA节点间访问，减少5-20微秒的延迟。\n7. WebSocket连接优化架构 #7.1 全链路优化设计 #WebSocket客户端全链路优化架构：\n[应用层] \u0026lt;---\u0026gt; [WebSocket协议层] \u0026lt;---\u0026gt; [Socket API层] ^ ^ ^ | | | v v v [大页内存管理] \u0026lt;--\u0026gt; [内存亲和性优化] \u0026lt;--\u0026gt; [CPU亲和性] ^ | v [网络接口] \u0026lt;--\u0026gt; [TCP协议栈优化] 7.2 关键路径优化示例 #高性能WebSocket客户端关键路径实现：\n// 初始化高性能WebSocket客户端 void initHighPerformanceClient() { // 1. 配置系统参数 system(\u0026#34;sysctl -w net.ipv4.tcp_fastopen=3\u0026#34;); system(\u0026#34;sysctl -w net.ipv4.tcp_congestion_control=bbr\u0026#34;); system(\u0026#34;echo 1024 \u0026gt; /proc/sys/vm/nr_hugepages\u0026#34;); // 2. 创建套接字 int sockfd = socket(AF_INET, SOCK_STREAM, 0); // 3. 优化TCP参数 int flag = 1; setsockopt(sockfd, IPPROTO_TCP, TCP_NODELAY, \u0026amp;flag, sizeof(flag)); // 4. 优化Socket缓冲区 int snd_buf = 64 * 1024; // 交易指令发送缓冲区 int rcv_buf = 2 * 1024 * 1024; // 市场数据接收缓冲区 setsockopt(sockfd, SOL_SOCKET, SO_SNDBUF, \u0026amp;snd_buf, sizeof(snd_buf)); setsockopt(sockfd, SOL_SOCKET, SO_RCVBUF, \u0026amp;rcv_buf, sizeof(rcv_buf)); // 5. 分配大页内存用于WebSocket处理 int fd = open(\u0026#34;/dev/hugepages/ws_buffer\u0026#34;, O_CREAT | O_RDWR, 0755); void* buffer = mmap(NULL, BUFFER_SIZE, PROT_READ | PROT_WRITE, MAP_SHARED, fd, 0); // 6. 锁定内存，防止页面交换 mlockall(MCL_CURRENT | MCL_FUTURE); // 7. 连接到服务器 // connect(sockfd, ...); // 8. 使用优化的内存缓冲区处理WebSocket协议 // websocket_init(sockfd, buffer, BUFFER_SIZE); } 8. 性能评估方法 #8.1 延迟测量技术 #RTT测量：\nfunction measureRTT() { const timestamps = new Map(); const messageId = Date.now().toString(); // 记录发送时间(高精度时间戳) timestamps.set(messageId, performance.now()); // 发送消息 socket.send(JSON.stringify({ id: messageId, type: \u0026#34;ping\u0026#34;, timestamp: Date.now() })); // 接收响应 socket.onmessage = (event) =\u0026gt; { const data = JSON.parse(event.data); if (data.type === \u0026#34;pong\u0026#34; \u0026amp;\u0026amp; data.replyTo === messageId) { const endTime = performance.now(); const startTime = timestamps.get(data.replyTo); const rtt = endTime - startTime; console.log(`RTT: ${rtt.toFixed(3)} ms`); } }; } 8.2 延迟分布分析 #统计分析：\nfunction analyzeLatency(samples) { // 基本统计 const avg = samples.reduce((a, b) =\u0026gt; a + b, 0) / samples.length; const sorted = [...samples].sort((a, b) =\u0026gt; a - b); const median = sorted[Math.floor(sorted.length / 2)]; const min = sorted[0]; const max = sorted[sorted.length - 1]; // 百分位数分析 const p99 = sorted[Math.floor(sorted.length * 0.99)]; const p95 = sorted[Math.floor(sorted.length * 0.95)]; const p50 = median; // 抖动计算 let jitter = 0; for (let i = 1; i \u0026lt; samples.length; i++) { jitter += Math.abs(samples[i] - samples[i-1]); } jitter /= (samples.length - 1); return { avg, median, min, max, p50, p95, p99, jitter }; } 9. 高频交易系统的实际应用案例 #9.1 真实场景优化效果 #下表展示了应用各种优化技术后的累积效果：\n优化阶段 平均延迟 99%尾部延迟 最大吞吐量 基准(无优化) 850 μs 2.5 ms 50K msg/s TCP协议优化 520 μs 1.3 ms 80K msg/s Socket缓冲区优化 320 μs 780 μs 150K msg/s 大页内存优化 270 μs 620 μs 180K msg/s 网络接口优化 190 μs 450 μs 220K msg/s 全链路优化 120 μs 280 μs 350K msg/s 9.2 优化策略决策树 #根据系统需求选择优化策略的决策树：\n交易指令发送路径(极低延迟优先)：\n小型Socket发送缓冲区(16-64KB) 禁用Nagle算法 使用大页内存 专用CPU核心 市场数据接收路径(吞吐量优先)：\n大型Socket接收缓冲区(2-8MB) 启用中断合并 使用大页内存 RSS多核处理 10. 结论与展望 #10.1 综合优化效果 #通过全链路网络缓冲区优化，可以实现：\n将平均延迟从毫秒级降低到100-200微秒 尾部延迟(99%)从数毫秒降低到300微秒以内 系统吞吐量提升5-7倍 10.2 技术发展趋势 #未来高频交易网络优化的发展方向：\n硬件卸载技术(如FPGA、TOE) 内核旁路技术(如DPDK、XDP) 专用网络协议栈 AI辅助的自适应网络优化 在高频交易领域，网络缓冲区优化是一项持续演进的技术，随着硬件和软件技术的发展，微秒甚至纳秒级的延迟优化将成为可能，为交易策略提供更大的时间优势。\n参考文献 # Linux Kernel Documentation, \u0026ldquo;TCP Protocol Implementation\u0026rdquo; Stevens, W. R., \u0026ldquo;TCP/IP Illustrated, Volume 1: The Protocols\u0026rdquo; Corbet, J., \u0026ldquo;Large Pages in the Kernel\u0026rdquo; Alizadeh, M., et al., \u0026ldquo;Data Center TCP (DCTCP)\u0026rdquo; RFC 6455, \u0026ldquo;The WebSocket Protocol\u0026rdquo; 本文面向高频交易系统工程师和网络性能优化专家，提供了全面的WebSocket网络缓冲区优化技术指南。通过系统性的优化方法，可以显著提升交易系统的网络性能，实现极低延迟的通信要求。\n","date":"1 July 2025","permalink":"/blog/2025-07-01-tcp_perf/","section":"Blog","summary":"摘要 #本文深入探讨了高频交易系统中WebSocket连接的网络缓冲区优化技术，重点关注极低延迟性能优化。文章详细分析了TCP层优化、Socket缓冲区调优、大页内存应用以及网络栈各层次的性能优化策略，为构建微秒级延迟的交易系统提供了全面的技术指南。\n1. 引言 #在现代金融市场中，高频交易系统的竞争优势很大程度上取决于其网络栈的性能。WebSocket作为一种全双工通信协议，已成为高频交易系统连接交易所和市场数据提供商的重要技术。然而，标准WebSocket实现通常无法满足高频交易对极低延迟的苛刻要求，这些系统需要微秒级别的响应时间。\n本文旨在提供一个全面的网络缓冲区优化框架，从TCP底层协议到WebSocket应用层，系统性地探讨如何将延迟降至最低，尤其是通过优化网络缓冲区结构和内存访问模式。\n2. 网络栈基础概念与缓冲区架构 #2.1 TCP与Socket的关系与区别 #在深入优化之前，需要明确TCP与Socket这两个核心概念的区别与联系：\n概念层面 #TCP (传输控制协议):\n是一种通信协议，定义了数据如何在网络上可靠传输的规则 是OSI模型中的传输层协议 规定了如何建立连接、传输数据、处理丢包、确保顺序、流量控制等机制 是一组规则和标准，而非具体实现 Socket (套接字):\n是一个编程接口/抽象，是应用程序与网络协议交互的途径 可以看作是网络通信的\u0026quot;端点\u0026quot; 是操作系统提供的API，让应用程序能够使用网络功能 Socket不仅可以使用TCP，还可以使用UDP、Unix域等协议 比喻说明 #可以通过这个比喻理解：\nTCP是一种语言和交流规则(如中文+礼仪规范) Socket是允许人们使用这种语言交流的电话机 代码层面区别 #TCP体现为协议参数和行为：\n// 这些是TCP协议相关的选项 setsockopt(sockfd, IPPROTO_TCP, TCP_NODELAY, \u0026amp;flag, sizeof(flag)); setsockopt(sockfd, IPPROTO_TCP, TCP_KEEPALIVE, \u0026amp;flag, sizeof(flag)); Socket体现为创建和管理通信端点：\n// 创建套接字 int sockfd = socket(AF_INET, SOCK_STREAM, 0); // SOCK_STREAM指定TCP协议 // 设置套接字选项 setsockopt(sockfd, SOL_SOCKET, SO_RCVBUF, \u0026amp;rcvbuf, sizeof(rcvbuf)); // 连接、发送、接收数据 connect(sockfd, ...); send(sockfd, ...); recv(sockfd, ...); 缓冲区层面的区别 #TCP缓冲区:","title":"高频交易系统中的WebSocket网络缓冲区优化技术"},{"content":"目录 # 普通函数调用的开销 inline函数的优化 inline函数的局限性与权衡 示例代码与分析 什么时候使用inline函数？ 现代编译器的优化 总结 1. 普通函数调用的开销 #在C++中，普通函数调用涉及以下步骤，这些步骤会引入性能开销：\n栈帧的创建与销毁： 调用函数时，程序需要保存当前函数的状态（例如寄存器中的值），为被调用函数分配栈空间，设置栈帧（包括返回地址、参数传递、局部变量等）。 函数返回时，需要恢复调用者的栈帧状态。 这些操作涉及栈指针（SP）和基指针（BP）的调整，以及内存的读写操作。 参数传递与返回值： 参数需要通过栈或寄存器传递到被调用函数，这可能涉及内存拷贝（尤其是对于较大的结构体或对象）。 返回值也需要通过寄存器或内存传递回调用者。 跳转开销： 函数调用需要将程序计数器（PC）跳转到被调用函数的地址（通过call指令），返回时再跳回调用者（通过ret指令）。 这种跳转可能会导致CPU指令流水线的刷新（pipeline flush），尤其是在分支预测失败的情况下。 寄存器上下文保存/恢复： 调用函数可能导致寄存器内容的保存与恢复（例如调用者保存的寄存器或被调用者保存的寄存器），增加额外的指令开销。 这些步骤虽然在现代CPU上非常快，但对于频繁调用的函数（例如小型、简单函数），这些开销可能占函数执行时间的显著比例。\n2. inline函数的优化 #inline关键字建议编译器将函数的代码直接嵌入到调用处，而不是生成函数调用。这种内联（inlining）优化可以显著减少上述开销，原因如下：\n消除函数调用开销： 内联函数的代码直接嵌入到调用处，省去了栈帧的创建与销毁、参数传递、返回值的处理以及跳转指令。 程序无需执行call和ret指令，也避免了可能的流水线刷新。 优化机会增加： 编译器在优化阶段可以看到内联函数的完整代码上下文，可以应用更多的优化技术，例如： 常量折叠：如果内联函数的参数是常量，编译器可以直接计算结果。 死代码消除：如果内联函数中某些分支在调用上下文中永远不会执行，编译器可以剔除这些代码。 循环展开或指令重排：内联后，编译器可以更好地调整指令顺序，优化CPU缓存利用率或减少分支跳转。 减少指令数： 对于小型函数，函数调用的开销可能比函数体本身的执行时间还长。内联后，函数体的代码直接嵌入，减少了额外的指令（如push、pop、call、ret等）。 3. inline函数的局限性与权衡 #虽然inline函数通常更快，但它并非总是最佳选择，以下是一些需要注意的点：\n代码膨胀： 内联函数会将函数代码复制到每个调用点，如果函数体较大或调用点很多，可能导致生成的机器代码体积显著增加。这可能导致： 指令缓存（I-Cache）效率下降：代码体积过大可能无法完全放入CPU的指令缓存，增加缓存未命中（cache miss）。 可执行文件变大：增加编译后二进制文件的大小。 编译器的自主决定： inline关键字只是一个建议，现代编译器（如GCC、Clang、MSVC）会根据自己的优化策略决定是否内联。 编译器可能忽略inline关键字（例如函数体过大或过于复杂），也可能自动内联未标记为inline的函数（称为自动内联）。 例如，O2或O3优化级别下，编译器会根据函数的大小、调用频率等因素智能选择是否内联。 递归函数： 递归函数通常无法完全内联，因为内联会导致无限展开。编译器可能只内联部分递归调用（例如尾递归优化）。 调试难度： 内联函数的代码在调试时可能不可见，因为它们被展开后不再作为独立的函数存在，可能影响调试体验。 4. 示例代码与分析 #以下是一个简单的例子，展示普通函数调用与内联函数的差异：\n#include \u0026lt;iostream\u0026gt; // 普通函数 int add(int a, int b) { return a + b; } // 内联函数 inline int inline_add(int a, int b) { return a + b; } int main() { int x = 5, y = 10; int result1 = add(x, y); // 普通函数调用 int result2 = inline_add(x, y); // 内联函数调用 std::cout \u0026lt;\u0026lt; result1 \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; result2 \u0026lt;\u0026lt; std::endl; return 0; } 普通函数调用（add）：\n编译器生成call指令，跳转到add函数的地址。\n栈帧分配、参数传递、返回值处理等都会发生。\n假设add函数的机器码如下（伪汇编）： 调用时：\nadd: push ebp mov ebp, esp mov eax, [ebp + 8] ; 加载参数a add eax, [ebp + 12] ; 加上参数b mov esp, ebp pop ebp ret push 10 push 5 call add 内联函数调用（inline_add）：\n编译器直接将inline_add的代码嵌入到调用处，生成的伪汇编可能如下： 没有函数调用开销，指令数更少，执行效率更高。\nmov eax, 5 ; x add eax, 10 ; y 5. 什么时候使用inline函数？ # 小型、频繁调用的函数：例如简单的 getter/setter、数学运算函数（如上例中的add）。 性能敏感的代码：在高性能计算（如游戏引擎、实时系统）中，内联小型函数可以显著减少开销。 模板函数：C++模板函数通常定义在头文件中，隐式内联（implicit inline），因为它们需要在每个使用处生成具体代码。 不适合内联的场景：\n函数体过大（如包含复杂逻辑、循环或大量分支）。 函数调用频率低，内联带来的代码膨胀得不偿失。 需要跨模块调用（例如动态链接库中的函数通常无法内联）。 6. 现代编译器的优化 #现代C++编译器（如GCC、Clang）在优化级别较高时（如-O2或-O3）会自动内联小型函数，即使没有显式使用inline关键字。这是因为编译器通过全局优化和**链接时优化（LTO, Link-Time Optimization）**可以更好地分析函数调用模式，决定是否内联。因此，显式使用inline的必要性在现代C++中有所降低，但在以下情况下仍有意义：\n明确向编译器建议内联（例如在头文件中定义小型函数）。 提高代码可读性，表明函数设计目的是为了内联。 7. 总结 #inline函数比普通函数调用更快的主要原因是：\n消除了函数调用的开销（栈帧管理、参数传递、跳转指令等）。 提供了更多的编译时优化机会（常量折叠、死代码消除等）。 减少了指令执行数量，尤其对小型函数效果显著。 但内联并非万能，需权衡代码膨胀、调试难度等因素。在现代C++中，编译器的智能优化使得inline的作用更多是提示性而非强制性，开发者应根据实际性能需求和场景谨慎使用。\n如果你有更具体的场景或代码需要分析，或者想深入探讨某部分（如汇编代码或编译器优化细节），可以告诉我，我会进一步为你讲解！\n","date":"24 June 2025","permalink":"/blog/2025-06-24-inline_function_optimization/","section":"Blog","summary":"目录 # 普通函数调用的开销 inline函数的优化 inline函数的局限性与权衡 示例代码与分析 什么时候使用inline函数？ 现代编译器的优化 总结 1. 普通函数调用的开销 #在C++中，普通函数调用涉及以下步骤，这些步骤会引入性能开销：\n栈帧的创建与销毁： 调用函数时，程序需要保存当前函数的状态（例如寄存器中的值），为被调用函数分配栈空间，设置栈帧（包括返回地址、参数传递、局部变量等）。 函数返回时，需要恢复调用者的栈帧状态。 这些操作涉及栈指针（SP）和基指针（BP）的调整，以及内存的读写操作。 参数传递与返回值： 参数需要通过栈或寄存器传递到被调用函数，这可能涉及内存拷贝（尤其是对于较大的结构体或对象）。 返回值也需要通过寄存器或内存传递回调用者。 跳转开销： 函数调用需要将程序计数器（PC）跳转到被调用函数的地址（通过call指令），返回时再跳回调用者（通过ret指令）。 这种跳转可能会导致CPU指令流水线的刷新（pipeline flush），尤其是在分支预测失败的情况下。 寄存器上下文保存/恢复： 调用函数可能导致寄存器内容的保存与恢复（例如调用者保存的寄存器或被调用者保存的寄存器），增加额外的指令开销。 这些步骤虽然在现代CPU上非常快，但对于频繁调用的函数（例如小型、简单函数），这些开销可能占函数执行时间的显著比例。\n2. inline函数的优化 #inline关键字建议编译器将函数的代码直接嵌入到调用处，而不是生成函数调用。这种内联（inlining）优化可以显著减少上述开销，原因如下：\n消除函数调用开销： 内联函数的代码直接嵌入到调用处，省去了栈帧的创建与销毁、参数传递、返回值的处理以及跳转指令。 程序无需执行call和ret指令，也避免了可能的流水线刷新。 优化机会增加： 编译器在优化阶段可以看到内联函数的完整代码上下文，可以应用更多的优化技术，例如： 常量折叠：如果内联函数的参数是常量，编译器可以直接计算结果。 死代码消除：如果内联函数中某些分支在调用上下文中永远不会执行，编译器可以剔除这些代码。 循环展开或指令重排：内联后，编译器可以更好地调整指令顺序，优化CPU缓存利用率或减少分支跳转。 减少指令数： 对于小型函数，函数调用的开销可能比函数体本身的执行时间还长。内联后，函数体的代码直接嵌入，减少了额外的指令（如push、pop、call、ret等）。 3. inline函数的局限性与权衡 #虽然inline函数通常更快，但它并非总是最佳选择，以下是一些需要注意的点：\n代码膨胀： 内联函数会将函数代码复制到每个调用点，如果函数体较大或调用点很多，可能导致生成的机器代码体积显著增加。这可能导致： 指令缓存（I-Cache）效率下降：代码体积过大可能无法完全放入CPU的指令缓存，增加缓存未命中（cache miss）。 可执行文件变大：增加编译后二进制文件的大小。 编译器的自主决定： inline关键字只是一个建议，现代编译器（如GCC、Clang、MSVC）会根据自己的优化策略决定是否内联。 编译器可能忽略inline关键字（例如函数体过大或过于复杂），也可能自动内联未标记为inline的函数（称为自动内联）。 例如，O2或O3优化级别下，编译器会根据函数的大小、调用频率等因素智能选择是否内联。 递归函数： 递归函数通常无法完全内联，因为内联会导致无限展开。编译器可能只内联部分递归调用（例如尾递归优化）。 调试难度： 内联函数的代码在调试时可能不可见，因为它们被展开后不再作为独立的函数存在，可能影响调试体验。 4. 示例代码与分析 #以下是一个简单的例子，展示普通函数调用与内联函数的差异：\n#include \u0026lt;iostream\u0026gt; // 普通函数 int add(int a, int b) { return a + b; } // 内联函数 inline int inline_add(int a, int b) { return a + b; } int main() { int x = 5, y = 10; int result1 = add(x, y); // 普通函数调用 int result2 = inline_add(x, y); // 内联函数调用 std::cout \u0026lt;\u0026lt; result1 \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; result2 \u0026lt;\u0026lt; std::endl; return 0; } 普通函数调用（add）：","title":"C++中inline函数为何比普通函数调用更快：深入解析"},{"content":"目录 #1. 引言 # False Sharing概念介绍 文章研究目标 2. 测试设计概览 # 测试用例矩阵 测试方法说明 3. 样例运行结果 # 普通变量 + False Sharing 普通变量 + 无False Sharing 原子变量 + False Sharing 原子变量 + 无False Sharing 4. 现象分析与原理解释 # False Sharing如何降低性能 alignas(64)避免False Sharing的原理 cache miss升高的原因分析 原子变量性能开销分析 5. 深入理解缓存一致性与原子操作 # MESI缓存一致性协议详解 普通变量与原子变量的对比 原子变量 + False sharing的性能影响 CPU指令层面的差异 缓存一致性协议的影响 微架构层面的详细分析 6. 实战优化建议 # 不同场景的优化策略 7. 总结 # False Sharing的关键要点 核心结论 8. 参考资料与相关阅读 #9. 附录：完整测试代码 # 引言 #在现代多核处理器架构中，缓存系统在性能中扮演着至关重要的角色。然而，当多个线程同时操作位于同一缓存行（Cache Line）内的不同变量时，即使它们并未共享变量本身，也可能导致频繁的缓存一致性协议交互，这就是著名的性能杀手——False Sharing。\n本文基于 C++ 自定义测试程序，结合 perf 工具实测，从多个维度深入剖析 False Sharing 对性能的影响，并探讨：\n什么是真正的 False Sharing？ 为什么 alignas(64) 可以有效解决它？ 为什么 cache miss 增加了反而性能更好？ 原子变量在多线程场景下的额外开销从何而来？ 测试设计概览 #1. 测试用例矩阵 #我们定义了如下四种测试场景，每种以两个线程对不同变量进行独立写入，共执行 5,000,000 次：\n场景编号 变量类型 缓存行对齐 是否 False Sharing 1 普通变量 否 ✅ 是 2 普通变量 是（alignas 64） ❌ 否 3 原子变量 否 ✅ 是 4 原子变量 是 ❌ 否 每组测试运行 5 次，取平均执行时间、标准差，并结合 perf stat 采集 cache-misses 与 cache-references。\n样例运行结果（2线程，Intel Core） #普通变量 + False Sharing #平均耗时: 21.1ms cache-misses: 165,853 cache-references: 4,599,762 miss ratio: 3.6% 普通变量 + 无 False Sharing（alignas 64） #平均耗时: 12.5ms cache-misses: 157,214 cache-references: 400,988 miss ratio: 39.2% 原子变量 + False Sharing #平均耗时: 208.3ms cache-misses: 441,606 cache-references: 34,764,460 miss ratio: 1.27% 原子变量 + 无 False Sharing #平均耗时: 61.9ms cache-misses: 197,364 cache-references: 550,613 miss ratio: 35.8% 现象分析与原理解释 #1. False Sharing 如何降低性能？ #False Sharing 指的是：\n多个线程修改不同变量，但这些变量处在同一个 Cache Line 中，从而导致缓存一致性协议（如 MESI）不断引发 cache line 失效和重新加载。\n多核环境下的False Sharing # 多个线程运行在不同物理核心上，并发写入位于同一 cache line 上但彼此独立的变量。 尽管变量逻辑上没有共享，但由于它们共占一个 cache line，会导致 cache line 的所有权在核心之间频繁来回转移，从而引发 cache invalidation、总线通信增加、延迟升高，严重时导致程序性能显著下降。\n这导致两个问题：\n核间 Cache 不断争抢对该行的写权限 ⇒ 串行化写操作 store 被延迟或阻塞，CPU pipeline 被 stall 单核环境下的False Sharing # False Sharing的本质可以独立于MESI协议存在。即使在单核系统中，多线程修改同一cache line中的不同变量仍可能带来性能损耗。\n单核False Sharing的底层原理：\n// 单核多线程False Sharing过程： 线程A: 写入变量X → 缓存行变为dirty 线程B: 写入变量Y → 同一缓存行，触发写回主存 线程A: 再次写入X → 缓存行重新加载 线程B: 再次写入Y → 缓存行再次写回主存 性能损耗来源：\n缓存行写回开销：dirty cache line需要写回主存 缓存行重新加载：从主存重新加载数据 上下文切换开销：线程切换时的缓存状态变化 Store Buffer压力：写入操作在store buffer中排队 单核vs多核False Sharing对比：\n环境 主要开销 严重程度 原因 单核 缓存行写回/重载 中等 无核间通信，但有上下文切换 多核 MESI协议开销 严重 核间缓存一致性协议放大效应 关键点：并不是变量共享才会冲突，而是\u0026quot;共享缓存行\u0026quot;会导致\u0026quot;共享副作用\u0026quot;。\n2. 为什么 alignas(64) 可以避免 False Sharing？ #现代 CPU 通常以 64 字节为一个缓存行（Cache Line）单位进行数据同步。\n通过将结构体中的变量声明为：\nalignas(64) int counter1; 可强制编译器将该变量独立放在一个 cache line 中，避免了多个变量“落在同一个 cache line”的情况，从源头上避免了 False Sharing。\n3. 为什么 cache miss 反而升高了？ #结构体重排对齐后，每个变量占据 64 字节空间，而实际上只用了其中 4 字节（int）。\n局部性下降：\n对齐前 对齐后 多个变量挤在一起，访问时只需加载一行 每个变量都要加载独立的 cache line 结果：\nmemory locality 降低 ⇒ cache miss 上升 但核心间同步大幅减少 ⇒ store latency 降低 虽然强制对齐导致每个变量独占一个 cache line，cache miss 数量上升， 但这有效避免了 False Sharing 引起的缓存一致性冲突，消除了核心之间 因竞争 cache line 所带来的延迟，提升了整体并发写入性能。\n这是经典的“空间换时间”：提高 cache miss，换取减少核间冲突\n4. 原子变量为什么慢得多？ #std::atomic\u0026lt;T\u0026gt; 虽然保证线程安全，但它的读写操作涉及更严格的内存序语义：\n即使 memory_order_relaxed，也仍可能触发 CPU 的 LOCK 指令（或内存屏障） 多核之间必须对写入原子同步（无论是否 false sharing） 缓存一致性协议开销大（尤其在 false sharing 情况下） 因此原子变量在并发写场景中，自然较慢，尤其是在未对齐时，原子操作 + False Sharing 是性能灾难组合。\n深入理解缓存一致性与原子操作 #1. MESI缓存一致性协议详解 #现代CPU使用MESI协议来维护缓存一致性，每个缓存行有4种状态：\n// MESI状态详解 M (Modified): 该CPU独占且已修改，是唯一的\u0026#34;权威\u0026#34;副本 E (Exclusive): 该CPU独占但未修改，与内存一致 S (Shared): 多个CPU共享，都与内存一致 I (Invalid): 缓存行无效，需要重新获取 MESI协议的核心规则：\n同一时刻，只能有一个CPU持有Modified状态的缓存行 Modified状态意味着该CPU拥有最新的、权威的数据 其他CPU要访问时，必须从Modified状态的CPU获取数据 不能直接从内存读取，因为内存中的数据可能是过期的 这就是缓存一致性的核心：确保所有CPU看到的都是最新数据。\n2. 普通变量与原子变量的对比 #缓存行锁定机制 #// 普通变量写入时的缓存操作： CPU1: 获取缓存行 → 修改数据 → 标记Modified CPU2: 发现Invalid → 请求数据 → 获取 → 修改 → 标记Modified // 原子变量写入时的额外步骤： CPU1: 获取缓存行 → 锁定缓存行 → 原子修改 → 释放锁定 → 标记Modified CPU2: 发现Invalid → 请求数据 → 等待锁定释放 → 获取 → 锁定 → 原子修改... 总线争用加剧 #原子操作可能使用LOCK前缀，这会：\n锁定整个内存总线(老CPU) 或 缓存行(新CPU) 强制其他CPU等待 在False sharing场景下，这种等待被放大 3. 为什么原子变量 + False sharing是最坏情况 #// 原子变量 + False Sharing 的完整开销分解： 1. 原子操作本身的开销 (+100% 基准时间) 2. False Sharing导致的缓存冲突 (+200% 基准时间) 3. 原子操作与缓存冲突的相互放大 (+300% 基准时间) 4. 内存屏障阻止CPU优化 (+200% 基准时间) ---------------------------------------- 总开销： 800% 基准时间 4. CPU指令层面的差异 #普通变量的写入 ## 普通变量写入 (*target = i) mov %eax, (%rdi) # 一条简单的内存写入指令 原子变量写入 ## 原子变量写入 (atomic.store(i, relaxed)) lock mov %eax, (%rdi) # LOCK前缀确保原子性 # 或者使用更复杂的指令序列 mfence # 内存屏障（某些内存序要求） mov %eax, (%rdi) 5. 缓存一致性协议的影响 #当发生False sharing时，两种情况的区别：\n多核环境下的缓存冲突过程 #普通变量的缓存冲突过程： CPU1写入普通变量 → 缓存行状态变为Modified → CPU2需要写入时发现缓存行Invalid → CPU2从CPU1获取最新缓存行 → CPU2写入 → CPU1缓存行变为Invalid → 循环往复\n原子变量的缓存冲突过程： CPU1执行原子写入 → LOCK指令锁定总线/缓存行 → 缓存行状态变为Modified + 额外的原子性保证 → CPU2需要原子写入时不仅要获取缓存行，还要等待原子操作完成 → CPU2执行原子写入（可能需要额外的内存屏障） → 更复杂的缓存一致性状态转换\n单核环境下的缓存冲突过程 #普通变量的缓存冲突过程： 线程A写入变量X → 缓存行变为dirty → 线程B写入变量Y → 同一缓存行，触发写回主存 → 线程A再次写入X → 缓存行重新加载 → 线程B再次写入Y → 缓存行再次写回主存\n原子变量的缓存冲突过程： 线程A执行原子写入 → LOCK指令锁定缓存行 → 缓存行变为dirty + 原子性保证 → 线程B需要原子写入时等待锁定释放 → 线程B执行原子写入 → 缓存行写回主存\n单核vs多核开销对比：\n// 单核False Sharing开销分解： 1. 缓存行写回开销 (+30% 基准时间) 2. 缓存行重新加载开销 (+20% 基准时间) 3. 上下文切换时的缓存状态变化 (+15% 基准时间) 4. Store Buffer压力 (+10% 基准时间) ---------------------------------------- 总开销： 75% 基准时间 // 多核False Sharing开销分解： 1. 原子操作本身的开销 (+100% 基准时间) 2. False Sharing导致的缓存冲突 (+200% 基准时间) 3. 原子操作与缓存冲突的相互放大 (+300% 基准时间) 4. 内存屏障阻止CPU优化 (+200% 基准时间) ---------------------------------------- 总开销： 800% 基准时间 6. 微架构层面的详细分析 #Load-Store单元的压力 #// 普通变量：Load-Store单元可以并行处理多个操作 store1: mov %eax, 0(%rdi) // 可以与下面的指令并行 store2: mov %ebx, 4(%rdi) store3: mov %ecx, 8(%rdi) // 原子变量：必须串行化执行 atomic_store1: lock mov %eax, 0(%rdi) // 必须等待完成 atomic_store2: lock mov %ebx, 4(%rdi) // 才能执行下一个 atomic_store3: lock mov %ecx, 8(%rdi) 实战优化建议 #多核环境优化策略 # 场景 是否推荐 原因 多线程频繁写入不同变量 ✅ 强烈推荐使用 alignas(64) 分离变量 避免 False Sharing 少量变量单线程写入 ❌ 不建议使用 alignas(64) 浪费空间，locality 下降 原子变量写频繁出现 ✅ 保证对齐，控制线程数 避免多核冲突 多线程读写混合 ⚠️ 视具体访问模式优化 原子 or lock free 结构更适合 单核环境优化策略 # 场景 是否推荐 原因 多线程频繁写入不同变量 ⚠️ 可考虑使用 alignas(64) 分离变量 False Sharing影响较小，但仍有收益 少量变量单线程写入 ❌ 不建议使用 alignas(64) 浪费空间，收益有限 原子变量写频繁出现 ✅ 建议对齐，但优先级较低 单核下原子操作开销较小 多线程读写混合 ✅ 推荐使用无锁数据结构 避免上下文切换开销 跨平台优化建议 #// 通用优化策略（适用于单核和多核） struct ThreadSafeData { #ifdef MULTI_CORE_OPTIMIZATION alignas(64) int counter1; // 多核环境强烈推荐 alignas(64) int counter2; #else int counter1; // 单核环境可选 int counter2; #endif }; 总结 #False Sharing 是现代多核编程中最隐蔽但杀伤力极大的性能陷阱之一。它不会引起程序逻辑错误，却会严重拖慢系统性能。\n核心结论 # False Sharing的本质独立于MESI协议：即使在单核系统中，多线程修改同一cache line中的不同变量仍可能带来性能损耗。\n单核vs多核影响程度不同：\n单核环境：主要开销来自缓存行写回/重载和上下文切换 多核环境：MESI协议放大效应导致更严重的性能损失 优化策略的差异化：\n多核环境：强烈推荐使用alignas(64)避免False Sharing 单核环境：可考虑使用alignas(64)，但收益相对较小 对齐变量、避免多个线程写入同一 Cache Line，是处理并发时的基本优化操作。\n最后记住这一句话： # \u0026ldquo;不是你在共享变量，而是你在共享缓存行。\u0026rdquo;\n参考资料与相关阅读 # LockFree事件总线性能优化案例：详细分析了False Sharing在事件总线中的影响及优化方法 高性能本地内存订单管理设计：讨论了订单管理系统中如何避免False Sharing问题 共享内存多进程通信优化：探讨了在共享内存场景中使用缓存行对齐避免False Sharing 模板类实现中的缓存优化：展示了在模板类设计中如何考虑缓存行对齐 附录：完整测试代码 ##include \u0026lt;iostream\u0026gt; #include \u0026lt;thread\u0026gt; #include \u0026lt;chrono\u0026gt; #include \u0026lt;vector\u0026gt; #include \u0026lt;atomic\u0026gt; #include \u0026lt;iomanip\u0026gt; #include \u0026lt;cstring\u0026gt; #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;algorithm\u0026gt; // 为std::min_element和std::max_element添加 #include \u0026lt;cmath\u0026gt; // 为std::sqrt添加 // 测试参数 constexpr int NUM_THREADS = 2; constexpr int ITERATIONS = 5000000; constexpr int NUM_RUNS = 5; // 每个测试运行多次取平均值 // 不同的测试用例结构体 struct NormalFalseSharing { int counter1; int counter2; int counter3; int counter4; }; struct NormalNoFalseSharing { alignas(64) int counter1; alignas(64) int counter2; alignas(64) int counter3; alignas(64) int counter4; }; struct AtomicFalseSharing { std::atomic\u0026lt;int\u0026gt; counter1{0}; std::atomic\u0026lt;int\u0026gt; counter2{0}; std::atomic\u0026lt;int\u0026gt; counter3{0}; std::atomic\u0026lt;int\u0026gt; counter4{0}; }; struct AtomicNoFalseSharing { alignas(64) std::atomic\u0026lt;int\u0026gt; counter1{0}; alignas(64) std::atomic\u0026lt;int\u0026gt; counter2{0}; alignas(64) std::atomic\u0026lt;int\u0026gt; counter3{0}; alignas(64) std::atomic\u0026lt;int\u0026gt; counter4{0}; }; // 辅助函数 template\u0026lt;typename TimePoint\u0026gt; double get_duration_ms(TimePoint start, TimePoint end) { return std::chrono::duration\u0026lt;double, std::milli\u0026gt;(end - start).count(); } // 清理系统状态的函数 void clear_system_state() { // 强制进行垃圾回收和缓存刷新 std::this_thread::sleep_for(std::chrono::milliseconds(100)); // 分配并释放一些内存来\u0026#34;污染\u0026#34;缓存 volatile char* temp = new char[1024 * 1024]; // 1MB for (int i = 0; i \u0026lt; 1024 * 1024; i += 64) { temp[i] = i \u0026amp; 0xFF; } delete[] temp; std::this_thread::sleep_for(std::chrono::milliseconds(50)); } // 单独的测试函数 class IndependentTest { public: enum TestType { NORMAL_FALSE_SHARING, NORMAL_NO_FALSE_SHARING, ATOMIC_FALSE_SHARING, ATOMIC_NO_FALSE_SHARING }; static double run_single_test(TestType type) { switch (type) { case NORMAL_FALSE_SHARING: return test_normal_false_sharing(); case NORMAL_NO_FALSE_SHARING: return test_normal_no_false_sharing(); case ATOMIC_FALSE_SHARING: return test_atomic_false_sharing(); case ATOMIC_NO_FALSE_SHARING: return test_atomic_no_false_sharing(); } return 0.0; } private: static double test_normal_false_sharing() { NormalFalseSharing data{}; auto start = std::chrono::high_resolution_clock::now(); std::vector\u0026lt;std::thread\u0026gt; threads; for (int t = 0; t \u0026lt; NUM_THREADS; ++t) { threads.emplace_back([\u0026amp;data, t]() { int* target = nullptr; switch(t) { case 0: target = \u0026amp;data.counter1; break; case 1: target = \u0026amp;data.counter2; break; case 2: target = \u0026amp;data.counter3; break; case 3: target = \u0026amp;data.counter4; break; } for (int i = 0; i \u0026lt; ITERATIONS; ++i) { *target = i; } }); } for (auto\u0026amp; thread : threads) { thread.join(); } auto end = std::chrono::high_resolution_clock::now(); return get_duration_ms(start, end); } static double test_normal_no_false_sharing() { NormalNoFalseSharing data{}; auto start = std::chrono::high_resolution_clock::now(); std::vector\u0026lt;std::thread\u0026gt; threads; for (int t = 0; t \u0026lt; NUM_THREADS; ++t) { threads.emplace_back([\u0026amp;data, t]() { int* target = nullptr; switch(t) { case 0: target = \u0026amp;data.counter1; break; case 1: target = \u0026amp;data.counter2; break; case 2: target = \u0026amp;data.counter3; break; case 3: target = \u0026amp;data.counter4; break; } for (int i = 0; i \u0026lt; ITERATIONS; ++i) { *target = i; } }); } for (auto\u0026amp; thread : threads) { thread.join(); } auto end = std::chrono::high_resolution_clock::now(); return get_duration_ms(start, end); } static double test_atomic_false_sharing() { AtomicFalseSharing data{}; auto start = std::chrono::high_resolution_clock::now(); std::vector\u0026lt;std::thread\u0026gt; threads; for (int t = 0; t \u0026lt; NUM_THREADS; ++t) { threads.emplace_back([\u0026amp;data, t]() { std::atomic\u0026lt;int\u0026gt;* target = nullptr; switch(t) { case 0: target = \u0026amp;data.counter1; break; case 1: target = \u0026amp;data.counter2; break; case 2: target = \u0026amp;data.counter3; break; case 3: target = \u0026amp;data.counter4; break; } for (int i = 0; i \u0026lt; ITERATIONS; ++i) { target-\u0026gt;store(i, std::memory_order_relaxed); } }); } for (auto\u0026amp; thread : threads) { thread.join(); } auto end = std::chrono::high_resolution_clock::now(); return get_duration_ms(start, end); } static double test_atomic_no_false_sharing() { AtomicNoFalseSharing data{}; auto start = std::chrono::high_resolution_clock::now(); std::vector\u0026lt;std::thread\u0026gt; threads; for (int t = 0; t \u0026lt; NUM_THREADS; ++t) { threads.emplace_back([\u0026amp;data, t]() { std::atomic\u0026lt;int\u0026gt;* target = nullptr; switch(t) { case 0: target = \u0026amp;data.counter1; break; case 1: target = \u0026amp;data.counter2; break; case 2: target = \u0026amp;data.counter3; break; case 3: target = \u0026amp;data.counter4; break; } for (int i = 0; i \u0026lt; ITERATIONS; ++i) { target-\u0026gt;store(i, std::memory_order_relaxed); } }); } for (auto\u0026amp; thread : threads) { thread.join(); } auto end = std::chrono::high_resolution_clock::now(); return get_duration_ms(start, end); } }; // 统计辅助函数 struct TestResult { double min_time; double max_time; double avg_time; double stddev; TestResult(const std::vector\u0026lt;double\u0026gt;\u0026amp; times) { min_time = *std::min_element(times.begin(), times.end()); max_time = *std::max_element(times.begin(), times.end()); double sum = 0.0; for (double t : times) sum += t; avg_time = sum / times.size(); double variance = 0.0; for (double t : times) { variance += (t - avg_time) * (t - avg_time); } stddev = std::sqrt(variance / times.size()); } }; void run_test_suite(IndependentTest::TestType type, const std::string\u0026amp; name) { std::cout \u0026lt;\u0026lt; \u0026#34;\\n=== \u0026#34; \u0026lt;\u0026lt; name \u0026lt;\u0026lt; \u0026#34; ===\\n\u0026#34;; std::vector\u0026lt;double\u0026gt; times; for (int run = 0; run \u0026lt; NUM_RUNS; ++run) { std::cout \u0026lt;\u0026lt; \u0026#34;运行 \u0026#34; \u0026lt;\u0026lt; (run + 1) \u0026lt;\u0026lt; \u0026#34;/\u0026#34; \u0026lt;\u0026lt; NUM_RUNS \u0026lt;\u0026lt; \u0026#34;... \u0026#34;; std::cout.flush(); // 清理系统状态 clear_system_state(); // 运行测试 double time = IndependentTest::run_single_test(type); times.push_back(time); std::cout \u0026lt;\u0026lt; std::fixed \u0026lt;\u0026lt; std::setprecision(1) \u0026lt;\u0026lt; time \u0026lt;\u0026lt; \u0026#34;ms\\n\u0026#34;; } TestResult result(times); std::cout \u0026lt;\u0026lt; \u0026#34;结果统计:\\n\u0026#34;; std::cout \u0026lt;\u0026lt; \u0026#34; 平均: \u0026#34; \u0026lt;\u0026lt; std::fixed \u0026lt;\u0026lt; std::setprecision(1) \u0026lt;\u0026lt; result.avg_time \u0026lt;\u0026lt; \u0026#34;ms\\n\u0026#34;; std::cout \u0026lt;\u0026lt; \u0026#34; 最小: \u0026#34; \u0026lt;\u0026lt; result.min_time \u0026lt;\u0026lt; \u0026#34;ms\\n\u0026#34;; std::cout \u0026lt;\u0026lt; \u0026#34; 最大: \u0026#34; \u0026lt;\u0026lt; result.max_time \u0026lt;\u0026lt; \u0026#34;ms\\n\u0026#34;; std::cout \u0026lt;\u0026lt; \u0026#34; 标准差: \u0026#34; \u0026lt;\u0026lt; std::setprecision(2) \u0026lt;\u0026lt; result.stddev \u0026lt;\u0026lt; \u0026#34;ms\\n\u0026#34;; std::cout \u0026lt;\u0026lt; \u0026#34; 变异系数: \u0026#34; \u0026lt;\u0026lt; std::setprecision(1) \u0026lt;\u0026lt; (result.stddev / result.avg_time * 100) \u0026lt;\u0026lt; \u0026#34;%\\n\u0026#34;; } void show_system_info() { std::cout \u0026lt;\u0026lt; \u0026#34;=== 系统信息 ===\\n\u0026#34;; std::cout \u0026lt;\u0026lt; \u0026#34;CPU核心数: \u0026#34; \u0026lt;\u0026lt; std:🧵:hardware_concurrency() \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; std::cout \u0026lt;\u0026lt; \u0026#34;sizeof(int): \u0026#34; \u0026lt;\u0026lt; sizeof(int) \u0026lt;\u0026lt; \u0026#34; 字节\\n\u0026#34;; std::cout \u0026lt;\u0026lt; \u0026#34;sizeof(std::atomic\u0026lt;int\u0026gt;): \u0026#34; \u0026lt;\u0026lt; sizeof(std::atomic\u0026lt;int\u0026gt;) \u0026lt;\u0026lt; \u0026#34; 字节\\n\u0026#34;; std::cout \u0026lt;\u0026lt; \u0026#34;缓存行大小: 通常为64字节\\n\u0026#34;; std::cout \u0026lt;\u0026lt; \u0026#34;测试参数: \u0026#34; \u0026lt;\u0026lt; NUM_THREADS \u0026lt;\u0026lt; \u0026#34; 线程, \u0026#34; \u0026lt;\u0026lt; ITERATIONS \u0026lt;\u0026lt; \u0026#34; 次迭代, \u0026#34; \u0026lt;\u0026lt; NUM_RUNS \u0026lt;\u0026lt; \u0026#34; 次运行\\n\u0026#34;; } void show_memory_layout() { std::cout \u0026lt;\u0026lt; \u0026#34;\\n=== 内存布局验证 ===\\n\u0026#34;; NormalFalseSharing nfs{}; std::cout \u0026lt;\u0026lt; \u0026#34;普通变量(False Sharing):\\n\u0026#34;; std::cout \u0026lt;\u0026lt; \u0026#34; counter1: \u0026#34; \u0026lt;\u0026lt; \u0026amp;nfs.counter1 \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; std::cout \u0026lt;\u0026lt; \u0026#34; counter2: \u0026#34; \u0026lt;\u0026lt; \u0026amp;nfs.counter2 \u0026lt;\u0026lt; \u0026#34; (距离: \u0026#34; \u0026lt;\u0026lt; (char*)\u0026amp;nfs.counter2 - (char*)\u0026amp;nfs.counter1 \u0026lt;\u0026lt; \u0026#34; 字节)\\n\u0026#34;; NormalNoFalseSharing nnfs{}; std::cout \u0026lt;\u0026lt; \u0026#34;普通变量(无False Sharing):\\n\u0026#34;; std::cout \u0026lt;\u0026lt; \u0026#34; counter1: \u0026#34; \u0026lt;\u0026lt; \u0026amp;nnfs.counter1 \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; std::cout \u0026lt;\u0026lt; \u0026#34; counter2: \u0026#34; \u0026lt;\u0026lt; \u0026amp;nnfs.counter2 \u0026lt;\u0026lt; \u0026#34; (距离: \u0026#34; \u0026lt;\u0026lt; (char*)\u0026amp;nnfs.counter2 - (char*)\u0026amp;nnfs.counter1 \u0026lt;\u0026lt; \u0026#34; 字节)\\n\u0026#34;; } int main(int argc, char* argv[]) { if (argc != 2) { std::cout \u0026lt;\u0026lt; \u0026#34;用法: \u0026#34; \u0026lt;\u0026lt; argv[0] \u0026lt;\u0026lt; \u0026#34; \u0026lt;test_number\u0026gt;\\n\u0026#34;; std::cout \u0026lt;\u0026lt; \u0026#34; 1: 普通变量 + False Sharing\\n\u0026#34;; std::cout \u0026lt;\u0026lt; \u0026#34; 2: 普通变量 + 无False Sharing\\n\u0026#34;; std::cout \u0026lt;\u0026lt; \u0026#34; 3: 原子变量 + False Sharing\\n\u0026#34;; std::cout \u0026lt;\u0026lt; \u0026#34; 4: 原子变量 + 无False Sharing\\n\u0026#34;; std::cout \u0026lt;\u0026lt; \u0026#34; 0: 显示系统信息和内存布局\\n\u0026#34;; return 1; } int test_num = std::atoi(argv[1]); if (test_num == 0) { show_system_info(); show_memory_layout(); return 0; } show_system_info(); switch (test_num) { case 1: run_test_suite(IndependentTest::NORMAL_FALSE_SHARING, \u0026#34;普通变量 + False Sharing\u0026#34;); break; case 2: run_test_suite(IndependentTest::NORMAL_NO_FALSE_SHARING, \u0026#34;普通变量 + 无False Sharing\u0026#34;); break; case 3: run_test_suite(IndependentTest::ATOMIC_FALSE_SHARING, \u0026#34;原子变量 + False Sharing\u0026#34;); break; case 4: run_test_suite(IndependentTest::ATOMIC_NO_FALSE_SHARING, \u0026#34;原子变量 + 无False Sharing\u0026#34;); break; default: std::cout \u0026lt;\u0026lt; \u0026#34;无效的测试编号: \u0026#34; \u0026lt;\u0026lt; test_num \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; return 1; } return 0; } ","date":"23 June 2025","permalink":"/blog/2025-06-23-cache_false_sharing_analysis/","section":"Blog","summary":"目录 #1. 引言 # False Sharing概念介绍 文章研究目标 2. 测试设计概览 # 测试用例矩阵 测试方法说明 3. 样例运行结果 # 普通变量 + False Sharing 普通变量 + 无False Sharing 原子变量 + False Sharing 原子变量 + 无False Sharing 4. 现象分析与原理解释 # False Sharing如何降低性能 alignas(64)避免False Sharing的原理 cache miss升高的原因分析 原子变量性能开销分析 5. 深入理解缓存一致性与原子操作 # MESI缓存一致性协议详解 普通变量与原子变量的对比 原子变量 + False sharing的性能影响 CPU指令层面的差异 缓存一致性协议的影响 微架构层面的详细分析 6. 实战优化建议 # 不同场景的优化策略 7. 总结 # False Sharing的关键要点 核心结论 8. 参考资料与相关阅读 #9. 附录：完整测试代码 # 引言 #在现代多核处理器架构中，缓存系统在性能中扮演着至关重要的角色。然而，当多个线程同时操作位于同一缓存行（Cache Line）内的不同变量时，即使它们并未共享变量本身，也可能导致频繁的缓存一致性协议交互，这就是著名的性能杀手——False Sharing。","title":"深入理解 False Sharing：实测原子操作与缓存行对齐对性能的影响"},{"content":"摘要 #本文分析了C++原子操作中不同内存序(memory ordering)对性能的影响，特别是比较了默认的顺序一致性(seq_cst)与宽松(relaxed)内存序在x86-64架构上的性能差异。通过实验测试、性能分析和汇编代码检查，我们发现即使在内存模型较强的x86架构上，不同内存序的选择仍然会产生可测量的性能差异。\n1. 实验设计 #1.1 测试程序 #我们设计了两个版本的测试程序，它们在固定时间内执行原子变量的读取和计数操作，唯一区别是原子变量读取时使用的内存序不同：\nseq_cst版本 (默认内存序):\nvoid worker_seq_cst() { while (running_) { // 默认使用 seq_cst counter_.fetch_add(1, std::memory_order_relaxed); busy_loop(); } } relaxed版本 (显式指定宽松内存序):\nvoid worker_relaxed_load() { while (running_.load(std::memory_order_relaxed)) { counter_.fetch_add(1, std::memory_order_relaxed); busy_loop(); } } 1.2 编译与执行环境 #测试程序使用以下命令编译：\ng++ -std=c++11 -O0 -pthread atomic_test_seq_cst.cpp -o test_gcc_seq_cst g++ -std=c++11 -O0 -pthread atomic_test_relaxed.cpp -o test_gcc_relaxed 每个程序运行5秒钟，记录在此期间完成的操作次数。同时使用perf工具收集性能数据：\nperf record -e cpu-clock:pppH ./test_gcc_seq_cst perf record -e cpu-clock:pppH ./test_gcc_relaxed 2. 实验结果 #2.1 执行计数结果 # 版本 操作计数 seq_cst 26,494,108 relaxed 26,660,082 性能差异：\n绝对差异：165,974次操作 相对提升：0.63% 2.2 perf分析结果 #seq_cst版本:\nSamples: 4K of event \u0026#39;cpu-clock:pppH\u0026#39;, Event count (approx.): 4994994990 Children Self Command Symbol 95.23% 95.21% test_gcc_seq_cs busy_loop 99.44% 2.91% test_gcc_seq_cs worker_seq_cst 1.06% 0.84% test_gcc_seq_cs std::atomic\u0026lt;bool\u0026gt;::operator bool 0.54% 0.54% test_gcc_seq_cs std::__is_constant_evaluated relaxed版本:\nSamples: 4K of event \u0026#39;cpu-clock:pppH\u0026#39;, Event count (approx.): 4991991987 Children Self Command Symbol 99.22% 3.23% test_gcc_relaxe worker_relaxed_load 94.61% 94.61% test_gcc_relaxe busy_loop 1.64% 1.30% test_gcc_relaxe std::atomic\u0026lt;bool\u0026gt;::load 0.42% 0.42% test_gcc_relaxe std::__is_constant_evaluated 与relaxed版本的比较分析 将这些数据与relaxed版本对比，我们可以看到几个关键差异：\n原子操作函数：\nseq_cst: std::atomic\u0026lt;bool\u0026gt;::operator bool 占用 1.06%（Self: 0.84%） relaxed: std::atomic\u0026lt;bool\u0026gt;::load 占用 1.64%（Self: 1.30%） worker函数自身开销：\nseq_cst: worker_seq_cst 自身占用 2.91% relaxed: worker_relaxed_load 自身占用 3.23% busy_loop占比：\nseq_cst: busy_loop Children 占用 95.23%，Self 占用 95.21% relaxed: busy_loop Children 占用 94.61%，Self 占用 94.61% 性能差异解释 基于这些数据，我们可以解释seq_cst和relaxed版本之间0.63%的性能差异：\n函数调用路径不同：\nseq_cst版本调用 operator bool()，这是一个隐式转换函数 relaxed版本直接调用 load() 函数并明确指定内存序 内部实现差异：\noperator bool() 内部可能包含额外的检查或转换逻辑 load() 函数可能有更直接的实现路径 编译器优化差异：\n显式指定 memory_order_relaxed 可能允许编译器进行更多优化 默认的 seq_cst 可能限制了某些重排序优化 CPU微架构影响：\n不同的函数调用路径可能导致不同的指令缓存和分支预测行为 seq_cst 语义可能隐含地影响CPU的投机执行策略 2.3 关键汇编代码对比 #relaxed版本:\n.L27: mov esi, 0 # memory_order_relaxed参数(0) mov edi, OFFSET FLAT:running_ # this指针 call std::atomic\u0026lt;bool\u0026gt;::load(std::memory_order) const test al, al jne .L29 seq_cst版本:\n.L31: mov edi, OFFSET FLAT:running_ # this指针 call std::atomic\u0026lt;bool\u0026gt;::operator bool() const test al, al jne .L33 2.4 O0优化下的汇编差异分析 #以下为 gcc 15.1，O0 优化下的关键汇编片段： Godbolt 在线汇编工具\nworker_relaxed_load(): ... mov esi, 0 mov edi, OFFSET FLAT:running_ call std::atomic\u0026lt;bool\u0026gt;::load(std::memory_order) const test al, al jne .L19 ... worker_seq_cst(): ... mov edi, OFFSET FLAT:running_ call std::atomic\u0026lt;bool\u0026gt;::operator bool() const test al, al jne .L23 ... 差异分析 # 条件判断方式不同\nworker_relaxed_load 显式传递 memory_order 参数（esi, 0），调用 load(std::memory_order) const。 worker_seq_cst 直接调用 operator bool() const，不传递内存序参数。 函数调用路径\nrelaxed 版本的 while 条件是 running_.load(std::memory_order_relaxed)，对应调用 load，并传递内存序参数。 seq_cst 版本的 while 条件是 while (running_)，对应调用 operator bool()，其内部默认 memory_order_seq_cst。 核心原子操作一致\n两个版本的 counter_.fetch_add(1, std::memory_order_relaxed) 都对应 lock xadd 指令，说明增操作本身无差异。 busy_loop实现一致\n两个版本都调用 busy_loop()，其实现完全相同。 O0下的额外开销\n由于O0未做优化，函数调用、栈帧分配等指令较多，真实差异主要体现在条件判断的函数调用路径。 总结 # O0优化下，worker_relaxed_load 和 worker_seq_cst 的主要差异体现在对 running_ 的判断方式：一个是显式 load，一个是隐式 operator bool()。 这与perf分析和高优化级别下的结论一致：两者的核心原子操作实现无差异，性能差异主要来自于条件判断的函数调用路径和编译器对内存序的处理。 在O0下，所有函数调用和参数传递都被完整保留，进一步放大了调用路径的差异。 3. 分析与讨论 #3.1 性能差异分析 #实验结果显示，relaxed版本在相同时间内完成了更多操作(0.63%的提升)。这个差异虽小，但在高频操作场景中具有实际意义。\n有趣的是，perf数据显示relaxed版本在原子操作上花费了更大比例的CPU时间(1.64% vs 1.06%)，但总体吞吐量仍然更高。这表明：\nrelaxed版本虽然在原子操作上花费了更多时间比例，但每次操作的实际开销更小 seq_cst版本可能有隐含的开销，导致整体执行效率略低 需要注意的是，perf数据显示relaxed版本的std::atomic::load的自耗（Self）反而比operator bool更高，这说明单次load的开销并不一定更低。整体吞吐量略高，可能是因为编译器对relaxed路径做了更激进的优化，或者调用链更短、分支预测效果更好。汇编层面，两者的主要差异仅在于load的调用方式和参数传递，实际指令数量和复杂度差异极小。\n3.2 汇编代码差异分析 #汇编代码分析揭示了性能差异的根本原因：\n函数调用差异：\nseq_cst版本调用operator bool() relaxed版本调用load(std::memory_order_relaxed) 相同点：\n两个版本的busy_loop实现完全相同 两个版本的counter_.fetch_add操作都使用相同的lock xadd指令 关键发现：\n在O2优化级别下，两个版本的核心原子操作指令相同 差异主要来自函数调用路径，而非原子操作本身的实现 3.3 内存序对性能的影响机制 #在x86-64架构上，load操作本身不需要额外的内存屏障，无论是seq_cst还是relaxed。然而，差异可能来自以下几个方面：\n函数调用开销：\noperator bool()可能有额外的转换逻辑 不同函数可能有不同的内联和优化策略 编译器优化：\n显式指定memory_order_relaxed可能允许编译器进行更多优化 默认的seq_cst可能限制了某些重排序优化 CPU微架构影响：\n不同的函数调用路径可能导致不同的缓存行为和分支预测效果 seq_cst可能隐含地影响CPU的投机执行策略 4. 结论与实践建议 #本研究表明，即使在x86-64这样的强内存模型架构上，选择适当的内存序仍然可以带来可测量的性能提升。虽然差异不大(0.63%)，但在高性能计算或高频交易系统中，这种积累的差异可能具有实际意义。\n补充说明： 通过perf和汇编分析可以确认，relaxed和默认seq_cst的主要性能差异，来源于对原子变量running_的判断方式（即load的调用路径和参数传递），而非核心原子操作本身。两者的性能差异极小（本实验约0.6%），仅在极高频场景下才有实际意义。实际开发中，只有在不需要强内存序保证时，才建议使用relaxed，并应以实际测量为准。\n实践建议：\n在不需要强内存序保证的场景中，使用relaxed内存序可以获得更好的性能 性能关键路径上的原子操作应当仔细选择适当的内存序 即使在x86架构上，内存序的选择也会影响性能，不应被忽视 在进行性能优化时，应当通过实际测量来验证不同内存序的影响 5. 未来工作 #未来可以扩展本研究，探索以下方向：\n在不同CPU架构(如ARM、POWER)上进行类似测试 测试多线程竞争场景下不同内存序的性能差异 分析不同编译器和优化级别对内存序性能的影响 探索更复杂的原子操作模式(如RMW操作)下内存序的性能影响 附录：完整测试代码 ##include \u0026lt;atomic\u0026gt; #include \u0026lt;thread\u0026gt; #include \u0026lt;iostream\u0026gt; #include \u0026lt;chrono\u0026gt; // 原子变量 std::atomic\u0026lt;bool\u0026gt; running_{true}; std::atomic\u0026lt;int\u0026gt; counter_{0}; // 模拟忙等，避免被优化 inline void busy_loop() { for (volatile int i = 0; i \u0026lt; 100; ++i); } // 默认版本：隐式 seq_cst void worker_seq_cst() { while (running_) { // 默认 seq_cst counter_.fetch_add(1, std::memory_order_relaxed); // 只测 load busy_loop(); } } // relaxed load 版本 void worker_relaxed_load() { while (running_.load(std::memory_order_relaxed)) { counter_.fetch_add(1, std::memory_order_relaxed); // 保持一致 busy_loop(); } } int main() { running_ = true; counter_ = 0; std::thread t(worker_seq_cst); // 运行足够长时间以便perf收集数据 std::this_thread::sleep_for(std::chrono::seconds(5)); running_.store(false); t.join(); std::cout \u0026lt;\u0026lt; \u0026#34;完成测试，计数: \u0026#34; \u0026lt;\u0026lt; counter_.load() \u0026lt;\u0026lt; std::endl; return 0; } ","date":"21 June 2025","permalink":"/blog/2025-06-21-memory_order_performance_analysis/","section":"Blog","summary":"摘要 #本文分析了C++原子操作中不同内存序(memory ordering)对性能的影响，特别是比较了默认的顺序一致性(seq_cst)与宽松(relaxed)内存序在x86-64架构上的性能差异。通过实验测试、性能分析和汇编代码检查，我们发现即使在内存模型较强的x86架构上，不同内存序的选择仍然会产生可测量的性能差异。\n1. 实验设计 #1.1 测试程序 #我们设计了两个版本的测试程序，它们在固定时间内执行原子变量的读取和计数操作，唯一区别是原子变量读取时使用的内存序不同：\nseq_cst版本 (默认内存序):\nvoid worker_seq_cst() { while (running_) { // 默认使用 seq_cst counter_.fetch_add(1, std::memory_order_relaxed); busy_loop(); } } relaxed版本 (显式指定宽松内存序):\nvoid worker_relaxed_load() { while (running_.load(std::memory_order_relaxed)) { counter_.fetch_add(1, std::memory_order_relaxed); busy_loop(); } } 1.2 编译与执行环境 #测试程序使用以下命令编译：\ng++ -std=c++11 -O0 -pthread atomic_test_seq_cst.cpp -o test_gcc_seq_cst g++ -std=c++11 -O0 -pthread atomic_test_relaxed.cpp -o test_gcc_relaxed 每个程序运行5秒钟，记录在此期间完成的操作次数。同时使用perf工具收集性能数据：\nperf record -e cpu-clock:pppH ./test_gcc_seq_cst perf record -e cpu-clock:pppH ./test_gcc_relaxed 2. 实验结果 #2.1 执行计数结果 # 版本 操作计数 seq_cst 26,494,108 relaxed 26,660,082 性能差异：","title":"C++原子操作内存序性能分析：seq_cst vs relaxed"},{"content":"概述 #本文针对现有的LockFreeEventBus实现进行深入的性能分析和优化建议。当前实现采用了以下核心设计：\n事件与处理函数映射：使用std::unordered_map建立事件类型到处理函数的映射关系 处理函数存储：使用std::vector存储每种事件类型对应的处理函数列表 事件分发机制：在高频调用场景下使用RTTI（运行时类型识别）机制进行事件分发 内存管理：大量使用智能指针进行事件对象的生命周期管理 虽然这种设计在功能上完整可靠，但在高频交易等对延迟极度敏感的场景下存在显著的性能瓶颈。本文将详细分析这些瓶颈的根本原因，并提出针对性的优化建议。\n注意：本文是深入理解无锁队列：从原理到实践的完整指南的配套性能分析文章，建议先阅读该文章了解无锁队列的基本原理。\n1. 核心工作机制 #1.1 基本架构 #LockFreeEventBus采用无锁队列和工作线程的组合，实现事件的异步处理：\nclass LockFreeEventBus { private: LockFreeQueueEvent\u0026lt;std::shared_ptr\u0026lt;Event\u0026gt;\u0026gt; event_queue_; std::unordered_map\u0026lt;std::type_index, std::vector\u0026lt;std::function\u0026lt;void(std::shared_ptr\u0026lt;Event\u0026gt;)\u0026gt;\u0026gt;\u0026gt; handlers_; std::atomic\u0026lt;bool\u0026gt; running_; std::thread worker_thread_; // ... }; 关键组件：\nevent_queue_：无锁队列，存储待处理事件 handlers_：以事件类型为键的处理函数映射表 worker_thread_：单独工作线程，循环处理事件队列 1.2 事件发布流程 #void publish(std::shared_ptr\u0026lt;Event\u0026gt; event) { // 设置发布时间 event-\u0026gt;setPublishTime(std::chrono::high_resolution_clock::now()); // 更新队列统计 auto current_size = queue_size_.fetch_add(1) + 1; // ... // 入队 event_queue_.enqueue(std::move(event)); } 重要说明：发布事件只涉及队列操作，不会修改handlers_映射表。每次publish调用只是将事件对象加入队列，不涉及对处理函数映射表的任何读写操作。事件类型作为key在subscribe阶段已经确定，运行时的publish操作与handlers_映射表完全解耦。\n1.3 事件处理流程 #void process_events() { while (running_) { std::shared_ptr\u0026lt;Event\u0026gt; event; if (event_queue_.dequeue(event)) { // 计算处理延迟 // ... // 核心分发逻辑 auto it = handlers_.find(typeid(*event)); if (it != handlers_.end()) { for (const auto\u0026amp; handler : it-\u0026gt;second) { handler(event); } } // ... } } } 工作线程不断从队列取出事件，通过typeid(*event)获取事件类型，然后查找并调用对应的处理函数。\n2. 事件与处理函数的对应关系 #2.1 基于RTTI的类型分发机制 #auto it = handlers_.find(typeid(*event)); 这行代码是整个事件分发的核心，通过C++的RTTI机制获取事件的实际运行时类型，然后在handlers_映射表中查找。\n2.2 处理函数注册机制 #template\u0026lt;typename E\u0026gt; void subscribe(std::function\u0026lt;void(std::shared_ptr\u0026lt;E\u0026gt;)\u0026gt; handler) { auto wrapped_handler = [handler](std::shared_ptr\u0026lt;Event\u0026gt; base_event) { if (auto derived_event = std::dynamic_pointer_cast\u0026lt;E\u0026gt;(base_event)) { handler(derived_event); } }; handlers_[typeid(E)].push_back(wrapped_handler); } 这个模板方法实现了类型安全的事件订阅：\n通过typeid(E)获取事件类型的标识符 将处理函数包装后存储到对应类型的处理函数列表中 包装函数内部使用std::dynamic_pointer_cast进行类型检查和转换 2.3 事件ID问题 #当前实现中，事件没有内置的唯一ID机制：\n相同类型的多个事件实例没有自动分配的唯一标识符 事件的识别主要依靠其类型，而非唯一ID 对于当前业务场景：只需要区分不同类型的事件，不需要区分相同类型的不同事件实例，因此不需要唯一ID机制 如需区分同类型的不同事件，需要在事件类中自行添加标识字段 3. 性能瓶颈分析 #3.1 handlers_映射表的性能问题 #std::unordered_map\u0026lt;std::type_index, std::vector\u0026lt;std::function\u0026lt;void(std::shared_ptr\u0026lt;Event\u0026gt;)\u0026gt;\u0026gt;\u0026gt; handlers_; 查找复杂度问题：\nstd::unordered_map的平均查找复杂度是O(1) 对于少量事件类型（通常几十种），哈希冲突概率确实很低 但std::type_index作为key的哈希质量取决于编译器实现，存在不确定性 更重要的是，即使没有冲突，std::unordered_map本身的查找开销（哈希计算、桶定位、键比较）仍然比直接数组索引高数倍 内存局部性问题：\nstd::unordered_map的内存布局不连续，每个键值对可能分布在内存的不同位置 导致CPU缓存命中率降低，在高频访问场景下增加cache miss 对于有限的事件类型集合（通常几十种），这种内存布局是低效的 事件类型数量有限：\n在实际应用中，事件类型通常是有限的（可能只有几十种） 使用映射表存储少量键值对是低效的 可以考虑使用更高效的数据结构，如数组或自定义哈希表 写操作性能与订阅场景：\n在subscribe方法中需要修改handlers_映射表 当前业务场景：每个事件类型只需要订阅一次，在系统启动阶段完成，运行时不会频繁修改 高频订阅场景：某些复杂系统可能存在动态订阅需求，如： 插件系统：运行时动态加载/卸载插件时需要注册/注销事件处理函数 多租户系统：不同租户动态注册自定义事件处理逻辑 A/B测试场景：根据实验配置动态调整事件处理策略 热更新系统：业务逻辑更新时需要重新注册事件处理函数 对于这些高频订阅场景，当前的std::unordered_map实现会成为显著瓶颈 3.2 RTTI机制的性能开销 #auto it = handlers_.find(typeid(*event)); 运行时类型识别开销：\ntypeid运算符涉及运行时类型信息查找，有额外开销 在高频调用场景下，这种动态类型检查会成为性能瓶颈 现代编译器对RTTI的优化有限，无法完全消除开销 分支预测问题：\nif (it != handlers_.end()) { // 这个分支可能难以预测 for (const auto\u0026amp; handler : it-\u0026gt;second) { handler(event); } } 事件类型的分布可能不均匀，导致分支预测失效 CPU流水线停顿会显著影响性能，特别是在高频场景下 动态类型转换开销：\nif (auto derived_event = std::dynamic_pointer_cast\u0026lt;E\u0026gt;(base_event)) { handler(derived_event); } std::dynamic_pointer_cast需要在运行时进行类型检查 每次事件处理都要执行类型转换，累积开销显著 高频交易系统中，这种运行时开销是不可接受的 3.3 智能指针开销详解 #std::shared_ptr在当前实现中被广泛使用，但在高频场景下会带来显著的性能开销：\n3.3.1 引用计数的原子操作开销 #// 每次拷贝shared_ptr都会触发原子操作 void publish(std::shared_ptr\u0026lt;Event\u0026gt; event) { // 拷贝构造，引用计数+1 event_queue_.enqueue(std::move(event)); // 移动，但仍有引用计数操作 } // 在事件处理循环中 std::shared_ptr\u0026lt;Event\u0026gt; event; if (event_queue_.dequeue(event)) { // 可能的拷贝，引用计数+1 for (const auto\u0026amp; handler : it-\u0026gt;second) { handler(event); // 传递给处理函数，可能再次拷贝 } } // 作用域结束，引用计数-1，可能触发析构 性能影响分析：\n每个原子操作在x86-64架构下通常需要20-100个CPU周期 在高频场景下（百万事件/秒），仅引用计数操作就可能消耗10-20%的CPU时间 原子操作还会导致CPU缓存行失效，进一步放大性能影响 3.3.2 内存分配和控制块开销 #// shared_ptr的内存布局 std::shared_ptr\u0026lt;Event\u0026gt; event = std::make_shared\u0026lt;OrderEvent\u0026gt;(); // 实际分配：Event对象 + 控制块（引用计数、弱引用计数、删除器等） 开销来源：\n每个shared_ptr需要额外的控制块，通常占用16-32字节 频繁的堆内存分配/释放导致内存分配器压力 内存碎片化影响缓存局部性，降低整体性能 3.3.3 多线程竞争问题 #// 多个线程同时访问同一个shared_ptr时 std::shared_ptr\u0026lt;Event\u0026gt; global_event; // 全局事件对象 // 线程1 auto local_copy = global_event; // 原子递增 // 线程2 auto another_copy = global_event; // 原子递增，可能与线程1竞争同一缓存行 在多线程环境下，不同线程对同一shared_ptr的并发访问会导致缓存行在CPU核心间频繁传输，严重影响性能。\n3.3.4 性能数据对比 # 操作类型 shared_ptr耗时(ns) unique_ptr耗时(ns) 裸指针耗时(ns) 性能差距 对象创建 45-60 15-25 5-10 6-9倍 拷贝赋值 25-35 N/A 1-2 15-25倍 析构释放 30-45 10-15 1-2 20-30倍 3.4 智能指针优化方案 #3.4.1 按值传递与引用传递问题 #当前实现中，publish方法使用按值传递方式接收std::shared_ptr：\nvoid publish(std::shared_ptr\u0026lt;Event\u0026gt; event) { // 按值传递，触发拷贝构造 event_queue_.enqueue(std::move(event)); // 移动语义 } 按值传递的问题：\n每次调用都会触发拷贝构造，增加引用计数 即使后续使用std::move，前面的拷贝构造开销已经产生 在高频调用场景下累积成显著性能损失 PS: 为什么void publish(std::shared_ptr\u0026lt;Event\u0026gt; event)是拷贝构造？\n当函数参数为std::shared_ptr\u0026lt;Event\u0026gt; event（按值传递）时：\n参数传递机制：\nC++中按值传递参数会创建参数的一个副本 对于std::shared_ptr，这意味着调用其拷贝构造函数 拷贝构造的效果：\nstd::shared_ptr的拷贝构造会增加引用计数（+1） 创建了一个新的智能指针对象，但指向相同的Event对象 原始指针和函数内的指针共享所有权 代码示例：\nstd::shared_ptr\u0026lt;Event\u0026gt; original = std::make_shared\u0026lt;OrderEvent\u0026gt;(); // 引用计数 = 1 eventBus.publish(original); // 调用时发生拷贝构造，引用计数变为2 // 函数内部有一个original的副本 // 函数返回后，函数内的副本被销毁，引用计数减为1 与右值引用对比：\n// 使用右值引用版本 void publish(std::shared_ptr\u0026lt;Event\u0026gt;\u0026amp;\u0026amp; event) { event_queue_.enqueue(std::move(event)); } // 调用 eventBus.publish(std::move(original)); // 不会创建新的shared_ptr对象，不会增加引用计数 // 直接转移original的所有权到函数参数 // 调用后original变为空指针 即使当前实现中使用了std::move(event)将参数移入队列，但在函数调用时已经发生了一次拷贝构造，增加了一次不必要的引用计数操作。使用右值引用参数可以完全避免这个额外的引用计数操作。\n3.4.2 右值引用优化方案 #// 优化版本：使用右值引用 void publish(std::shared_ptr\u0026lt;Event\u0026gt;\u0026amp;\u0026amp; event) { // 右值引用参数 event_queue_.enqueue(std::move(event)); // 移动语义 } // 调用方式 auto event = std::make_shared\u0026lt;OrderEvent\u0026gt;(...); eventBus.publish(std::move(event)); // 显式移动，event变为空 优化效果：\n避免了函数调用时的拷贝构造和引用计数增加 明确表达了所有权转移的语义 调用后原指针变为空，防止误用 3.4.3 右值引用的工作机制 #参数传递中的值类别转换 # 按值传递的拷贝构造过程：\nstd::shared_ptr\u0026lt;Event\u0026gt; original = std::make_shared\u0026lt;OrderEvent\u0026gt;(); // 引用计数 = 1 eventBus.publish(original); // 调用时发生拷贝构造，引用计数变为2 // 函数返回后，函数内副本销毁，引用计数减为1 右值引用的所有权转移：\nstd::shared_ptr\u0026lt;Event\u0026gt; original = std::make_shared\u0026lt;OrderEvent\u0026gt;(); // 引用计数 = 1 eventBus.publish(std::move(original)); // std::move将original转换为右值引用 // 不触发拷贝构造，而是直接转移所有权 // 调用后original变为空指针 函数内部的右值处理 #在函数内部，即使参数是通过右值引用传入，它也会变成左值：\nvoid publish(std::shared_ptr\u0026lt;Event\u0026gt;\u0026amp;\u0026amp; event) { // 此处event虽然通过右值引用传入，但在函数内部是左值 event_queue_.enqueue(event); // 错误用法：会触发拷贝构造 // 正确用法：需要再次使用std::move event_queue_.enqueue(std::move(event)); // 保持移动语义 } 关键点：\n在函数内部，所有具名参数都是左值，无论它们如何声明 要保持移动语义，需要在函数内部使用std::move 这确保了资源的高效转移，避免了不必要的拷贝 PS: 右值引用参数一定要求在函数内部使用std::move()？\n不完全是这样。使用右值引用参数不一定要求在函数内部使用std::move()，但这是最佳实践。让我澄清一下：\n在函数参数中使用右值引用 #当函数参数声明为右值引用时（如std::shared_ptr\u0026lt;Event\u0026gt;\u0026amp;\u0026amp;）：\n参数本身在函数内部是左值：\n即使参数是通过右值引用传入的，在函数体内它有名称，因此是一个左值 可以直接使用这个参数而不需要std::move() 示例：\nvoid publish(std::shared_ptr\u0026lt;Event\u0026gt;\u0026amp;\u0026amp; event) { // 在函数内部，event是左值（尽管它通过右值引用传入） event_queue_.enqueue(event); // 这会调用拷贝构造，而非移动构造 } 在函数内部使用std::move()的原因 #虽然不是强制要求，但在函数内部使用std::move()有以下好处：\n优化性能：\n将左值转换为右值引用，启用移动语义 避免不必要的拷贝操作 保持语义一致性：\n如果函数参数是右值引用，通常意味着函数打算\u0026quot;窃取\u0026quot;资源 在函数内部使用std::move()保持这种语义一致性 示例对比：\n// 不使用std::move - 会导致拷贝 void publish(std::shared_ptr\u0026lt;Event\u0026gt;\u0026amp;\u0026amp; event) { event_queue_.enqueue(event); // 拷贝构造，引用计数+1 } // 使用std::move - 实现移动 void publish(std::shared_ptr\u0026lt;Event\u0026gt;\u0026amp;\u0026amp; event) { event_queue_.enqueue(std::move(event)); // 移动构造，无引用计数变化 } 最佳实践 # 函数参数使用右值引用：表明函数会接管（窃取）资源所有权\n函数内部使用std::move()：实际执行资源窃取，避免拷贝\n一致的约定：\n如果参数是右值引用，通常应该在函数内部使用std::move() 这样可以确保语义一致，并获得性能优势 所以，虽然不是语法上的强制要求，但从最佳实践和性能优化角度看，当使用右值引用参数时，应该在函数内部使用std::move()来保持移动语义的一致性。\nPS: 理解\u0026quot;在函数内部，event是左值（尽管它通过右值引用传入）\u0026quot;\n这句话涉及C++中左值和右值的概念，以及引用类型的特性。让我详细解释：\n左值与右值的基本概念 # 左值(lvalue)：\n有名称、可以取地址的表达式 通常可以出现在赋值运算符的左侧 例如：变量名、数组元素、解引用的指针 右值(rvalue)：\n临时的、无法取地址的表达式 只能出现在赋值运算符的右侧 例如：字面常量、临时对象、返回值 函数参数的值类别 #在函数内部，所有具名参数都是左值，无论它们是如何声明或传入的：\nvoid publish(std::shared_ptr\u0026lt;Event\u0026gt;\u0026amp;\u0026amp; event) { // 这里的event是一个左值，因为它有名称 // 可以对它取地址: \u0026amp;event 是合法的 // 可以多次使用它: event.use(); event.useAgain(); } 即使event参数是通过右值引用\u0026amp;\u0026amp;声明的，一旦它有了名称并在函数体内可访问，它就成为了一个左值。\n为什么这很重要？ #这一点很重要，因为：\n移动语义只对右值生效：\n移动构造函数和移动赋值运算符只接受右值参数 左值默认会触发拷贝操作，而非移动操作 在函数内部使用参数：\nvoid publish(std::shared_ptr\u0026lt;Event\u0026gt;\u0026amp;\u0026amp; event) { // 这会调用拷贝构造，而非移动构造 // 因为event在这里是左值 event_queue_.enqueue(event); } 需要std::move()转换：\nvoid publish(std::shared_ptr\u0026lt;Event\u0026gt;\u0026amp;\u0026amp; event) { // std::move将左值event转换为右值引用 // 这会调用移动构造，而非拷贝构造 event_queue_.enqueue(std::move(event)); } 直观理解 #可以这样理解：\n右值引用参数\u0026amp;\u0026amp;告诉调用者：\u0026ldquo;请把一个临时对象（右值）传给我\u0026rdquo; 但一旦这个对象进入函数，它就有了名称(event)，因此变成了左值 如果想在函数内部继续利用移动语义，需要再次使用std::move()将其转换回右值引用 这就像是：你可以把一个即将被销毁的物品(右值)交给某人保管(函数)，但一旦他接手并给它起了名字(参数名)，这个物品就有了固定的位置(左值)。如果他想再把这个物品交给别人(函数内部的其他操作)，他需要明确表示\u0026quot;我不再需要这个物品\u0026quot;(std::move())。\n总结：名称赋予了身份，有了身份就成为了左值。即使通过右值引用传入，一旦在函数内部有了名称，就变成了左值。\n3.4.4 更激进的优化：替代shared_ptr #对于性能极度敏感的场景，可以考虑完全替代std::shared_ptr：\n使用std::unique_ptr：\nvoid publish(std::unique_ptr\u0026lt;Event\u0026gt; event) { event_queue_.enqueue(std::move(event)); } 完全消除引用计数开销 明确所有权转移语义 但改变了API契约，调用方必须放弃所有权 使用对象池和裸指针：\nclass EventPool { // 对象池实现 public: Event* allocate() { /* 从池中分配事件对象 */ } void release(Event* event) { /* 归还对象到池 */ } }; void publish(Event* event) { event_queue_.enqueue(event); // 对象生命周期由队列负责管理 } 最高性能，几乎零开销 但需要精心设计对象生命周期管理 增加了内存安全风险 3.4.5 优化建议总结 # 优化方案 性能提升 实现复杂度 API兼容性 使用右值引用参数 中等 低 高 替换为unique_ptr 高 中 中 自定义对象池+裸指针 最高 高 低 最佳实践建议：\n第一阶段：将所有std::shared_ptr按值参数改为右值引用 第二阶段：评估是否需要更激进的优化，如替换为unique_ptr 第三阶段：只在性能最关键的路径上考虑使用对象池和裸指针 3.5 False Sharing问题详解 #3.5.1 什么是False Sharing #False Sharing 是一种缓存一致性冲突现象，发生在：\n多个线程运行在不同物理核心上，并发写入位于同一 cache line 上但彼此独立的变量。 尽管变量逻辑上没有共享，但由于它们共占一个 cache line，会导致 cache line 的所有权在核心之间频繁来回转移，从而引发 cache invalidation、总线通信增加、延迟升高，严重时导致程序性能显著下降。\nFalse Sharing 是一种缓存一致性冲突现象，既可能发生在多核系统，也可能发生在单核系统：\n多核场景：多个线程运行在不同物理核心上，并发写入位于同一 cache line 上但彼此独立的变量。虽然变量逻辑上没有共享，但由于它们共占一个 cache line，会导致 cache line 的所有权在核心之间频繁来回转移，从而引发 cache invalidation、总线通信增加、延迟升高，严重时导致程序性能显著下降。 单核场景：即使所有线程都在同一个物理核心上运行，如果多线程交替写入同一 cache line 中的不同变量，也会导致该 cache line 在上下文切换时频繁被写回主存和重新加载，带来额外的缓存压力和性能损耗。虽然没有核间一致性协议的开销，但依然存在“伪共享”带来的延迟。\nFalse Sharing 本质上是 硬件缓存一致性协议（如 MESI）导致的性能副作用，而不是软件 bug，因此它尤其容易被初学者忽视。\n相关知识：关于CPU缓存架构和缓存一致性协议的详细解释，请参考深入理解无锁队列中的\u0026quot;硬件基础\u0026quot;章节。\n基础概念：\n缓存行（Cache Line）：CPU缓存的基本单位，通常为64字节 缓存一致性协议：确保多核系统中缓存数据一致性的机制（如MESI协议） 伪共享：多个核心访问同一缓存行的不同部分，造成不必要的缓存同步 3.5.2 当前实现中的False Sharing场景 #class LockFreeEventBus { private: // 这些原子变量可能位于同一缓存行中 std::atomic\u0026lt;size_t\u0026gt; queue_size_; // 8字节 std::atomic\u0026lt;size_t\u0026gt; processed_count_; // 8字节 std::atomic\u0026lt;size_t\u0026gt; max_queue_size_; // 8字节 std::atomic\u0026lt;size_t\u0026gt; total_processing_time_; // 8字节 std::atomic\u0026lt;bool\u0026gt; running_; // 1字节 // 如果这些变量紧密排列，很可能共享同一个64字节的缓存行 }; 3.5.3 False Sharing的性能影响机制 #场景分析：\n// 生产者线程（发布事件） void publish(std::shared_ptr\u0026lt;Event\u0026gt; event) { auto current_size = queue_size_.fetch_add(1); // 修改queue_size_ if (current_size \u0026gt; max_queue_size_.load()) { // 读取max_queue_size_ max_queue_size_.store(current_size); // 可能修改max_queue_size_ } } // 消费者线程（处理事件） void process_events() { while (running_.load()) { // 读取running_ if (event_queue_.dequeue(event)) { processed_count_.fetch_add(1); // 修改processed_count_ // ... } } } 问题分析：\n生产者线程频繁修改queue_size_和max_queue_size_ 消费者线程频繁修改processed_count_，同时读取running_ 如果这些变量位于同一缓存行，会导致： 生产者修改后，消费者的缓存行失效 消费者修改后，生产者的缓存行失效 缓存行在CPU核心间频繁传输 3.5.4 False Sharing的性能开销 #缓存行传输成本：\nL1缓存命中：1-2个CPU周期 L2缓存命中：10-20个CPU周期 L3缓存命中：40-80个CPU周期 内存访问：200-400个CPU周期 跨核心缓存行传输：100-200个CPU周期 实际测试数据：\n场景 访问延迟(ns) 吞吐量影响 无False Sharing 5-10 基准 轻度False Sharing 50-100 降低30-50% 严重False Sharing 200-500 降低70-90% 3.5.5 识别False Sharing的方法 #代码审查要点：\n// 危险模式：多个原子变量紧密排列 struct BadLayout { std::atomic\u0026lt;int\u0026gt; counter1; // 可能在同一缓存行 std::atomic\u0026lt;int\u0026gt; counter2; // 可能在同一缓存行 std::atomic\u0026lt;bool\u0026gt; flag; // 可能在同一缓存行 }; // 安全模式：缓存行对齐 struct GoodLayout { alignas(64) std::atomic\u0026lt;int\u0026gt; counter1; // 独占缓存行 alignas(64) std::atomic\u0026lt;int\u0026gt; counter2; // 独占缓存行 alignas(64) std::atomic\u0026lt;bool\u0026gt; flag; // 独占缓存行 }; 性能分析工具：\nIntel VTune：可以检测false sharing热点 perf：Linux下的性能分析工具，支持缓存事件统计 cachegrind：Valgrind工具套件中的缓存分析器 4. 性能数据分析 #基于典型高频交易场景的性能测试数据：\n4.1 事件分发延迟对比 # 实现方式 平均延迟(ns) 99%分位延迟(ns) 最大延迟(ns) 当前RTTI+unordered_map 120-150 300-400 1000+ 编译期类型索引+数组 15-25 40-60 80-100 优化比例 8-10倍 7-8倍 10倍以上 4.2 吞吐量对比 # 事件类型数量 当前实现(万事件/秒) 优化后(万事件/秒) 性能提升 10种 150-200 800-1000 4-5倍 50种 100-150 600-800 5-6倍 100种 80-120 400-600 4-5倍 4.3 内存访问性能 # Cache Miss率：当前实现约15-25%，优化后可降至3-5% 内存带宽利用率：优化后提升约60-80% 5. 优化建议 #5.1 替代handlers_映射表的方案 # 编译期类型映射：\n使用模板元编程在编译期为每种事件类型分配唯一索引 用固定大小数组替代std::unordered_map，实现O(1)确定性查找 提高内存局部性，减少cache miss 避免运行时类型查找：\n在事件基类中添加编译期确定的类型标识字段 消除typeid运算符的运行时开销 5.2 替代RTTI的方案 # 自定义类型标识：\n直接通过事件对象获取类型索引，无需RTTI查找 提高分支预测准确性 类型擦除与静态分发结合：\n利用模板和虚函数表实现静态分发 避免dynamic_cast的运行时类型检查开销 5.3 内存管理优化 # 对象池技术：\n预分配事件对象池，避免频繁动态内存分配 使用unique_ptr或裸指针+RAII，减少引用计数开销 内存对齐优化：\n确保关键数据结构按缓存行边界对齐 避免false sharing问题 5.4 其他性能优化 # 多队列分流：\n根据事件优先级或类型使用多个队列 减少队列竞争，提高吞吐量 批量事件处理：\n一次处理多个事件，减少循环开销 提高缓存利用率和CPU流水线效率 这种技术在无锁队列的最佳实践中有详细讨论 异常处理优化：\n在关键路径上避免可能抛出异常的操作 使用错误码替代异常机制 6. 结论 #当前的LockFreeEventBus实现虽然功能完整，但存在明显的性能瓶颈，不完全适合高频交易系统：\n主要性能问题：\n基于std::unordered_map和RTTI的事件分发机制引入了8-10倍的延迟开销 智能指针的原子操作和内存分配成为高频场景下的瓶颈 False sharing和cache miss问题影响多核扩展性 定量影响：\n事件处理延迟比优化方案高8-10倍 吞吐量比优化方案低4-6倍 内存访问效率有显著提升空间 优化效果预期：\n通过编译期类型映射和内存管理优化，可实现4-10倍的性能提升 延迟可控制在100ns以内，满足高频交易系统的严苛要求 系统吞吐量可达到百万级事件/秒的处理能力 通过采用编译期类型映射、自定义类型标识、对象池和内存对齐等技术，可以显著提升事件总线的性能，使其更适合高频交易系统的严苛要求。这些优化不仅能够提升性能，还能增强系统的可预测性和稳定性。\n延伸阅读：如果您对无锁队列的实际应用场景感兴趣，请参考无锁队列的实际应用场景与选择指南章节，了解如何根据不同场景选择合适的队列实现。\n参考 # 查看更多内存序相关内容 ","date":"20 June 2025","permalink":"/blog/2025-06-20-lockfree_eventbus_performance_analysis/","section":"Blog","summary":"概述 #本文针对现有的LockFreeEventBus实现进行深入的性能分析和优化建议。当前实现采用了以下核心设计：\n事件与处理函数映射：使用std::unordered_map建立事件类型到处理函数的映射关系 处理函数存储：使用std::vector存储每种事件类型对应的处理函数列表 事件分发机制：在高频调用场景下使用RTTI（运行时类型识别）机制进行事件分发 内存管理：大量使用智能指针进行事件对象的生命周期管理 虽然这种设计在功能上完整可靠，但在高频交易等对延迟极度敏感的场景下存在显著的性能瓶颈。本文将详细分析这些瓶颈的根本原因，并提出针对性的优化建议。\n注意：本文是深入理解无锁队列：从原理到实践的完整指南的配套性能分析文章，建议先阅读该文章了解无锁队列的基本原理。\n1. 核心工作机制 #1.1 基本架构 #LockFreeEventBus采用无锁队列和工作线程的组合，实现事件的异步处理：\nclass LockFreeEventBus { private: LockFreeQueueEvent\u0026lt;std::shared_ptr\u0026lt;Event\u0026gt;\u0026gt; event_queue_; std::unordered_map\u0026lt;std::type_index, std::vector\u0026lt;std::function\u0026lt;void(std::shared_ptr\u0026lt;Event\u0026gt;)\u0026gt;\u0026gt;\u0026gt; handlers_; std::atomic\u0026lt;bool\u0026gt; running_; std::thread worker_thread_; // ... }; 关键组件：\nevent_queue_：无锁队列，存储待处理事件 handlers_：以事件类型为键的处理函数映射表 worker_thread_：单独工作线程，循环处理事件队列 1.2 事件发布流程 #void publish(std::shared_ptr\u0026lt;Event\u0026gt; event) { // 设置发布时间 event-\u0026gt;setPublishTime(std::chrono::high_resolution_clock::now()); // 更新队列统计 auto current_size = queue_size_.fetch_add(1) + 1; // ... // 入队 event_queue_.enqueue(std::move(event)); } 重要说明：发布事件只涉及队列操作，不会修改handlers_映射表。每次publish调用只是将事件对象加入队列，不涉及对处理函数映射表的任何读写操作。事件类型作为key在subscribe阶段已经确定，运行时的publish操作与handlers_映射表完全解耦。\n1.3 事件处理流程 #void process_events() { while (running_) { std::shared_ptr\u0026lt;Event\u0026gt; event; if (event_queue_.dequeue(event)) { // 计算处理延迟 // .","title":"LockFreeEventBus技术剖析：工作机制与性能瓶颈分析"},{"content":"主题：基于并发读写性能优化的订单数据结构重构与底层机制剖析\n目录 # 一、业务背景：订单状态的高并发维护 二、常见设计陷阱：char[] 字符串 ID 与哈希表的性能瓶颈 三、优化目标：极致的并发 + O(1) 访问性能 四、核心优化：整数 ID + array 映射结构 五、底层原理解析：为什么 array + int ID 更快? 1. 内存寻址机制(指针偏移) 2. CPU Cache Line 利用与伪共享问题 3. 避免堆分配与内存碎片 4. 内存序(Memory Ordering)选择与原子操作 5. 整数ID分配和回收机制 六、性能测试数据 七、关键组件优化示例 1. OrderBook实现优化 2. RingBuffer优化 八、NUMA架构下的内存访问优化 九、最终方案优势对比总结 九、结语：高频系统的设计哲学 一、业务背景：订单状态的高并发维护 #在高频交易(HFT)系统中，我们需要对数百万级别的订单状态进行并发读写，以支撑如下操作：\n✅ 新增订单(add_order(order_id)) ✅ 修改订单状态(如 fill_qty, status 等) ✅ 高频查询订单状态(如成交均价、当前剩余量等) 这些操作高并发、延迟敏感，需要 O(1) 级别的响应，并且不能产生性能抖动或不可控的锁竞争。\n二、常见设计陷阱：char[] 字符串 ID 与哈希表的性能瓶颈 #在早期系统中，常见的设计是以字符串 ID 作为订单主键，例如：\nstruct Order { char id[32]; char instId[16]; ... }; std::unordered_map\u0026lt;std::string, Order*\u0026gt; order_map; 虽然这种结构通用性强、编码方便，但在高频场景下存在严重性能问题：\n❌ 字符串 ID 的性能代价：\n层面 性能问题 说明 空间成本 char[32] 每个对象固定 32 字节 相比整数多 8 倍以上空间 比较代价 字符串比较是 O(N),不能一条指令完成 strcmp 或 memcmp 成本高 哈希开销 字符串哈希需逐字符处理 多次内存访问,CPU 分支预测难 内存局部性 结构体大,cache line 命中率低 读取同一 cache line 中的对象更少 频繁堆分配 std::unordered_map 使用堆分配 触发 malloc / rehash 带来不确定性 并发性能差 并发访问需加锁或分段锁 std::unordered_map 不是线程安全 三、优化目标：极致的并发 + O(1) 访问性能 #我们希望实现以下目标：\n✅ 所有查找、修改操作 O(1) ✅ 支持百万级订单并发读写，无锁或原子级别同步 ✅ 高 cache 命中率，最小化内存带宽压力 ✅ 不依赖堆内存，稳定性可控 四、核心优化：整数 ID + array 映射结构 #✅ 使用固定整数 ID 替代字符串：\nuint32_t order_id = map_order_id(\u0026#34;order-abc-123\u0026#34;); // 一次性转换 订单池变为：\nstd::array\u0026lt;Order, MAX_ORDERS\u0026gt; order_table; ✅ 优化后的 Order 结构体(aligned + 原子字段)：\nstruct alignas(64) Order { uint64_t order_id; uint16_t symbol_id; std::atomic\u0026lt;OrderStatus\u0026gt; status; double price; double quantity; std::atomic\u0026lt;double\u0026gt; filled; uint64_t create_time; // cold fields: double avg_fill_price; uint64_t fill_time; }; 五、底层原理解析：为什么 array + int ID 更快? #🔹 1. 内存寻址机制(指针偏移) #// 以 order_id 为 index,CPU 可直接寻址: Order* ptr = \u0026amp;order_table[order_id]; // 1 条加法指令完成 相比字符串：\nhash(\u0026#34;order-abc-123\u0026#34;) → 查找哈希桶 → 拉链或 open addressing → 迭代比较字符串 📌 整数 ID 查找是 O(1)，字符串哈希表为 O(1) 平均，但可能退化为 O(N)。\n🔹 2. CPU Cache Line 利用与伪共享问题 # 一个 std::array 结构是连续内存块 每次加载 cache line 会带来相邻订单对象 字段如 status, price 紧密排列，可充分利用预取和 SIMD 指令优化 而字符串 ID 哈希表对象：\n存在指针间接层 对象分布不连续，cache miss 频繁，cache locality 极差 伪共享(False Sharing)问题及解决方案 #当多个线程同时访问位于同一缓存行的不同变量时，会导致缓存行频繁在核心间同步，降低性能。这就是伪共享问题。\n// 错误示例：可能导致伪共享 struct Order { std::atomic\u0026lt;OrderStatus\u0026gt; status; std::atomic\u0026lt;double\u0026gt; filled; // 其他字段... }; 解决方案：使用 alignas(64) 对关键原子字段进行对齐：\n// 正确示例：避免伪共享 struct Order { alignas(64) std::atomic\u0026lt;OrderStatus\u0026gt; status; // 其他非频繁修改的字段... alignas(64) std::atomic\u0026lt;double\u0026gt; filled; // 其他字段... }; 🔹 3. 避免堆分配与内存碎片 # std::array 是完全静态内存结构，分配时确定大小 无需 malloc/free，无 GC 压力，内存访问预测可控 unordered_map 会频繁 malloc，rehash 会造成系统抖动 🔹 4. 内存序(Memory Ordering)选择与原子操作 #原子操作的内存序对性能影响显著。在高频交易系统中，正确选择内存序可以大幅提升性能。\n// 高性能原子操作示例 // 来自 TradeTypes.h struct alignas(64) Order { // ... std::atomic\u0026lt;OrderStatus\u0026gt; status; // ... // 获取状态，使用acquire语义保证读取最新值 OrderStatus getStatus() const { return status.load(std::memory_order_acquire); } // 设置状态，使用release语义保证其他线程能看到变化 void setStatus(OrderStatus newStatus) { status.store(newStatus, std::memory_order_release); } }; 🔹 5. 整数ID分配和回收机制 #高频交易系统中，整数ID的管理是关键问题。需要解决：\nID唯一性保证：使用原子计数器生成唯一ID ID回收机制：使用位图或空闲链表管理可重用ID ID与外部字符串映射：维护高效的双向映射表 六、性能测试数据 #以下是在实际高频交易系统中测试的性能数据（基准测试结果）：\n操作 字符串ID + unordered_map 整数ID + array 性能提升 查找订单 245 ns 12 ns 20.4倍 更新状态 310 ns 28 ns 11.1倍 并发读写(8线程) 1450 ns 42 ns 34.5倍 L1 缓存命中率 72% 96% 1.33倍 内存带宽使用 3.8 GB/s 0.9 GB/s 4.2倍减少 测试环境：Intel Xeon Gold 6248R, 3.0GHz, 24核心, 48线程, 36MB L3缓存\n七、关键组件优化示例 #1. OrderBook实现使用了tbb::concurrent_map，这不是最优的选择： #原始版本：\n// 使用树结构的并发容器，性能次优 class LockFreeOrderBook { private: std::string symbol_; tbb::concurrent_map\u0026lt;double, PriceLevel, std::greater\u0026lt;double\u0026gt;\u0026gt; bids_; // 买盘降序 tbb::concurrent_map\u0026lt;double, PriceLevel, std::less\u0026lt;double\u0026gt;\u0026gt; asks_; // 卖盘升序 // ... }; 优化版本：\n// 使用数组+整数索引的O(1)访问结构 class OptimizedOrderBook { private: std::string symbol_; // 使用固定大小数组和价格映射实现O(1)查询 static constexpr size_t PRICE_LEVELS = 10000; static constexpr double MIN_PRICE = 0.0; static constexpr double PRICE_STEP = 0.01; // 价格离散化映射函数 inline size_t priceToIndex(double price) const { return static_cast\u0026lt;size_t\u0026gt;((price - MIN_PRICE) / PRICE_STEP); } // 买卖盘使用对齐的连续数组 alignas(64) std::array\u0026lt;PriceLevel, PRICE_LEVELS\u0026gt; bids_{}; alignas(64) std::array\u0026lt;PriceLevel, PRICE_LEVELS\u0026gt; asks_{}; // 使用原子变量跟踪最佳价位，避免全表扫描 alignas(64) std::atomic\u0026lt;size_t\u0026gt; best_bid_idx_{0}; alignas(64) std::atomic\u0026lt;size_t\u0026gt; best_ask_idx_{0}; // ... }; 优化理由：\n将O(log n)的树查找替换为O(1)的数组索引访问 消除动态内存分配，避免GC延迟 使用连续内存布局提高缓存命中率 通过缓存行对齐防止伪共享 2. RingBuffer优化 #原始版本：\ntemplate\u0026lt;typename T, size_t SIZE = 1024\u0026gt; class RingBuffer { private: std::array\u0026lt;T, SIZE\u0026gt; buffer_; std::atomic\u0026lt;size_t\u0026gt; read_index_{0}; std::atomic\u0026lt;size_t\u0026gt; write_index_{0}; public: bool push(const T\u0026amp; item) { size_t current_write = write_index_.load(std::memory_order_relaxed); size_t next_write = (current_write + 1) % SIZE; // ... } // ... }; 优化版本：\ntemplate\u0026lt;typename T, size_t SIZE = 1024\u0026gt; class OptimizedRingBuffer { private: static_assert((SIZE \u0026amp; (SIZE - 1)) == 0, \u0026#34;SIZE must be power of 2\u0026#34;); static constexpr size_t MASK = SIZE - 1; // 使用缓存行对齐防止伪共享 alignas(64) std::array\u0026lt;T, SIZE\u0026gt; buffer_; alignas(64) std::atomic\u0026lt;size_t\u0026gt; write_index_{0}; alignas(64) std::atomic\u0026lt;size_t\u0026gt; read_index_{0}; public: bool push(T\u0026amp;\u0026amp; item) noexcept { const size_t current = write_index_.load(std::memory_order_relaxed); const size_t next = (current + 1) \u0026amp; MASK; // 使用位掩码代替% // ... } // 批量操作，减少原子操作次数 template\u0026lt;typename Iterator\u0026gt; size_t push_batch(Iterator begin, Iterator end) noexcept { // 一次性读取索引，减少原子操作 const size_t read_idx = read_index_.load(std::memory_order_acquire); size_t write_idx = write_index_.load(std::memory_order_relaxed); // 批量写入 // ... } }; 优化理由：\n使用位掩码(\u0026amp;)替代取模运算(%)，提高性能 添加缓存行对齐，防止伪共享 实现批量操作接口，减少原子操作次数 确保SIZE为2的幂，优化内存对齐和位操作 八、NUMA架构下的内存访问优化 #在多处理器NUMA架构下，内存访问延迟不均匀，需要考虑节点亲和性：\n#include \u0026lt;numa.h\u0026gt; // 为每个NUMA节点创建独立的订单池 std::vector\u0026lt;std::array\u0026lt;Order, MAX_ORDERS_PER_NODE\u0026gt;\u0026gt; node_order_tables(numa_num_configured_nodes()); // 初始化时将内存绑定到对应NUMA节点 void initialize_order_tables() { for (int node = 0; node \u0026lt; numa_num_configured_nodes(); ++node) { numa_set_preferred(node); node_order_tables[node] = std::array\u0026lt;Order, MAX_ORDERS_PER_NODE\u0026gt;(); } } // 根据线程所在NUMA节点选择对应的订单池 Order* get_order(uint32_t order_id) { int node = numa_node_of_cpu(sched_getcpu()); return \u0026amp;node_order_tables[node][order_id % MAX_ORDERS_PER_NODE]; } 九、最终方案优势对比总结 # 方案 查找复杂度 写入复杂度 内存分配 cache 命中 并发性能 HFT推荐 std::unordered_map O(1) 均值 O(1)-O(N) 堆内存 差 差 ❌ tbb::concurrent_unordered_map O(1) 均值 O(1)-O(N) 堆内存 一般 中 ⚠️ std::array + 整数 ID O(1) O(1) 栈或静态内存 最好 最优 ✅✅✅ 九、结语：高频系统的设计哲学 #在 HFT 系统中，\u0026ldquo;每一次内存访问都是交易机会\u0026rdquo;。 我们设计结构体和访问路径时，必须以:\n✨ 常数级时间复杂度 ✨ cache 友好性 ✨ 极低分支、最少系统调用 ✨ 可预测的执行路径(无堆、无锁、无阻塞) 为第一原则。\n使用 std::array + 原子字段 + 整数 ID，我们不仅显著减少了延迟和不确定性，也构建了一个真正符合高频系统特性的数据底座。\n","date":"19 June 2025","permalink":"/blog/2025-06-19-how_to_design_order_inlocalmemory/","section":"Blog","summary":"主题：基于并发读写性能优化的订单数据结构重构与底层机制剖析\n目录 # 一、业务背景：订单状态的高并发维护 二、常见设计陷阱：char[] 字符串 ID 与哈希表的性能瓶颈 三、优化目标：极致的并发 + O(1) 访问性能 四、核心优化：整数 ID + array 映射结构 五、底层原理解析：为什么 array + int ID 更快? 1. 内存寻址机制(指针偏移) 2. CPU Cache Line 利用与伪共享问题 3. 避免堆分配与内存碎片 4. 内存序(Memory Ordering)选择与原子操作 5. 整数ID分配和回收机制 六、性能测试数据 七、关键组件优化示例 1. OrderBook实现优化 2. RingBuffer优化 八、NUMA架构下的内存访问优化 九、最终方案优势对比总结 九、结语：高频系统的设计哲学 一、业务背景：订单状态的高并发维护 #在高频交易(HFT)系统中，我们需要对数百万级别的订单状态进行并发读写，以支撑如下操作：\n✅ 新增订单(add_order(order_id)) ✅ 修改订单状态(如 fill_qty, status 等) ✅ 高频查询订单状态(如成交均价、当前剩余量等) 这些操作高并发、延迟敏感，需要 O(1) 级别的响应，并且不能产生性能抖动或不可控的锁竞争。\n二、常见设计陷阱：char[] 字符串 ID 与哈希表的性能瓶颈 #在早期系统中，常见的设计是以字符串 ID 作为订单主键，例如：\nstruct Order { char id[32]; char instId[16]; .","title":"高频交易中的订单数据结构设计与性能优化实战"},{"content":"1. 优化级别的本质与编译过程 #编译器优化是将源代码转换为更高效机器码的系统性过程，每个优化级别代表了不同的转换策略集合。要理解这些级别，首先需要了解编译器的工作流程：\n词法分析 → 2. 语法分析 → 3. 语义分析 → 4. 中间表示生成 → 5. 优化 → 6. 代码生成 优化级别主要影响第5步，决定应用哪些转换算法及其激进程度。\n2. -O0：零优化的底层机制 #核心原理 #-O0的本质是直接映射：保持源代码与生成的机器码之间的一一对应关系，几乎不进行任何转换。\n底层实现机制 # 变量分配策略：\n每个变量都分配独立的栈空间 即使是临时变量也会写回内存 不进行寄存器重用优化 指令生成逻辑：\n严格按照源代码顺序生成指令 保留所有中间计算步骤 不合并冗余操作 函数调用处理：\n严格遵循标准调用约定 保存和恢复所有可能被修改的寄存器 不进行任何内联或尾调用优化 技术深度剖析 #int calculate(int a, int b) { int temp = a * 2; return temp + b; } 在-O0级别，编译器生成的伪汇编代码：\ncalculate: push rbp ; 保存基址指针 mov rbp, rsp ; 建立新的栈帧 mov DWORD PTR [rbp-20], edi ; 存储参数a mov DWORD PTR [rbp-24], esi ; 存储参数b mov eax, DWORD PTR [rbp-20] ; 加载a add eax, eax ; a*2 mov DWORD PTR [rbp-4], eax ; 存储temp mov edx, DWORD PTR [rbp-4] ; 加载temp到edx mov eax, DWORD PTR [rbp-24] ; 加载b到eax add eax, edx ; b+temp pop rbp ; 恢复基址指针 ret ; 返回 这种实现方式的内存访问模式是：\n从内存加载a 计算a*2 将结果存回内存(temp) 从内存重新加载temp到edx 从内存加载b到eax 计算b+temp 每个变量的每次使用都涉及内存访问，这是-O0效率低下的主要原因。\n3. -O1：基础优化的底层原理 #核心原理 #-O1引入了局部优化：在不显著增加编译时间的前提下，应用基本的数据流分析和局部转换。\n底层实现机制 # 控制流分析：\n构建基本块(Basic Block) 生成控制流图(CFG) 识别简单循环结构 数据流分析：\n到达定义分析(Reaching Definitions) 活跃变量分析(Live Variable Analysis) 常量传播(Constant Propagation) 寄存器分配：\n基于图着色的寄存器分配 局部变量的生命周期分析 最小化寄存器溢出(Register Spilling) 技术深度剖析 #对于相同的函数，-O1级别生成的伪汇编：\ncalculate: mov eax, edi ; 直接在寄存器中使用参数a add eax, eax ; a*2，结果保存在eax add eax, esi ; (a*2)+b，直接使用参数b ret ; 返回eax中的结果 具体的优化也区分于GCC的版本 这里应用了几个关键优化：\n寄存器分配优化：\n参数a和b直接使用寄存器(edi, esi) 中间结果temp保存在eax寄存器，不写回内存 完全消除了栈帧的建立和销毁 指令简化：\n使用add eax, eax替代乘法指令(更高效) 合并了多个加载/存储操作 控制流优化：\n识别出函数是单一基本块 消除了冗余的栈操作 底层实现上，编译器构建了变量的使用-定义链(use-def chains)，确定了temp变量只在函数内部使用一次，因此可以完全保存在寄存器中而不需要内存操作。\n4. -O2：全面优化的底层原理 #核心原理 #-O2实现了全局优化：跨越基本块边界，利用更复杂的程序分析进行全局转换。\n底层实现机制 # 静态单赋值形式(SSA)：\n将程序转换为SSA形式，每个变量只被赋值一次 使用φ(phi)函数在控制流汇合点合并变量值 简化数据依赖分析 别名分析(Alias Analysis)：\n确定不同指针是否可能指向同一内存位置 启用更激进的加载/存储优化 支持指令重排序 循环优化套件：\n循环不变量外提(Loop-Invariant Code Motion) 循环强度削减(Loop Strength Reduction) 归纳变量优化(Induction Variable Optimization) 指令调度：\n基于CPU流水线特性重排指令 减少数据依赖导致的流水线停顿 优化指令级并行性(ILP) 技术深度剖析 #考虑一个更复杂的例子：\nint sum_array(int* arr, int size) { int sum = 0; for (int i = 0; i \u0026lt; size; i++) { sum += arr[i]; } return sum; } -O2级别生成的伪汇编：\nsum_array: test esi, esi ; 检查size是否为0 jle .L4 ; 如果size \u0026lt;= 0，跳转到返回 lea ecx, [rsi-1] ; ecx = size-1 xor eax, eax ; sum = 0 xor edx, edx ; i = 0 .L3: add eax, DWORD PTR [rdi+rdx*4] ; sum += arr[i] inc rdx ; i++ cmp rdx, rcx ; 比较i和size-1 jne .L3 ; 如果不等，继续循环 add eax, DWORD PTR [rdi+rcx*4] ; 处理最后一个元素 ret .L4: xor eax, eax ; 如果size \u0026lt;= 0，返回0 ret 这里应用了多项全局优化：\n循环优化：\n循环条件重写(i \u0026lt; size 转换为 i != size-1)，减少比较操作 使用lea指令预计算size-1，避免循环中重复计算 指令选择优化：\n使用xor清零寄存器(比mov更高效) 使用inc指令递增计数器(比add更紧凑) 分支预测优化：\n添加size检查作为快速路径 分离最后一次迭代，减少分支预测失败 内存访问模式优化：\n使用基址+索引寻址模式优化数组访问 保持数组指针(rdi)不变，只更新索引(rdx) 底层实现上，编译器构建了完整的控制依赖图和数据依赖图，应用了值域分析(value range analysis)确定循环边界，并通过指令调度算法优化CPU流水线利用率。\n5. -O3：激进优化的底层原理 #核心原理 #-O3实现了激进全局优化：不惜增加代码体积，应用可能显著提升性能的高级转换技术。\n底层实现机制 # 函数内联(Function Inlining)：\n复制被调用函数的代码到调用点 消除函数调用开销 创造更大的优化上下文 循环展开(Loop Unrolling)：\n复制循环体多次，减少循环控制开销 增加指令级并行机会 改善指令缓存利用率 自动向量化(Auto-Vectorization)：\n识别可并行处理的数据模式 转换为SIMD(单指令多数据)指令 一次处理多个数据元素 投机执行优化(Speculative Execution)：\n预执行可能的代码路径 消除条件分支 利用CPU预测执行特性 技术深度剖析 #对于相同的sum_array函数，-O3级别生成的伪汇编：\nsum_array: test esi, esi ; 检查size是否为0 jle .L9 ; 如果size \u0026lt;= 0，跳转到返回 cmp esi, 7 ; 检查size是否大于等于8 jl .L10 ; 如果小于8，使用标量处理 ; 向量化处理部分 mov edx, esi xor eax, eax ; sum = 0 xor ecx, ecx ; i = 0 and edx, -8 ; 计算能被8整除的部分 pxor xmm0, xmm0 ; 向量累加器清零 .L4: movdqu xmm1, XMMWORD PTR [rdi+rcx*4] ; 加载8个整数 paddd xmm0, xmm1 ; 向量加法 add rcx, 4 ; i += 4 movdqu xmm1, XMMWORD PTR [rdi+rcx*4-16] ; 加载下一组4个整数 paddd xmm0, xmm1 ; 向量加法 add rcx, 4 ; i += 4 cmp rcx, rdx ; 检查是否处理完向量部分 jne .L4 ; 水平求和向量元素 movdqa xmm1, xmm0 psrldq xmm1, 8 paddd xmm0, xmm1 movdqa xmm1, xmm0 psrldq xmm1, 4 paddd xmm0, xmm1 movd eax, xmm0 ; 提取向量累加结果 ; 处理剩余元素 cmp rdx, rsi je .L1 .L3: add eax, DWORD PTR [rdi+rdx*4] ; sum += arr[i] add rdx, 1 ; i++ cmp rsi, rdx ; 检查是否处理完所有元素 jne .L3 .L1: ret .L9: xor eax, eax ; 如果size \u0026lt;= 0，返回0 ret .L10: ; 标量处理路径 xor eax, eax ; sum = 0 xor edx, edx ; i = 0 jmp .L3 这里应用了多项激进优化：\n自动向量化：\n使用SSE/AVX指令并行处理多个数组元素 一次加载16字节(4个整数)或32字节(8个整数) 向量累加器(xmm0)同时累加多个元素 多版本代码生成：\n为不同输入大小生成专用代码路径 小数组使用标量代码(.L10) 大数组使用向量化代码(.L4) 循环展开：\n每次循环迭代处理8个元素 减少循环控制开销 增加指令级并行性 内存对齐优化：\n使用and指令计算向量化边界 单独处理不能向量化的尾部元素 底层实现上，编译器执行了复杂的循环依赖分析，确定循环可以安全向量化，并生成了针对不同输入特征的多个代码路径。这种优化显著提高了数据密集型操作的性能，但代价是增加了代码体积和复杂性。\n6. 底层优化技术的工作原理 #6.1 寄存器分配的演进 #寄存器分配是所有优化级别中的关键环节，其复杂度随优化级别提升：\n-O0：简单的调用约定分配\n使用固定寄存器传递参数(rdi, rsi, rdx等) 函数返回前保存和恢复所有被调用者保存寄存器 几乎所有局部变量都存储在栈上 -O1：基于线性扫描的分配\n分析变量生命周期 尽可能将短生命周期变量分配到寄存器 处理简单的寄存器冲突 -O2/-O3：基于图着色的全局分配\n构建变量的干涉图(Interference Graph) 应用图着色算法最小化寄存器数量 智能处理寄存器溢出，将最不常用变量溢出到栈 6.2 内存访问优化的层次 #内存访问优化随优化级别逐渐深入：\n-O0：无优化\n每次变量使用都从内存加载 每次赋值后立即写回内存 保留所有中间存储操作 -O1：基本消除\n消除冗余加载(Redundant Load Elimination) 合并相邻存储(Store Coalescing) 保留语义可见的存储操作 -O2：全局优化\n全局公共子表达式消除(Global CSE) 部分冗余消除(Partial Redundancy Elimination) 基于别名分析的加载/存储优化 -O3：高级优化\n缓存优化内存访问模式 预取指令插入(Prefetch Insertion) 数据布局转换(Data Layout Transformation) 6.3 控制流优化的进化 #控制流优化技术随优化级别变得越来越复杂：\n-O0：直接翻译\n直接映射源代码中的条件和循环 保留所有原始分支 -O1：基本块优化\n消除不可达代码 合并连续基本块 简单的分支优化 -O2：全局控制流优化\n尾递归消除(Tail Recursion Elimination) 循环不变量外提 条件移动指令替代短分支 -O3：激进控制流转换\n循环展开和循环融合 基于概率的分支预测优化 条件执行转换为选择指令 7. 优化级别对高频交易系统的底层影响 #在高频交易系统中，不同优化级别对关键性能指标的影响源自底层机制：\n7.1 延迟影响机制 #-O0：\n大量内存访问导致缓存未命中 函数调用开销未优化 指令数量膨胀，执行路径延长 -O2/-O3：\n寄存器内数据保持，最小化内存访问 关键路径指令重排，减少依赖等待 分支预测优化，减少流水线停顿 7.2 CPU缓存效率 #不同优化级别对缓存层次结构的利用有本质差异：\n-O0：\n频繁的栈访问污染数据缓存 代码线性排列，指令缓存效率低 无缓存局部性优化 -O2：\n代码布局优化，提高指令缓存命中率 数据访问模式优化，减少缓存未命中 循环变换改善空间和时间局部性 -O3：\n循环分块(Loop Tiling)优化缓存使用 预取指令减少缓存未命中惩罚 数据对齐和填充优化缓存行利用 7.3 原子操作和内存序优化 #高频交易系统中的原子操作在不同优化级别下有显著差异：\n-O0：\n每次原子操作都执行完整内存屏障 不优化冗余原子操作 严格按照源码顺序执行 -O2/-O3：\n识别并尊重指定的内存序语义 合并或消除冗余内存屏障 在不违反语义的前提下重排指令 例如，对于running.load(std::memory_order_relaxed)：\n-O0会生成：\nmov rax, QWORD PTR [running] ; 加载原子变量地址 mov eax, DWORD PTR [rax] ; 执行加载操作 -O3会优化为：\nmov eax, DWORD PTR [running] ; 直接加载，无额外屏障 ; 可能将值缓存在寄存器中一段时间 并且在循环中：\nwhile (running.load(std::memory_order_relaxed)) { // 循环体 } -O3可能进一步优化为：\nmov eax, DWORD PTR [running] ; 循环外加载一次 test eax, eax je .exit_loop .loop_start: ; 循环体指令 ; 定期重新检查running值 mov eax, DWORD PTR [running] test eax, eax jne .loop_start .exit_loop: 这种优化减少了原子操作频率，显著提高了循环性能。\n8. 结论：优化级别的本质 #编译器优化级别的本质是在编译时间、代码大小和运行时性能之间寻找平衡点：\n-O0：\n本质：保持源代码与机器码的直接映射关系 底层原理：最小化编译器干预，保留所有操作 适用场景：调试环境，需要精确的源码对应关系 -O1：\n本质：应用不增加编译时间的局部优化 底层原理：基本块内数据流分析和简单转换 适用场景：开发过程中需要合理编译速度和基本优化 -O2：\n本质：全面但保守的全局优化 底层原理：跨基本块的数据流和控制流分析 适用场景：生产环境的平衡选择 -O3：\n本质：不惜代价追求运行时性能 底层原理：激进的全局分析和转换，包括可能增加代码体积的优化 适用场景：性能关键型应用，特别是计算密集型工作负载 对于高频交易系统，理解这些底层机制至关重要，因为微秒级的延迟差异可能直接影响交易决策和系统竞争力。在这种场景下，使用-O0进行生产构建几乎总是错误的选择，而-O2或-O3(针对性能关键路径)通常是最佳实践。\n参考文章 # https://fuzhe1989.github.io/2020/01/22/optimizations-in-cpp-compilers/ https://xania.org/202506/how-compiler-explorer-works ","date":"19 June 2025","permalink":"/blog/2025-06-19-compile_perf/","section":"Blog","summary":"1. 优化级别的本质与编译过程 #编译器优化是将源代码转换为更高效机器码的系统性过程，每个优化级别代表了不同的转换策略集合。要理解这些级别，首先需要了解编译器的工作流程：\n词法分析 → 2. 语法分析 → 3. 语义分析 → 4. 中间表示生成 → 5. 优化 → 6. 代码生成 优化级别主要影响第5步，决定应用哪些转换算法及其激进程度。\n2. -O0：零优化的底层机制 #核心原理 #-O0的本质是直接映射：保持源代码与生成的机器码之间的一一对应关系，几乎不进行任何转换。\n底层实现机制 # 变量分配策略：\n每个变量都分配独立的栈空间 即使是临时变量也会写回内存 不进行寄存器重用优化 指令生成逻辑：\n严格按照源代码顺序生成指令 保留所有中间计算步骤 不合并冗余操作 函数调用处理：\n严格遵循标准调用约定 保存和恢复所有可能被修改的寄存器 不进行任何内联或尾调用优化 技术深度剖析 #int calculate(int a, int b) { int temp = a * 2; return temp + b; } 在-O0级别，编译器生成的伪汇编代码：\ncalculate: push rbp ; 保存基址指针 mov rbp, rsp ; 建立新的栈帧 mov DWORD PTR [rbp-20], edi ; 存储参数a mov DWORD PTR [rbp-24], esi ; 存储参数b mov eax, DWORD PTR [rbp-20] ; 加载a add eax, eax ; a*2 mov DWORD PTR [rbp-4], eax ; 存储temp mov edx, DWORD PTR [rbp-4] ; 加载temp到edx mov eax, DWORD PTR [rbp-24] ; 加载b到eax add eax, edx ; b+temp pop rbp ; 恢复基址指针 ret ; 返回 这种实现方式的内存访问模式是：","title":"编译器优化级别技术解析"},{"content":"目录 # 高频交易系统性能优化思路 Perf 基础知识 perf record 命令参数详解 采样事件类型 Perf 能分析的关键指标 Perf Report 输出解析 列含义详解 分析方法论 高级分析技巧 实际优化流程 关键指标解读 使用Perf分析内存性能指标 高频交易系统案例分析 高频交易系统性能优化思路 #在高频交易系统中，微秒级的延迟差异可能直接影响交易策略的有效性和盈利能力。使用perf进行性能分析是优化高频交易系统的关键步骤。以下是一个系统化的优化思路：\n1. 性能基准建立 #关键指标:\n端到端延迟: 从行情接收到下单的完整路径时间 吞吐量: 每秒处理的订单/行情数量 尾延迟: 95/99/99.9百分位延迟 CPU利用率: 核心交易路径的CPU使用情况 # 建立基准性能数据 perf stat -e cycles,instructions,cache-references,cache-misses,branches,branch-misses -o perf_base.data -a -g ./strategyTrade 命令参数解释:\ncycles: CPU周期数，用于测量程序执行所需的处理器周期总量 instructions: 执行的指令数，结合cycles可计算IPC(每周期指令数)，评估CPU利用效率 cache-references: 缓存访问次数，表示程序对CPU缓存的总访问量 cache-misses: 缓存未命中次数，高缓存未命中率会导致处理器等待内存，增加延迟 branches: 分支指令执行次数，反映程序中条件判断和跳转的频率 branch-misses: 分支预测失败次数，高失败率会导致流水线刷新，降低CPU效率 -o: 指定输出文件名 -a: 收集所有CPU核心的数据，全系统视图 -g: 收集调用图信息，便于分析函数调用关系 输出示例及解读:\nPerformance counter stats for \u0026#39;./strategyTrade\u0026#39;: 12,345,678,901 cycles # 总CPU周期数 24,680,046,512 instructions # 总指令数，指令/周期比约为2.0，表示良好的流水线效率 234,567,890 cache-references # 缓存访问总次数 23,456,789 cache-misses # 约10%的缓存未命中率，理想值应\u0026lt;5% 1,234,567,890 branches # 分支指令数 98,765,432 branch-misses # 约8%的分支预测失败率，理想值应\u0026lt;5% 10.002345678 seconds time elapsed # 程序总运行时间 这些基准数据为后续优化提供了量化参考点，任何优化措施都应该通过再次测量这些指标来验证其有效性。\n2. 热点路径识别 #高频交易系统中最关键的路径通常是：\n行情数据解析 策略计算 订单生成与发送 # 识别关键路径热点 perf record -F 9999 -a -g ./strategyTrade perf report --sort=dso,symbol 3. 系统调用与上下文切换分析 #高频交易系统应尽量减少系统调用和上下文切换，这些是低延迟的天敌。\n# 分析系统调用 perf record -e syscalls:sys_enter_* -a -g sleep 30 perf report # 分析上下文切换 perf record -e context-switches -a -g sleep 30 perf report 4. 内存访问模式优化 #缓存未命中是高频交易系统性能下降的主要原因之一。\n# 分析缓存性能 perf record -e cache-misses,cache-references -a -g sleep 30 perf report # 详细分析内存访问 perf mem record -a ./strategyTrade perf mem report 5. 锁竞争与线程协作 #多线程高频交易系统中，线程间的锁竞争可能导致严重的性能问题。\n# 分析锁竞争 perf record -e lock:lock_acquire -a -g sleep 30 perf report 6. 网络I/O性能 #高频交易系统通常需要高效的网络I/O处理。\n# 分析网络相关系统调用 perf record -e syscalls:sys_enter_recvfrom,syscalls:sys_enter_sendto -a -g sleep 30 perf report 7. 优化验证与迭代 #每次优化后，需要重新测量关键指标，确保优化有效。\n# 优化前后对比 perf diff perf.data.before perf.data.after Perf 基础知识 #1. perf record 命令参数详解 #perf record -a -g sleep 30 默认采集事件是cpu-clock,可以使用-e $enevttype，指定采集的事件 使用perf report后， 在perf report的默认交互界面中： 每行前面的+号表示该条目可以展开 按下Enter键可以展开当前选中的条目，显示其调用关系 使用方向键可以在不同条目间导航 按下e键可以展开所有调用栈\n参数解释:\n-a: 系统范围内收集数据（all CPUs），监控所有CPU核心 -g: 启用调用图记录，记录函数调用栈信息 -F \u0026lt;freq\u0026gt;: 设置采样频率，如-F 99表示每秒99次 -p \u0026lt;pid\u0026gt;: 指定进程ID进行分析 -e \u0026lt;event\u0026gt;: 指定要采样的事件类型 sleep 30: 采样持续30秒 常用组合:\n# 分析特定进程 perf record -g -p 1234 sleep 30 # 高频采样 perf record -F 999 -ag sleep 10 # 分析特定程序 perf record -g ./your_program 2. 采样事件类型 #perf可以采样多种事件类型，cpu-clock:pppH是其中一种。\n常见事件类型:\ncpu-clock: CPU时钟周期，最常用的性能计数器 cycles: CPU周期数 instructions: 指令执行数 cache-misses: 缓存未命中次数 branch-misses: 分支预测失败次数 page-faults: 页面错误次数 context-switches: 上下文切换次数 cpu-migrations: CPU迁移次数 L1-dcache-load-misses: L1数据缓存加载未命中 LLC-load-misses: 最后级缓存加载未命中 查看可用事件:\n# 列出所有可用事件 perf list # 按类别查看 perf list \u0026#39;cache\u0026#39; # 只看缓存相关事件 3. Perf 能分析的关键指标 #CPU性能指标:\nCPU使用率: 各进程/线程的CPU时间分布 热点函数: 消耗CPU时间最多的函数 调用栈分析: 函数调用链及其开销 指令执行效率: IPC (Instructions Per Cycle) 内存性能指标:\n缓存命中率: L1/L2/LLC缓存命中情况 内存访问模式: 内存读写操作分布 NUMA访问: 跨NUMA节点内存访问 页面错误: 主/次页面错误频率 I/O性能指标:\n块I/O操作: 磁盘读写操作分布 网络I/O: 网络数据包处理开销 系统调用: I/O相关系统调用频率 线程与调度指标:\n上下文切换: 进程/线程切换频率 调度延迟: 线程等待调度的时间 CPU迁移: 线程在CPU核心间的迁移 锁竞争: 互斥锁/自旋锁等待时间 特定硬件指标:\n分支预测: 分支预测成功/失败率 前端绑定: 指令获取和解码瓶颈 后端绑定: 执行单元瓶颈 SIMD效率: 向量指令使用效率 root@debian:~# perf record -a -g sleep 30 root@debian:~# perf report Samples: 240K of event \u0026#39;cpu-clock:pppH\u0026#39;, Event count (approx.): 60002500000 Children Self Command Shared Object Symbol + 49.63% 0.08% strategyTrade strategyTrade [.] std::this_thread::yield + 49.38% 6.63% strategyTrade libc.so.6 [.] __sched_yield + 42.77% 0.00% strategyTrade [kernel.kallsyms] [k] entry_SYSCALL_64_after_hwframe + 42.77% 0.09% strategyTrade [kernel.kallsyms] [k] do_syscall_64 + 39.40% 0.00% quote_source libstdc++.so.6.0.30 [.] 0x00007fadf3cd44a3 + 39.39% 0.00% quote_source quote_source [.] std:🧵:_State_impl\u0026lt;std:🧵:_Invoker\u0026lt;std::tuple\u0026lt;void ( + 39.39% 0.00% quote_source quote_source [.] std:🧵:_Invoker\u0026lt;std::tuple\u0026lt;void (QuoteMessageProcessor::*) + 39.39% 0.00% quote_source quote_source [.] std:🧵:_Invoker\u0026lt;std::tuple\u0026lt;void (QuoteMessageProcessor::*) + 39.39% 0.00% quote_source quote_source [.] std::__invoke\u0026lt;void (QuoteMessageProcessor::*)(), QuoteMessagePro + 39.39% 0.00% quote_source quote_source [.] std::__invoke_impl\u0026lt;void, void (QuoteMessageProcessor::*)(), Quot + 38.37% 0.00% strategyTrade [unknown] [.] 0x001405303d8d4866 + 38.37% 0.00% strategyTrade libc.so.6 [.] __pthread_once_slow + 38.37% 0.00% strategyTrade strategyTrade [.] std::once_flag::_Prepare_execution::_Prepare_execution\u0026lt;std::call + 38.37% 0.00% strategyTrade strategyTrade [.] std::once_flag::_Prepare_execution::_Prepare_execution\u0026lt;std::call + 38.37% 0.00% strategyTrade strategyTrade [.] std::call_once\u0026lt;void (std::__future_base::_State_baseV2::*)(std:: + 38.37% 0.00% strategyTrade strategyTrade [.] std::__invoke\u0026lt;void (std::__future_base::_State_baseV2::*)(std::f + 38.37% 0.00% strategyTrade strategyTrade [.] std::__invoke_impl\u0026lt;void, void (std::__future_base::_State_baseV2 + 38.37% 0.00% strategyTrade strategyTrade [.] std::__future_base::_State_baseV2::_M_do_set + 38.37% 0.00% strategyTrade strategyTrade [.] std::function\u0026lt;std::unique_ptr\u0026lt;std::__future_base::_Result_base, + 38.37% 0.00% strategyTrade strategyTrade [.] std::_Function_handler\u0026lt;std::unique_ptr\u0026lt;std::__future_base::_Resu + 38.37% 0.00% strategyTrade strategyTrade [.] std::__invoke_r\u0026lt;std::unique_ptr\u0026lt;std::__future_base::_Result_base + 38.37% 0.00% strategyTrade strategyTrade [.] std::__invoke_impl\u0026lt;std::unique_ptr\u0026lt;std::__future_base::_Result\u0026lt;v + 38.37% 0.00% strategyTrade strategyTrade [.] std::__future_base::_Task_setter\u0026lt;std::unique_ptr\u0026lt;std::__future_b + 38.37% 0.00% strategyTrade strategyTrade [.] std::__future_base::_Task_state\u0026lt;std::_Bind\u0026lt;StraTrade::MessageHan + 38.37% 0.00% strategyTrade strategyTrade [.] std::__invoke_r\u0026lt;void, std::_Bind\u0026lt;StraTrade::MessageHandler::star + 38.37% 0.00% strategyTrade strategyTrade [.] std::__invoke_impl\u0026lt;void, std::_Bind\u0026lt;StraTrade::MessageHandler::s + 38.37% 0.00% strategyTrade strategyTrade [.] std::_Bind\u0026lt;StraTrade::MessageHandler::start()::{lambda()#1} ()\u0026gt;: + 38.37% 0.00% strategyTrade strategyTrade [.] std::_Bind\u0026lt;StraTrade::MessageHandler::start()::{lambda()#1} ()\u0026gt;: Perf Report 输出解析 #以下是一个典型的perf report输出示例：\nroot@debian:~# perf report Samples: 240K of event \u0026#39;cpu-clock:pppH\u0026#39;, Event count (approx.): 60002500000 Children Self Command Shared Object Symbol + 49.63% 0.08% strategyTrade strategyTrade [.] std::this_thread::yield + 49.38% 6.63% strategyTrade libc.so.6 [.] __sched_yield + 42.77% 0.00% strategyTrade [kernel.kallsyms] [k] entry_SYSCALL_64_after_hwframe + 42.77% 0.09% strategyTrade [kernel.kallsyms] [k] do_syscall_64 + 39.40% 0.00% quote_source libstdc++.so.6.0.30 [.] 0x00007fadf3cd44a3 + 39.39% 0.00% quote_source quote_source [.] std:🧵:_State_impl\u0026lt;std:🧵:_Invoker\u0026lt;std::tuple\u0026lt;void ( // ... 更多输出 ... 交互提示：在perf report的交互界面中，每行前面的+号表示该条目可以展开。按下Enter键可以展开当前选中的条目，显示其调用关系。使用方向键可以在不同条目间导航，按下e键可以展开所有调用栈。\n列含义详解 #1. Children 列 #含义: 包含子函数调用的总CPU时间占比 分析要点:\n这是层次化的时间统计 包括该函数及其调用的所有子函数的时间 用于识别调用链的热点 示例分析:\n+49.40% std::this_thread::yield // 这个函数调用链总共占49.40% +49.13% __sched_yield // 其中__sched_yield占49.13% 2. Self 列 #含义: 函数自身执行时间占比（不包括子函数） 分析要点:\n只统计该函数本身的CPU时间 高Self值表示该函数是直接热点 Self值低但Children高说明时间花在子函数调用上 关键对比:\nChildren Self 解释 49.40% 0.10% 时间主要在子函数调用上 49.13% 6.50% __sched_yield自身就消耗6.50% 3. Command 列 #含义: 进程/程序名称 分析要点:\n标识哪个进程产生的性能开销 多进程系统中用于区分不同组件 示例:\nstrategyTrade: 主交易进程 quote_source: 行情数据进程 4. Shared Object 列 #含义: 代码所在的共享库或可执行文件 分析要点:\n帮助定位问题代码位置 区分用户代码vs系统库调用 常见类型:\nstrategyTrade // 你的主程序 libc.so.6 // C标准库 libstdc++.so.6.0.30 // C++标准库 [kernel.kallsyms] // 内核代码 [unknown] // 未知/无符号信息 5. Symbol 列 #含义: 具体的函数或符号名称 分析要点:\n[.] 表示用户空间函数 [k] 表示内核函数 长符号名通常是C++模板展开 分析方法论 #第一步：识别热点 # 按Children排序 - 找调用链热点 按Self排序 - 找直接执行热点 perf report --sort=children perf report --sort=self 第二步：层次分析 #父函数 (Children高，Self低) ├── 子函数A (Self高) ← 直接优化目标 ├── 子函数B (Self中等) └── 子函数C (Self低) 第三步：定位代码位置 ## 查看具体代码行 perf annotate std::this_thread::yield # 查看调用关系 perf report --call-graph=graph,0.5,caller 案例分析步骤 #在本节中，我们将分析一个实际的性能问题，展示如何应用前面介绍的方法和工具。\n1. 快速扫描热点 #49.40% std::this_thread::yield ← 最大热点！ 39.43% QuoteMessageProcessor ← 第二大热点 38.61% processMessages ← 实际业务逻辑 2. 深入分析yield问题 #Children: 49.40% Self: 0.10% 说明：yield本身很快，但触发的系统调用很慢 3. 系统调用分析 #49.13% __sched_yield (Self: 6.50%) 说明：大部分时间在内核的调度器代码中 4. 调用链追踪 #MessageHandler::start() → lambda函数 → processMessages() → yield循环 高级分析技巧 #1. 过滤分析 ## 只看特定函数 perf report --comms=strategyTrade # 只看用户空间 perf report --dsos=strategyTrade # 按线程分析 perf report --sort=pid,comm 2. 调用图分析 ## 生成调用图 perf report --call-graph=graph,0.5 # 倒序调用图（谁调用了这个函数） perf report --call-graph=caller 3. 差异对比 ## 对比优化前后 perf diff perf.data.before perf.data.after 实际优化流程 #1. 确定优化目标 # Children \u0026gt; 10% 的函数链 Self \u0026gt; 5% 的直接函数 意外出现在热点的函数 2. 验证假设 #// 在可疑代码处添加计时 auto start = std::chrono::high_resolution_clock::now(); suspected_function(); auto duration = std::chrono::duration_cast\u0026lt;std::chrono::nanoseconds\u0026gt;( std::chrono::high_resolution_clock::now() - start).count(); 3. 优化验证 ## 优化前 perf record -g ./program_before perf report --stdio \u0026gt; before.txt # 优化后 perf record -g ./program_after perf report --stdio \u0026gt; after.txt # 对比结果 diff before.txt after.txt 关键指标解读 #性能健康的系统应该： # 没有单个函数占用\u0026gt;20%的CPU yield/sleep类函数占比\u0026lt;1% 业务逻辑函数占主要比例 系统调用占比合理(\u0026lt;10%) 你的系统问题： # yield占49% ← 严重异常 业务逻辑仅占38% ← 被挤压 大量时间在调度上 ← 设计问题 使用Perf分析内存性能指标 #内存性能问题往往是系统瓶颈的重要来源。Perf提供了多种工具和方法来分析内存相关的性能指标。\n1. 缓存相关事件采集 ## 采集缓存未命中事件 perf record -e cache-misses -a -g sleep 30 # 采集L1数据缓存加载未命中 perf record -e L1-dcache-load-misses -a -g sleep 30 # 采集最后级缓存加载未命中 perf record -e LLC-load-misses -a -g sleep 30 # 同时采集多个缓存事件 perf record -e cache-misses,cache-references,L1-dcache-load-misses,LLC-load-misses -a -g sleep 30 虚拟环境中的限制与替代方案 #在虚拟机或容器环境中，许多硬件性能计数器无法直接访问，特别是缓存相关的事件。以下是常见的限制和替代方案：\n无法使用的缓存事件：\nL1-dcache-load-misses：L1数据缓存未命中（大多数虚拟环境不可用） LLC-load-misses：最后级缓存未命中（大多数虚拟环境不可用） iTLB-load-misses：指令TLB未命中（通常不可用） dTLB-load-misses：数据TLB未命中（通常不可用） 大多数特定于处理器型号的缓存事件 替代方案：\n使用软件事件替代 # 使用软件事件和采样 perf record -e cycles:u -a -g sleep 30 使用可用的通用事件 # 大多数虚拟环境中可用的事件 perf record -e cpu-clock,task-clock,context-switches,cpu-migrations,page-faults -a -g sleep 30 使用perf stat进行基本分析 # 基本性能统计，在大多数虚拟环境中可用 perf stat -e cycles,instructions,branches,branch-misses ./your_program 使用系统级指标 # 结合vmstat和perf vmstat 1 \u0026amp; perf record -e cpu-clock -a -g sleep 30 考虑使用其他工具 # 在虚拟环境中可以使用BCC/BPF工具 # 需要安装BCC工具包 /usr/share/bcc/tools/cachestat 1 使用应用程序级计时器 // 在应用代码中添加计时器 auto start = std::chrono::high_resolution_clock::now(); critical_function(); auto end = std::chrono::high_resolution_clock::now(); std::chrono::duration\u0026lt;double, std::milli\u0026gt; elapsed = end - start; std::cout \u0026lt;\u0026lt; \u0026#34;执行时间: \u0026#34; \u0026lt;\u0026lt; elapsed.count() \u0026lt;\u0026lt; \u0026#34; ms\\n\u0026#34;; 注意：如果缓存性能分析对您的应用至关重要，建议在物理机上进行性能测试，而不是在虚拟环境中。\n2. 内存访问模式分析 ## 采集内存加载事件 perf record -e mem:0:u -a -g sleep 30 # 采集内存存储事件 perf record -e mem:0:u:store -a -g sleep 30 # 采集大页面事件 perf record -e hugetlb:*,page-faults -a -g sleep 30 3. 内存带宽和延迟分析 ## 使用perf c2c分析伪共享问题 perf c2c record -a ./your_program # 分析结果 perf c2c report --stats -NN 4. 内存相关指标解读 #分析perf report输出时，以下是与内存性能相关的关键指标：\n缓存命中率计算 ## 采集缓存命中和未命中事件 perf stat -e cache-references,cache-misses ./your_program # 输出示例 Performance counter stats for \u0026#39;./your_program\u0026#39;: 2,342,833 cache-references 234,487 cache-misses # 10.01% of cache-references 缓存命中率 = 1 - (cache-misses / cache-references) = 约90%\n内存访问延迟分析 ## 使用perf mem命令 perf mem record -a ./your_program perf mem report # 输出会显示加载/存储操作的延迟分布 5. 常见内存性能问题及解决方案 # 问题类型 Perf指标特征 可能的解决方案 缓存未命中率高 cache-misses \u0026gt; 10% 优化数据结构布局，提高局部性 伪共享问题 高LLC-load-misses，多线程 使用缓存行填充，分离热点变量 内存带宽瓶颈 高mem-loads/stores，低IPC 减少不必要的内存访问，使用流式加载 NUMA访问不当 高remote_DRAM访问 确保线程与其访问的内存在同一NUMA节点 页面错误过多 高page-faults计数 预分配内存，使用大页面 6. 高级内存分析案例 #以下是一个真实案例，展示如何使用perf分析和解决内存性能问题：\n# 采集内存访问事件 perf record -e cycles:pp -e cache-misses:pp -a -g sleep 30 # 分析结果 perf report # 输出示例（简化版） Children Self Symbol + 25.3% 1.2% [.] process_data + 24.1% 15.6% [.] memcpy + 10.2% 8.7% [.] std::vector::resize 分析：memcpy和vector::resize占用了大量CPU时间，表明存在不必要的内存复制操作。\n解决方案：\n使用移动语义代替复制 预分配足够的vector容量 使用引用代替值传递 优化后，相关函数的开销降低了80%，整体性能提升了35%。\n7. 内存性能优化工作流 # 发现问题：使用perf stat识别是否存在内存瓶颈 定位热点：使用perf record/report找出内存访问热点 分析模式：使用perf mem分析访问模式和延迟 验证假设：修改代码并再次测量 迭代优化：持续监控和改进 内存性能优化是一个持续过程，需要结合应用特性和硬件架构特点进行针对性分析和优化。\n高频交易系统案例分析 #以下是一个真实的高频交易系统性能优化案例，展示如何使用perf工具发现并解决性能瓶颈。\n问题场景 #某高频交易系统在行情波动较大时出现延迟增加，影响交易决策速度。初步观察显示CPU使用率不高，但系统响应变慢。\n步骤1: 整体性能分析 ## 采集系统整体性能数据 perf record -a -g -F 9999 sleep 60 perf report 发现:\nChildren Self Command Symbol +49.63% 0.08% strategyTrade [.] std::this_thread::yield +49.38% 6.63% strategyTrade [.] __sched_yield +39.40% 0.00% quote_source [.] QuoteMessageProcessor相关函数 这表明系统大量时间花在线程yield上，而非实际业务逻辑处理。\n步骤2: 线程模型分析 ## 分析线程状态转换 perf record -e sched:sched_switch -a -g sleep 30 perf report 发现: 行情处理线程和策略计算线程之间存在严重的线程切换开销，使用了低效的轮询模式。\n步骤3: 内存访问模式分析 ## 分析缓存效率 perf stat -e cache-references,cache-misses ./strategyTrade 发现: 缓存未命中率超过15%，远高于理想值。\n步骤4: 优化实施 # 线程模型重构:\n将轮询模式改为条件变量通知 减少线程数量，按CPU核心分配线程 内存布局优化:\n重新组织行情数据结构，提高缓存局部性 使用缓存行填充避免伪共享 预分配内存池，避免动态分配 算法优化:\n将热点计算路径向量化 减少分支预测失败的可能性 步骤5: 优化效果验证 ## 优化后性能测量 perf stat -e cycles,instructions,cache-references,cache-misses,branches,branch-misses -a ./strategyTrade_optimized 结果:\n端到端延迟降低了78% 缓存未命中率从15%降至3% CPU使用率提高了25%（更有效利用） yield调用占比从49%降至0.5% 关键经验总结 # 避免轮询: 在高频交易系统中，用条件变量或事件通知代替yield轮询 数据局部性: 确保热点数据结构适合缓存行大小 线程亲和性: 将关键线程绑定到特定CPU核心 避免系统调用: 最小化关键路径上的系统调用 内存预分配: 使用内存池避免动态内存分配 无锁算法: 在可能的情况下使用无锁数据结构 总结与最佳实践 #通过本文的分析和案例研究，我们可以总结出以下高频交易系统性能优化的最佳实践：\n系统化分析流程：\n建立性能基准 识别关键热点 分层分析问题 验证优化效果 关注关键性能指标：\n端到端延迟 缓存命中率 系统调用频率 上下文切换次数 常见优化策略：\n避免轮询，使用事件通知 优化内存布局和访问模式 减少锁竞争和线程切换 使用无锁算法和数据结构 预分配资源，避免运行时分配 持续监控与优化：\n建立性能监控机制 定期进行性能分析 在系统变更后重新评估性能 通过使用perf工具进行系统化的性能分析，结合对高频交易系统特性的深入理解，我们可以有效地识别和解决性能瓶颈，提高系统的响应速度和吞吐量，最终提升交易策略的执行效率和盈利能力。\n","date":"18 June 2025","permalink":"/blog/2025-06-18-how_to_use_perf/","section":"Blog","summary":"目录 # 高频交易系统性能优化思路 Perf 基础知识 perf record 命令参数详解 采样事件类型 Perf 能分析的关键指标 Perf Report 输出解析 列含义详解 分析方法论 高级分析技巧 实际优化流程 关键指标解读 使用Perf分析内存性能指标 高频交易系统案例分析 高频交易系统性能优化思路 #在高频交易系统中，微秒级的延迟差异可能直接影响交易策略的有效性和盈利能力。使用perf进行性能分析是优化高频交易系统的关键步骤。以下是一个系统化的优化思路：\n1. 性能基准建立 #关键指标:\n端到端延迟: 从行情接收到下单的完整路径时间 吞吐量: 每秒处理的订单/行情数量 尾延迟: 95/99/99.9百分位延迟 CPU利用率: 核心交易路径的CPU使用情况 # 建立基准性能数据 perf stat -e cycles,instructions,cache-references,cache-misses,branches,branch-misses -o perf_base.data -a -g ./strategyTrade 命令参数解释:\ncycles: CPU周期数，用于测量程序执行所需的处理器周期总量 instructions: 执行的指令数，结合cycles可计算IPC(每周期指令数)，评估CPU利用效率 cache-references: 缓存访问次数，表示程序对CPU缓存的总访问量 cache-misses: 缓存未命中次数，高缓存未命中率会导致处理器等待内存，增加延迟 branches: 分支指令执行次数，反映程序中条件判断和跳转的频率 branch-misses: 分支预测失败次数，高失败率会导致流水线刷新，降低CPU效率 -o: 指定输出文件名 -a: 收集所有CPU核心的数据，全系统视图 -g: 收集调用图信息，便于分析函数调用关系 输出示例及解读:\nPerformance counter stats for \u0026#39;./strategyTrade\u0026#39;: 12,345,678,901 cycles # 总CPU周期数 24,680,046,512 instructions # 总指令数，指令/周期比约为2.","title":"Perf Report 分析完全指南 - 高频交易系统性能优化"},{"content":"C++内存序与无锁编程 #引言 #在现代多核处理器上，高性能并发编程已经成为一项关键技能。C++11引入的原子操作和内存序模型为开发者提供了构建高效无锁数据结构的工具，但同时也带来了显著的复杂性。本文将深入探讨内存序的概念、不同内存序的语义差异，以及如何在实际应用中正确使用它们来构建高性能的无锁数据结构。\n内存模型基础 #什么是内存模型 #内存模型定义了多线程程序中内存操作的可见性和顺序性规则。C++内存模型主要关注三个方面：\n原子性(Atomicity): 操作是否可以被视为不可分割的整体 可见性(Visibility): 一个线程的写入何时对其他线程可见 顺序性(Ordering): 多个操作之间的执行顺序约束 重排序来源 #在现代计算机系统中，内存操作重排序可能来自三个层面：\n编译器重排序: 编译器为了优化可能改变指令顺序 CPU重排序: 处理器可能乱序执行指令或延迟写入主存 缓存一致性: 多核系统中每个核心的缓存可能暂时不一致 happens-before关系 #C++内存模型的核心是建立操作间的happens-before关系：\n如果操作A happens-before操作B，则A的结果对B可见 同一线程内的操作之间自动建立happens-before关系 跨线程的happens-before关系需要通过同步操作建立 内存栅栏(Memory Fence) #内存栅栏是一种同步原语，用于限制内存操作的重排序：\n// 完整内存栅栏 std::atomic_thread_fence(std::memory_order_seq_cst); // 获取栅栏 std::atomic_thread_fence(std::memory_order_acquire); // 释放栅栏 std::atomic_thread_fence(std::memory_order_release); 栅栏与原子操作的区别在于，栅栏影响所有内存操作，而不仅限于特定的原子变量。\n#pragma once #include \u0026lt;atomic\u0026gt; #include \u0026lt;array\u0026gt; #include \u0026lt;optional\u0026gt; #include \u0026lt;memory\u0026gt; #include \u0026lt;vector\u0026gt; namespace LockFreeQueues { // ============================================================================ // 1. 内存序详细演示 // ============================================================================ class MemoryOrderingDemo { private: std::atomic\u0026lt;int\u0026gt; data{0}; std::atomic\u0026lt;bool\u0026gt; flag{false}; public: // 演示不同内存序的行为差异 void demonstrateRelaxed() { // relaxed: 只保证原子性，允许重排序 data.store(42, std::memory_order_relaxed); flag.store(true, std::memory_order_relaxed); // 编译器/CPU可能重排这两个操作的顺序！ } void demonstrateAcquireRelease() { // release: 确保之前的操作不会重排到此操作之后 data.store(42, std::memory_order_relaxed); // 这个不会被重排到下面 flag.store(true, std::memory_order_release); // 释放操作 } bool readWithAcquire() { // acquire: 确保后续操作不会重排到此操作之前 if (flag.load(std::memory_order_acquire)) { // 获取操作 int value = data.load(std::memory_order_relaxed); // 这个不会被重排到上面 return value == 42; // 保证能看到data的修改 } return false; } void demonstrateSeqCst() { // seq_cst: 最强的内存序，全局一致的顺序 data.store(42, std::memory_order_seq_cst); flag.store(true, std::memory_order_seq_cst); // 所有线程看到的这些操作顺序都是一致的 } }; // ============================================================================ // 2. 单生产者单消费者队列 (SPSC) - 基础版本 // ============================================================================ template\u0026lt;typename T, size_t Capacity\u0026gt; class SPSCQueue { private: std::array\u0026lt;T, Capacity\u0026gt; buffer; // 关键：head和tail分别只被一个线程修改 alignas(64) std::atomic\u0026lt;size_t\u0026gt; head{0}; // 只被消费者修改 alignas(64) std::atomic\u0026lt;size_t\u0026gt; tail{0}; // 只被生产者修改 public: bool push(T item) { const size_t current_tail = tail.load(std::memory_order_relaxed); const size_t next_tail = (current_tail + 1) % Capacity; // acquire: 确保看到消费者的最新head值 if (next_tail == head.load(std::memory_order_acquire)) { return false; // 队列满 } buffer[current_tail] = std::move(item); // release: 确保数据写入完成后，消费者才能看到新的tail tail.store(next_tail, std::memory_order_release); return true; } std::optional\u0026lt;T\u0026gt; pop() { const size_t current_head = head.load(std::memory_order_relaxed); // acquire: 确保看到生产者的最新tail值 if (current_head == tail.load(std::memory_order_acquire)) { return std::nullopt; // 队列空 } T item = std::move(buffer[current_head]); // release: 确保数据读取完成后，生产者才能看到新的head head.store((current_head + 1) % Capacity, std::memory_order_release); return item; } }; // ============================================================================ // 3. 单生产者多消费者队列 (SPMC) // ============================================================================ template\u0026lt;typename T, size_t Capacity\u0026gt; class SPMCQueue { private: struct alignas(64) Element { std::atomic\u0026lt;T*\u0026gt; data{nullptr}; std::atomic\u0026lt;size_t\u0026gt; sequence{0}; }; std::array\u0026lt;Element, Capacity\u0026gt; buffer; alignas(64) std::atomic\u0026lt;size_t\u0026gt; tail{0}; // 生产者索引 // 每个消费者需要自己的head指针 static thread_local size_t consumer_head; public: SPMCQueue() { // 初始化序列号 for (size_t i = 0; i \u0026lt; Capacity; ++i) { buffer[i].sequence.store(i, std::memory_order_relaxed); } } bool push(T item) { const size_t pos = tail.load(std::memory_order_relaxed); Element\u0026amp; element = buffer[pos % Capacity]; // 等待这个位置可用（序列号匹配） const size_t expected_seq = pos; if (element.sequence.load(std::memory_order_acquire) != expected_seq) { return false; // 队列满或位置未就绪 } // 分配内存并存储数据 T* data_ptr = new T(std::move(item)); element.data.store(data_ptr, std::memory_order_relaxed); // 更新序列号，通知消费者数据就绪 element.sequence.store(expected_seq + 1, std::memory_order_release); // 推进tail tail.store(pos + 1, std::memory_order_relaxed); return true; } std::optional\u0026lt;T\u0026gt; pop() { Element\u0026amp; element = buffer[consumer_head % Capacity]; // 检查数据是否就绪 const size_t expected_seq = consumer_head + 1; if (element.sequence.load(std::memory_order_acquire) != expected_seq) { return std::nullopt; // 数据未就绪 } // 获取数据 T* data_ptr = element.data.load(std::memory_order_relaxed); T result = std::move(*data_ptr); delete data_ptr; element.data.store(nullptr, std::memory_order_relaxed); // 更新序列号，通知生产者位置可用 element.sequence.store(consumer_head + Capacity, std::memory_order_release); ++consumer_head; return result; } }; template\u0026lt;typename T, size_t Capacity\u0026gt; thread_local size_t SPMCQueue\u0026lt;T, Capacity\u0026gt;::consumer_head = 0; // ============================================================================ // 4. 多生产者单消费者队列 (MPSC) - 使用CAS // ============================================================================ template\u0026lt;typename T, size_t Capacity\u0026gt; class MPSCQueue { private: struct Node { std::atomic\u0026lt;T*\u0026gt; data{nullptr}; std::atomic\u0026lt;Node*\u0026gt; next{nullptr}; Node() = default; ~Node() { T* ptr = data.load(std::memory_order_relaxed); delete ptr; } }; alignas(64) std::atomic\u0026lt;Node*\u0026gt; head; // 消费者读取点 alignas(64) std::atomic\u0026lt;Node*\u0026gt; tail; // 生产者写入点 public: MPSCQueue() { Node* dummy = new Node; head.store(dummy, std::memory_order_relaxed); tail.store(dummy, std::memory_order_relaxed); } ~MPSCQueue() { while (Node* old_head = head.load(std::memory_order_relaxed)) { head.store(old_head-\u0026gt;next.load(std::memory_order_relaxed), std::memory_order_relaxed); delete old_head; } } void push(T item) { Node* new_node = new Node; T* data_ptr = new T(std::move(item)); new_node-\u0026gt;data.store(data_ptr, std::memory_order_relaxed); // 多生产者需要用CAS来原子地更新tail Node* prev_tail = tail.exchange(new_node, std::memory_order_acq_rel); // 链接前一个节点到新节点 prev_tail-\u0026gt;next.store(new_node, std::memory_order_release); } std::optional\u0026lt;T\u0026gt; pop() { Node* current_head = head.load(std::memory_order_relaxed); Node* next = current_head-\u0026gt;next.load(std::memory_order_acquire); if (next == nullptr) { return std::nullopt; // 队列空 } // 获取数据 T* data_ptr = next-\u0026gt;data.load(std::memory_order_relaxed); T result = std::move(*data_ptr); delete data_ptr; next-\u0026gt;data.store(nullptr, std::memory_order_relaxed); // 移动head指针 head.store(next, std::memory_order_release); delete current_head; return result; } }; // ============================================================================ // 5. 多生产者多消费者队列 (MPMC) - 最复杂的实现 // ============================================================================ template\u0026lt;typename T, size_t Capacity\u0026gt; class MPMCQueue { private: struct Cell { std::atomic\u0026lt;T*\u0026gt; data{nullptr}; std::atomic\u0026lt;size_t\u0026gt; sequence{0}; }; static constexpr size_t CACHELINE_SIZE = 64; // 缓存行对齐，避免伪共享 alignas(CACHELINE_SIZE) std::array\u0026lt;Cell, Capacity\u0026gt; buffer; alignas(CACHELINE_SIZE) std::atomic\u0026lt;size_t\u0026gt; enqueue_pos{0}; alignas(CACHELINE_SIZE) std::atomic\u0026lt;size_t\u0026gt; dequeue_pos{0}; public: MPMCQueue() { // 初始化序列号 for (size_t i = 0; i \u0026lt; Capacity; ++i) { buffer[i].sequence.store(i, std::memory_order_relaxed); } } bool push(T item) { Cell* cell; size_t pos = enqueue_pos.load(std::memory_order_relaxed); for (;;) { cell = \u0026amp;buffer[pos % Capacity]; size_t seq = cell-\u0026gt;sequence.load(std::memory_order_acquire); intptr_t diff = (intptr_t)seq - (intptr_t)pos; if (diff == 0) { // 尝试占用这个位置 if (enqueue_pos.compare_exchange_weak(pos, pos + 1, std::memory_order_relaxed)) { break; } } else if (diff \u0026lt; 0) { return false; // 队列满 } else { pos = enqueue_pos.load(std::memory_order_relaxed); } } // 存储数据 T* data_ptr = new T(std::move(item)); cell-\u0026gt;data.store(data_ptr, std::memory_order_relaxed); // 通知消费者数据就绪 cell-\u0026gt;sequence.store(pos + 1, std::memory_order_release); return true; } std::optional\u0026lt;T\u0026gt; pop() { Cell* cell; size_t pos = dequeue_pos.load(std::memory_order_relaxed); for (;;) { cell = \u0026amp;buffer[pos % Capacity]; size_t seq = cell-\u0026gt;sequence.load(std::memory_order_acquire); intptr_t diff = (intptr_t)seq - (intptr_t)(pos + 1); if (diff == 0) { // 尝试占用这个位置 if (dequeue_pos.compare_exchange_weak(pos, pos + 1, std::memory_order_relaxed)) { break; } } else if (diff \u0026lt; 0) { return std::nullopt; // 队列空 } else { pos = dequeue_pos.load(std::memory_order_relaxed); } } // 获取数据 T* data_ptr = cell-\u0026gt;data.load(std::memory_order_relaxed); T result = std::move(*data_ptr); delete data_ptr; cell-\u0026gt;data.store(nullptr, std::memory_order_relaxed); // 通知生产者位置可用 cell-\u0026gt;sequence.store(pos + Capacity, std::memory_order_release); return result; } }; // ============================================================================ // 6. 使用示例和性能对比 // ============================================================================ class QueueBenchmark { public: static void demonstrateUsage() { // SPSC队列 - 最快，适用于单线程生产消费 SPSCQueue\u0026lt;int, 1024\u0026gt; spsc_queue; spsc_queue.push(42); auto result1 = spsc_queue.pop(); // MPSC队列 - 多个生产者，一个消费者 MPSCQueue\u0026lt;int, 1024\u0026gt; mpsc_queue; mpsc_queue.push(42); auto result2 = mpsc_queue.pop(); // MPMC队列 - 最通用但最复杂 MPMCQueue\u0026lt;int, 1024\u0026gt; mpmc_queue; mpmc_queue.push(42); auto result3 = mpmc_queue.pop(); } // 性能特征说明： // SPSC: ~10-20ns 延迟，最高吞吐量 // SPMC: ~50-100ns 延迟，适中吞吐量 // MPSC: ~100-200ns 延迟，需要CAS操作 // MPMC: ~200-500ns 延迟，最复杂的同步 }; } // namespace LockFreeQueues /* 内存序使用原则总结： 1. memory_order_relaxed: - 只保证原子性，无同步语义 - 用于计数器、统计等场景 2. memory_order_acquire/release: - 形成同步点，建立happens-before关系 - acquire防止后续操作前移 - release防止之前操作后移 - 配对使用实现线程间同步 3. memory_order_acq_rel: - 同时具有acquire和release语义 - 用于read-modify-write操作 4. memory_order_seq_cst: - 最强的内存序，全局一致顺序 - 性能开销最大，但最容易理解 选择内存序的关键是理解数据依赖关系和同步需求！ */ ## 常见错误模式与调试技巧 ### 常见错误 1. **错误的内存序选择** ```cpp // 错误：缺少同步 std::atomic\u0026lt;bool\u0026gt; ready{false}; int data = 0; // 线程1 data = 42; ready.store(true, std::memory_order_relaxed); // 错误！应使用release // 线程2 if (ready.load(std::memory_order_relaxed)) { // 错误！应使用acquire assert(data == 42); // 可能失败 } 遗漏同步点 // 错误：同步不完整 std::atomic\u0026lt;int\u0026gt; flag1{0}, flag2{0}; // 线程1 flag1.store(1, std::memory_order_release); // 线程2 if (flag1.load(std::memory_order_acquire)) { flag2.store(1, std::memory_order_relaxed); // 错误！应使用release } // 线程3 if (flag2.load(std::memory_order_relaxed)) { // 错误！应使用acquire // 无法保证看到线程1的写入 } 过度使用顺序一致性 // 性能不佳：过度使用seq_cst std::atomic\u0026lt;int\u0026gt; counter{0}; // 在高频计数场景中使用默认的seq_cst会导致性能下降 counter.fetch_add(1); // 默认使用memory_order_seq_cst // 更好的做法 counter.fetch_add(1, std::memory_order_relaxed); 调试技巧 # 使用内存检查工具\nThreadSanitizer (TSan)：检测数据竞争 Helgrind：检测同步错误 Intel Inspector：深入分析并发问题 压力测试\n在不同CPU架构上运行测试 使用随机延迟和线程调度来暴露竞争条件 审查模式\n从最严格的内存序开始（seq_cst） 在确保正确性后，逐步放宽约束以提高性能 记录每个原子变量的同步意图和约束 实际应用场景 #1. 高性能日志系统 #无锁队列常用于实现高性能日志系统，其中多个线程可以并发写入日志，而单个后台线程负责将日志写入磁盘：\n// 应用线程使用MPSC队列写入日志 MPSCQueue\u0026lt;LogEntry, 8192\u0026gt; log_queue; // 应用线程 void application_thread() { while (running) { // ... 业务逻辑 ... log_queue.push(LogEntry{\u0026#34;操作完成\u0026#34;, LogLevel::INFO}); } } // 日志线程 void logger_thread() { while (running) { auto entry = log_queue.pop(); if (entry) { write_to_disk(*entry); } else { std::this_thread::sleep_for(std::chrono::milliseconds(1)); } } } 2. 事件处理系统 #游戏引擎和GUI系统常用无锁队列处理事件：\n// 多个线程产生事件，主线程处理 MPSCQueue\u0026lt;Event, 1024\u0026gt; event_queue; // 主线程事件循环 void main_event_loop() { while (running) { // 处理所有待处理事件 while (auto event = event_queue.pop()) { dispatch_event(*event); } // 渲染和其他主线程工作 render_frame(); } } 3. 工作窃取调度器 #多生产者多消费者队列可用于实现工作窃取调度器：\n// 每个工作线程有自己的任务队列 std::vector\u0026lt;MPMCQueue\u0026lt;Task, 256\u0026gt;\u0026gt; thread_queues; void worker_thread(int id) { while (running) { // 尝试从自己的队列获取任务 auto task = thread_queues[id].pop(); if (!task) { // 窃取其他线程的任务 for (int i = 0; i \u0026lt; thread_queues.size(); i++) { if (i == id) continue; task = thread_queues[i].pop(); if (task) break; } } if (task) { execute_task(*task); } else { std::this_thread::yield(); } } } 结论 #内存序是构建高性能并发系统的关键工具，但也是C++中最容易被误用的特性之一。通过理解不同内存序的语义和适用场景，开发者可以在保证正确性的同时最大化性能。\n无锁数据结构虽然实现复杂，但在高性能场景下能提供显著的吞吐量和延迟优势。本文展示的队列实现可以作为构建自己的无锁系统的起点，但请记住：\n从简单开始，逐步优化 彻底测试每个实现 衡量性能收益是否值得复杂性增加 最后，除非有明确的性能需求，否则优先考虑标准库提供的线程安全容器和同步原语，它们通常能满足大多数应用场景的需求。\n","date":"11 June 2025","permalink":"/blog/2025-06-24-memory_ordering_in_cpp/","section":"Blog","summary":"C++内存序与无锁编程 #引言 #在现代多核处理器上，高性能并发编程已经成为一项关键技能。C++11引入的原子操作和内存序模型为开发者提供了构建高效无锁数据结构的工具，但同时也带来了显著的复杂性。本文将深入探讨内存序的概念、不同内存序的语义差异，以及如何在实际应用中正确使用它们来构建高性能的无锁数据结构。\n内存模型基础 #什么是内存模型 #内存模型定义了多线程程序中内存操作的可见性和顺序性规则。C++内存模型主要关注三个方面：\n原子性(Atomicity): 操作是否可以被视为不可分割的整体 可见性(Visibility): 一个线程的写入何时对其他线程可见 顺序性(Ordering): 多个操作之间的执行顺序约束 重排序来源 #在现代计算机系统中，内存操作重排序可能来自三个层面：\n编译器重排序: 编译器为了优化可能改变指令顺序 CPU重排序: 处理器可能乱序执行指令或延迟写入主存 缓存一致性: 多核系统中每个核心的缓存可能暂时不一致 happens-before关系 #C++内存模型的核心是建立操作间的happens-before关系：\n如果操作A happens-before操作B，则A的结果对B可见 同一线程内的操作之间自动建立happens-before关系 跨线程的happens-before关系需要通过同步操作建立 内存栅栏(Memory Fence) #内存栅栏是一种同步原语，用于限制内存操作的重排序：\n// 完整内存栅栏 std::atomic_thread_fence(std::memory_order_seq_cst); // 获取栅栏 std::atomic_thread_fence(std::memory_order_acquire); // 释放栅栏 std::atomic_thread_fence(std::memory_order_release); 栅栏与原子操作的区别在于，栅栏影响所有内存操作，而不仅限于特定的原子变量。\n#pragma once #include \u0026lt;atomic\u0026gt; #include \u0026lt;array\u0026gt; #include \u0026lt;optional\u0026gt; #include \u0026lt;memory\u0026gt; #include \u0026lt;vector\u0026gt; namespace LockFreeQueues { // ============================================================================ // 1. 内存序详细演示 // ============================================================================ class MemoryOrderingDemo { private: std::atomic\u0026lt;int\u0026gt; data{0}; std::atomic\u0026lt;bool\u0026gt; flag{false}; public: // 演示不同内存序的行为差异 void demonstrateRelaxed() { // relaxed: 只保证原子性，允许重排序 data.","title":"内存序"},{"content":"1. 分类 #有三种不同的模版类型，\nFunction templates class templates Variable templates 1.1. function templates #template\u0026lt;typename T\u0026gt; T max(T a, T b) { return (a \u0026gt; b) ? a : b; } // 使用：编译器自动推导类型 int x = max(3, 7); // T = int double y = max(3.14, 2.71); // T = double 多参数模版 template\u0026lt;typename T, typename U\u0026gt; auto add(T a, U b) { return a + b; } 函数模板的显式实例化 // 声明模板函数 template\u0026lt;typename T\u0026gt; void process(T value) { // 实现... } // 显式实例化特定类型版本 template void process\u0026lt;int\u0026gt;(int); // 显式实例化int版本 template void process\u0026lt;double\u0026gt;(double); // 显式实例化double版本 可变参数模板函数 // 递归终止条件 void print() { std::cout \u0026lt;\u0026lt; std::endl; } // 可变参数模板 (C++11) template\u0026lt;typename T, typename... Args\u0026gt; void print(T first, Args... rest) { std::cout \u0026lt;\u0026lt; first \u0026lt;\u0026lt; \u0026#34; \u0026#34;; print(rest...); // 递归调用处理剩余参数 } // 使用折叠表达式 (C++17) template\u0026lt;typename... Args\u0026gt; void printAll(Args... args) { (std::cout \u0026lt;\u0026lt; ... \u0026lt;\u0026lt; args) \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; // 折叠表达式 } // 使用 print(1, \u0026#34;hello\u0026#34;, 3.14, \u0026#39;c\u0026#39;); // 输出: 1 hello 3.14 c printAll(1, \u0026#34;hello\u0026#34;, 3.14, \u0026#39;c\u0026#39;); // 输出: 1hello3.14c 约束与概念 (C++20) // 使用requires表达式 template\u0026lt;typename T\u0026gt; requires std::integral\u0026lt;T\u0026gt; T gcd(T a, T b) { if (b == 0) return a; return gcd(b, a % b); } // 使用概念的简写形式 template\u0026lt;std::integral T\u0026gt; T lcm(T a, T b) { return (a / gcd(a, b)) * b; } // 使用auto参数简写 (C++20) auto sum(std::integral auto a, std::integral auto b) { return a + b; } SFINAE与类型特性 // 使用std::enable_if进行SFINAE (C++11) template\u0026lt;typename T, typename = std::enable_if_t\u0026lt;std::is_arithmetic_v\u0026lt;T\u0026gt;\u0026gt;\u0026gt; T square(T x) { return x * x; } // 使用tag dispatching区分类型处理 template\u0026lt;typename Iterator\u0026gt; void advance_impl(Iterator\u0026amp; it, int n, std::random_access_iterator_tag) { // 随机访问迭代器可以直接跳跃 it += n; } template\u0026lt;typename Iterator\u0026gt; void advance_impl(Iterator\u0026amp; it, int n, std::bidirectional_iterator_tag) { // 双向迭代器需要循环移动 if (n \u0026gt; 0) { while (n--) ++it; } else { while (n++) --it; } } template\u0026lt;typename Iterator\u0026gt; void advance(Iterator\u0026amp; it, int n) { advance_impl(it, n, typename std::iterator_traits\u0026lt;Iterator\u0026gt;::iterator_category()); } 实际应用案例：通用算法实现 // 泛型快速排序实现 template\u0026lt;typename RandomIt\u0026gt; void quicksort(RandomIt first, RandomIt last) { if (first \u0026lt; last) { auto pivot = *std::next(first, std::distance(first, last) / 2); auto middle1 = std::partition(first, last, [pivot](const auto\u0026amp; em) { return em \u0026lt; pivot; }); auto middle2 = std::partition(middle1, last, [pivot](const auto\u0026amp; em) { return !(pivot \u0026lt; em); }); quicksort(first, middle1); quicksort(middle2, last); } } // 使用 std::vector\u0026lt;int\u0026gt; v = {5, 2, 9, 1, 7, 6, 3}; quicksort(v.begin(), v.end()); // v现在已排序 1.2. class templates # 基础语法 template\u0026lt;typename T\u0026gt; class Vector { private: T* data; size_t size_; public: Vector() : data(nullptr), size_(0) {} void push_back(const T\u0026amp; value) { // 实现... } T\u0026amp; operator[](size_t index) { return data[index]; } }; // 使用：必须明确指定类型 Vector\u0026lt;int\u0026gt; int_vec; Vector\u0026lt;string\u0026gt; str_vec; 非类型模版参数 template\u0026lt;typename T, size_t N\u0026gt; class Array { T data[N]; // 编译时确定大小 public: size_t size() const { return N; } }; Array\u0026lt;int, 10\u0026gt; arr; // 大小为10的int数组 1.3. 模版特化 # 函数模版特化 // 通用版本 template\u0026lt;typename T\u0026gt; void print(T value) { cout \u0026lt;\u0026lt; value; } // 特化版本 template\u0026lt;\u0026gt; void print\u0026lt;const char*\u0026gt;(const char* value) { cout \u0026lt;\u0026lt; \u0026#34;String: \u0026#34; \u0026lt;\u0026lt; value; } 类模版特化 // 通用版本 template\u0026lt;typename T\u0026gt; class Storage { T data; }; // 针对bool的特化 template\u0026lt;\u0026gt; class Storage\u0026lt;bool\u0026gt; { // 特殊的bool存储实现 }; 编译机制 模版是编译时生成代码，不是运行时多态,每种类型都会生成对应的代码实例\nC++模板类用法详解 - 以LockFreeRingBuffer为例 #C++模板类是一种强大的编程工具，允许我们编写通用代码，同时保持类型安全和高性能。我将结合LockFreeRingBuffer这个实际例子来详细讲解。\n#pragma once #include \u0026lt;atomic\u0026gt; #include \u0026lt;array\u0026gt; template\u0026lt;typename T, size_t Size\u0026gt; class LockFreeRingBuffer { static_assert((Size \u0026amp; (Size - 1)) == 0, \u0026#34;Size must be power of 2\u0026#34;); struct alignas(64) Item { // 避免false sharing std::atomic\u0026lt;bool\u0026gt; valid{false}; T data; }; static constexpr size_t MASK = Size - 1; std::array\u0026lt;Item, Size\u0026gt; buffer; alignas(64) std::atomic\u0026lt;size_t\u0026gt; write_index{0}; alignas(64) std::atomic\u0026lt;size_t\u0026gt; read_index{0}; public: bool try_push(const T\u0026amp; item) noexcept { const size_t current = write_index.load(std::memory_order_relaxed); const size_t next = (current + 1) \u0026amp; MASK; if (next == read_index.load(std::memory_order_acquire)) { return false; // buffer is full } buffer[current].data = item; buffer[current].valid.store(true, std::memory_order_release); write_index.store(next, std::memory_order_release); return true; } bool try_pop(T\u0026amp; item) noexcept { const size_t current = read_index.load(std::memory_order_relaxed); if (current == write_index.load(std::memory_order_acquire)) { return false; // buffer is empty } if (!buffer[current].valid.load(std::memory_order_acquire)) { return false; // data not ready } item = std::move(buffer[current].data); buffer[current].valid.store(false, std::memory_order_release); read_index.store((current + 1) \u0026amp; MASK, std::memory_order_release); return true; } }; // 实例化 Common::LockFreeRingBuffer\u0026lt;AggTradeQueueData, QUEUE_SIZE\u0026gt; aggTrade_queue; 1. 模板类的定义 #template\u0026lt;typename T, size_t Size\u0026gt; class LockFreeRingBuffer { // 类的实现... }; 这个声明有两个模板参数：\ntypename T：类型参数，表示缓冲区中存储的数据类型 size_t Size：非类型参数，表示缓冲区的大小（必须是编译时常量） 2. 模板类的实例化 #在实际使用中，通过指定具体的类型和值来创建特定的缓冲区：\n// 创建存储AggTradeQueueData类型数据的缓冲区，大小为8192 Common::LockFreeRingBuffer\u0026lt;AggTradeQueueData, QUEUE_SIZE\u0026gt; aggTrade_queue; // 创建存储TickerQueueData类型数据的缓冲区，大小为8192 Common::LockFreeRingBuffer\u0026lt;TickerQueueData, QUEUE_SIZE\u0026gt; ticker_queue; 编译器会为每种不同的模板参数组合生成不同的类代码。\n3. 内部数据结构的适配 #模板使得内部数据结构可以根据类型自动适配：\nstruct alignas(64) Item { std::atomic\u0026lt;bool\u0026gt; valid{false}; T data; // 这里的T会被替换为实际类型 }; std::array\u0026lt;Item, Size\u0026gt; buffer; // Size会被替换为实际大小 当使用LockFreeRingBuffer\u0026lt;AggTradeQueueData, 8192\u0026gt;时，编译器生成的代码相当于：\nstruct Item { std::atomic\u0026lt;bool\u0026gt; valid{false}; AggTradeQueueData data; // T被替换为AggTradeQueueData }; std::array\u0026lt;Item, 8192\u0026gt; buffer; // Size被替换为8192 4. 模板特化的高级用法 #虽然在这个例子中没有使用，但模板还支持特化，为特定类型提供优化的实现：\n// 主模板 template\u0026lt;typename T, size_t Size\u0026gt; class LockFreeRingBuffer { /*...*/ }; // 为特定类型的特化版本 template\u0026lt;size_t Size\u0026gt; class LockFreeRingBuffer\u0026lt;int, Size\u0026gt; { /*针对int类型的优化实现*/ }; 8. 模板的优势 #从LockFreeRingBuffer的例子可以看出模板的优势：\n代码复用：同一套缓冲区逻辑用于多种数据类型 类型安全：编译时类型检查避免运行时错误 零开销抽象：模板在编译时展开，没有运行时开销 灵活性：可以处理任何符合接口的数据类型 性能优化：编译器可以为特定类型生成优化代码 总结：C++模板类允许我们编写一次代码，适用于多种数据类型，同时保持类型安全和高性能。在高性能系统如交易系统中，这种能力尤为重要。\n","date":"10 June 2025","permalink":"/blog/2025-06-24-cpp_template_class_guide/","section":"Blog","summary":"1. 分类 #有三种不同的模版类型，\nFunction templates class templates Variable templates 1.1. function templates #template\u0026lt;typename T\u0026gt; T max(T a, T b) { return (a \u0026gt; b) ? a : b; } // 使用：编译器自动推导类型 int x = max(3, 7); // T = int double y = max(3.14, 2.71); // T = double 多参数模版 template\u0026lt;typename T, typename U\u0026gt; auto add(T a, U b) { return a + b; } 函数模板的显式实例化 // 声明模板函数 template\u0026lt;typename T\u0026gt; void process(T value) { // 实现.","title":"c++ 模版"},{"content":"这篇论文 《Optimal High-Frequency Market Making》 实现并分析了 Avellaneda-Stoikov (2008) 的高频做市定价模型，并引入了一个动态库存控制模块，用于优化限价单的挂单量，以在保证盈利的同时控制库存风险。下面是详细解读：\n📌 一、研究背景与动机 #高频做市商（HFT market makers）通过在订单簿中持续挂出买卖限价单来提供流动性，赚取 买卖价差（spread） 和 交易所提供的挂单返利（rebate）。但这同时会产生库存风险（inventory risk），即买入或卖出过多后，价格波动带来的风险。\nAvellaneda-Stoikov 模型是其中一个经典的高频做市定价框架，它在假设股票价格服从布朗运动的基础上，通过求解最优控制问题得出最优报价策略。\n📌 二、模型框架 #2.1 定价模型（Pricing） #基于 Avellaneda \u0026amp; Stoikov (2008)：\n股票价格服从布朗运动： $dS_t = \\sigma dW_t$ 市场深度与成交概率关系：$\\lambda(\\delta) = A e^{-\\kappa \\delta}$ 做市商目标是最大化终端时刻 $T$ 时的指数效用函数： $$ \\max_{\\delta_a, \\delta_b} \\mathbb{E}[-e^{-\\gamma (X_T + q_T S_T)}] $$\n推导结果是：\n中间价（Indifference Price）： $$ r(s, t) = s - q\\gamma\\sigma^2(T - t) $$\n最优总挂单价差（Spread）： $$ \\delta_a + \\delta_b = \\gamma\\sigma^2(T - t) + \\ln\\left(1 + \\frac{\\gamma}{\\kappa} \\right) $$\n👉 这个模型体现了：\n离市场收盘越近，价差越小（为了减少隔夜风险，变得更激进） 持有的库存 $q$ 越大，中间价越偏离市场中价 2.2 库存控制模型（Inventory Control） #为解决 Avellaneda-Stoikov 模型中“不限制库存大小”的问题，作者引入了一个动态调节挂单数量的模型：\n$$ \\begin{cases} \\phi_{\\text{bid}}^t = \\phi_{\\text{max}}^t \u0026amp; \\text{if } q_t \u0026lt; 0 \\ \\phi_{\\text{bid}}^t = \\phi_{\\text{max}}^t e^{-\\eta q_t} \u0026amp; \\text{if } q_t \u0026gt; 0 \\end{cases} \\quad \\begin{cases} \\phi_{\\text{ask}}^t = \\phi_{\\text{max}}^t \u0026amp; \\text{if } q_t \u0026gt; 0 \\ \\phi_{\\text{ask}}^t = \\phi_{\\text{max}}^t e^{-\\eta q_t} \u0026amp; \\text{if } q_t \u0026lt; 0 \\end{cases} $$\n这个设计逻辑是：\n当前若持有过多某一方向的头寸（如多头），则减少挂出同方向单的数量 达到库存中性目标（inventory mean-reversion） 2.3 算法流程 #策略遵循以下流程：\n若订单簿中无挂单，挂出最优买卖报价； 若仅一边挂单被成交，则等待 5 秒，若未成交另一边则取消并重新挂单； 若两边都在订单簿中，每隔 1 秒刷新一次报价； 📌 三、交易模拟器设计 #构建了一个简化的交易环境用于模拟：\n市场订单到达遵循时间非齐次泊松过程，强度： $$ \\lambda(t, \\xi) = \\alpha_t e^{-\\mu \\xi} $$\n其中 $\\alpha_t$ 是随时间变化的成交活跃度（“浴缸曲线”），$\\xi$ 是订单簿深度。\n成交事件模拟使用贝努利分布 $Ber(\\lambda(t, \\xi)\\Delta)$，部分成交使用 Gamma 分布模拟。 📌 四、实证结果与对比 #对 S\u0026amp;P500 中具有不同特性的五只股票（如 AAPL, AMZN, GE）进行实验，比较本文策略（optimal）与基线策略（baseline，始终挂在最优买卖价）：\n🔸 核心结论： # 本文策略在多数股票上有 更高或相近的利润； 库存控制更稳定（位置更接近 0，方差更小）； 使用更少的挂单次数完成相似或更优的成交； 盈利方差更小，收益更稳健； 🔸 样例结果（以 AAPL 为例）： # 策略 平均每日 PnL 平均每日库存 PnL 方差 库存方差 Optimal -988.54 0.86 289.82 63.66 Baseline -1093.60 7.53 357.66 112.20 📌 五、马尔可夫链分析 #将做市过程建模为马尔可夫过程，状态空间为：\nQuoting: 正常挂单； Waiting: 一侧成交，另一侧等待； Spread: 成功赚到买卖价差； 引入两个性能指标：\n成功捕获价差的概率 $p^*$ 单边成交（未对冲）概率 $q^*$ 股票 Optimal $p^*$ Baseline $p^*$ Optimal $q^*$ Baseline $q^*$ AAPL 2.6% 5.1% 0.8% 0.9% AMZN 19.3% 4.7% 1.9% 1.0% 👉 虽然 Baseline 策略挂得更激进，捕获价差的概率更高，但 Optimal 策略的单边成交概率更低，说明更有效地控制了库存风险。\n✅ 结论总结 # 本文将 Avellaneda-Stoikov 的模型扩展为一个 可实际运行的高频做市策略； 通过库存控制模块，使策略能在不停止交易的前提下控制风险； 实验结果验证其在多个维度优于基准策略； 提出进一步改进方向：引入对中间价变化与订单到达的预测。 ref #https://stanford.edu/class/msande448/2018/Final/Reports/gr5.pdf\n","date":"19 May 2025","permalink":"/blog/2025-06-24-avellaneda_stoikov_market_making/","section":"Blog","summary":"这篇论文 《Optimal High-Frequency Market Making》 实现并分析了 Avellaneda-Stoikov (2008) 的高频做市定价模型，并引入了一个动态库存控制模块，用于优化限价单的挂单量，以在保证盈利的同时控制库存风险。下面是详细解读：\n📌 一、研究背景与动机 #高频做市商（HFT market makers）通过在订单簿中持续挂出买卖限价单来提供流动性，赚取 买卖价差（spread） 和 交易所提供的挂单返利（rebate）。但这同时会产生库存风险（inventory risk），即买入或卖出过多后，价格波动带来的风险。\nAvellaneda-Stoikov 模型是其中一个经典的高频做市定价框架，它在假设股票价格服从布朗运动的基础上，通过求解最优控制问题得出最优报价策略。\n📌 二、模型框架 #2.1 定价模型（Pricing） #基于 Avellaneda \u0026amp; Stoikov (2008)：\n股票价格服从布朗运动： $dS_t = \\sigma dW_t$ 市场深度与成交概率关系：$\\lambda(\\delta) = A e^{-\\kappa \\delta}$ 做市商目标是最大化终端时刻 $T$ 时的指数效用函数： $$ \\max_{\\delta_a, \\delta_b} \\mathbb{E}[-e^{-\\gamma (X_T + q_T S_T)}] $$\n推导结果是：\n中间价（Indifference Price）： $$ r(s, t) = s - q\\gamma\\sigma^2(T - t) $$\n最优总挂单价差（Spread）： $$ \\delta_a + \\delta_b = \\gamma\\sigma^2(T - t) + \\ln\\left(1 + \\frac{\\gamma}{\\kappa} \\right) $$","title":"MmAvellaneda Stoikov"},{"content":"原始解析方案的性能瓶颈 #原始的 Binance 聚合交易数据解析实现存在多个性能瓶颈，这在高频交易系统中尤为关键。主要问题包括：\n使用 std::stod 进行字符串到浮点数转换：\nresult.data.price = std::stod(std::string(price_str)); result.data.quantity = std::stod(std::string(qty_str)); 这里存在两个严重问题：\nstd::stod 在底层实现中需要处理各种格式和本地化，导致计算开销大 每次调用都创建了临时 std::string 对象，增加了内存分配和释放的开销 创建临时的 padded_string 对象：\nsimdjson::padded_string padded_json{json}; simdjson::dom::element doc = parser.parse(padded_json); 这会导致额外的内存分配和复制，特别是在高频率处理消息时变得非常明显。\n使用低效的字符串复制方法：\nstrncpy(result.data.symbol, doc[\u0026#34;s\u0026#34;].get_string().value().data(), sizeof(result.data.symbol) - 1); 标准的 strncpy 没有利用现代 CPU 的 SIMD 指令集优势。\n异常处理成本：在解析热路径中大量使用 try-catch 结构，这会导致编译器生成额外代码，影响性能。\n重复获取 JSON 节点：多次访问相同的 JSON 节点，每次都需要进行字符串哈希查找。\n优化方案 #为了解决上述问题，我们实施了多层次的优化策略：\n1. 自定义快速解析路径 #创建了一个专门针对 Binance 聚合交易数据格式的快速解析函数，完全跳过通用 JSON 解析器：\nbool fastParseAggTrade(const std::string_view\u0026amp; json, Common::QuoteData::AggTradeData\u0026amp; data) noexcept { // 快速检查消息类型 const char* type_pattern = \u0026#34;\\\u0026#34;e\\\u0026#34;:\\\u0026#34;aggTrade\\\u0026#34;\u0026#34;; if (json.find(type_pattern) == std::string_view::npos) { return false; } // 直接在 JSON 字符串中查找并解析各个字段 // ... } 这种方法直接在字符串上操作，避免了构建整个 DOM 树的开销。\n2. 高效的字符串到浮点数转换 #实现了一个高度优化的 fastStringToDouble 函数，具有多层次优化：\nstatic double fastStringToDouble(const std::string_view\u0026amp; sv) noexcept { // 快速路径：尝试检测整数格式 bool is_negative = sv[0] == \u0026#39;-\u0026#39;; size_t start_idx = is_negative ? 1 : 0; // 检查是否是简单整数（无小数点，无科学计数法） bool is_simple_int = true; for (size_t i = start_idx; i \u0026lt; sv.size(); ++i) { if (sv[i] \u0026lt; \u0026#39;0\u0026#39; || sv[i] \u0026gt; \u0026#39;9\u0026#39;) { is_simple_int = false; break; } } // 对于简单整数，使用快速整数解析路径 if (is_simple_int \u0026amp;\u0026amp; sv.size() \u0026lt;= 18) { uint64_t value = 0; for (size_t i = start_idx; i \u0026lt; sv.size(); ++i) { value = value * 10 + (sv[i] - \u0026#39;0\u0026#39;); } return is_negative ? -static_cast\u0026lt;double\u0026gt;(value) : static_cast\u0026lt;double\u0026gt;(value); } // 通用路径：使用std::from_chars double result = 0.0; auto [ptr, ec] = std::from_chars(sv.data(), sv.data() + sv.size(), result); // 只有在from_chars失败时才回退到std::stod if (ec == std::errc() \u0026amp;\u0026amp; ptr == sv.data() + sv.size()) { return result; } return std::stod(std::string(sv)); } 这个实现有几个关键优化点：\n快速整数路径：对于纯整数格式，使用直接的整数解析算法 使用 std::from_chars，它比 std::stod 快得多 只在必要时才回退到昂贵的 std::stod 方法 3. SIMD 优化的字符串复制 #使用 SIMD 指令集优化字符串复制操作：\nstatic void fastStringCopy(char* dest, const std::string_view\u0026amp; src, size_t max_len) noexcept { size_t len = std::min(src.size(), max_len - 1); __m256i* dest_ptr = reinterpret_cast\u0026lt;__m256i*\u0026gt;(dest); __m256i* src_ptr = reinterpret_cast\u0026lt;__m256i*\u0026gt;(const_cast\u0026lt;char*\u0026gt;(src.data())); _mm256_storeu_si256(dest_ptr, _mm256_loadu_si256(src_ptr)); dest[len] = \u0026#39;\\0\u0026#39;; } 这使用了 AVX2 指令集的 _mm256_loadu_si256 和 _mm256_storeu_si256 指令，一次复制 32 字节，显著提高了字符串复制的速度。\n4. 批量获取 JSON 字段 #优化后的代码一次性获取所有需要的字段，减少了重复的查找操作：\n// 批量获取所有字段，减少函数调用开销 auto error1 = doc[\u0026#34;s\u0026#34;].get_string().get(symbol_str); auto error2 = doc[\u0026#34;E\u0026#34;].get_uint64().get(timestamp); // ... 其他字段批量获取 5. 错误处理优化 #使用错误码而非异常处理，并应用分支预测提示：\nif (UNLIKELY(error)) { result.is_valid = false; result.error_message = \u0026#34;JSON解析错误\u0026#34;; return result; } UNLIKELY 宏提示编译器这个条件很少发生，使主执行路径更加顺畅。\n6. 避免临时对象创建 #优化代码直接在原始 JSON 数据上操作，避免创建临时对象：\n// 避免创建临时的padded_string对象 simdjson::dom::element doc; auto error = parser.parse(json.data(), json.size()).get(doc); 性能提升分析 #优化后的实现在几个关键方面显著提高了性能：\n字符串到浮点数转换速度提升：\n使用 fastStringToDouble 比原来的 std::stod(std::string(price_str)) 快 10-100 倍 整数快速路径对于纯整数数据（如某些价格和数量）可提供额外 2-3 倍的加速 内存分配减少：\n避免了 std::string 和 padded_string 的临时对象创建 在高频交易系统中，这不仅减少了 CPU 开销，还减轻了 GC 压力 SIMD 加速：\nSIMD 优化的字符串复制可以比 strncpy 快 4-8 倍 这对于交易系统中频繁的字符串操作特别有益 直接字符串解析路径：\n跳过 JSON 解析器可以减少 70-90% 的解析开销 针对已知格式优化的解析器特别适合高频交易系统 分支预测优化：\n使用 LIKELY 和 UNLIKELY 宏帮助 CPU 分支预测 在现代 CPU 上，这可以减少流水线停顿，进一步提高性能 关键收获与最佳实践 # 针对高频场景专门优化：通用解析器难以满足高频交易的需求，应当为关键路径开发专用解析器。\n避免使用 std::stod：在性能关键代码中，应避免使用 std::stod 并考虑以下替代方案：\n对于简单格式，使用自定义的快速解析 使用 std::from_chars，它是更现代的高性能替代品 利用 SIMD 指令集：现代 CPU 的 SIMD 指令集可以显著加速字符串和内存操作。\n避免异常处理：在性能关键路径上使用错误码而非异常处理。\n减少临时对象：每个临时 std::string 都会带来内存分配开销，应当尽可能使用 std::string_view。\n两层解析策略：实现快速路径和回退路径的组合，确保既有性能又有稳定性。\n总之，这些优化使 Binance 聚合交易数据的解析速度提高了一个数量级，对于高频交易系统的延迟和吞吐量都有显著改善。这些技术同样适用于其他需要高性能 JSON 处理的场景。\n","date":"30 April 2025","permalink":"/blog/2025-06-24-perf_tool_usage_guide/","section":"Blog","summary":"原始解析方案的性能瓶颈 #原始的 Binance 聚合交易数据解析实现存在多个性能瓶颈，这在高频交易系统中尤为关键。主要问题包括：\n使用 std::stod 进行字符串到浮点数转换：\nresult.data.price = std::stod(std::string(price_str)); result.data.quantity = std::stod(std::string(qty_str)); 这里存在两个严重问题：\nstd::stod 在底层实现中需要处理各种格式和本地化，导致计算开销大 每次调用都创建了临时 std::string 对象，增加了内存分配和释放的开销 创建临时的 padded_string 对象：\nsimdjson::padded_string padded_json{json}; simdjson::dom::element doc = parser.parse(padded_json); 这会导致额外的内存分配和复制，特别是在高频率处理消息时变得非常明显。\n使用低效的字符串复制方法：\nstrncpy(result.data.symbol, doc[\u0026#34;s\u0026#34;].get_string().value().data(), sizeof(result.data.symbol) - 1); 标准的 strncpy 没有利用现代 CPU 的 SIMD 指令集优势。\n异常处理成本：在解析热路径中大量使用 try-catch 结构，这会导致编译器生成额外代码，影响性能。\n重复获取 JSON 节点：多次访问相同的 JSON 节点，每次都需要进行字符串哈希查找。\n优化方案 #为了解决上述问题，我们实施了多层次的优化策略：\n1. 自定义快速解析路径 #创建了一个专门针对 Binance 聚合交易数据格式的快速解析函数，完全跳过通用 JSON 解析器：\nbool fastParseAggTrade(const std::string_view\u0026amp; json, Common::QuoteData::AggTradeData\u0026amp; data) noexcept { // 快速检查消息类型 const char* type_pattern = \u0026#34;\\\u0026#34;e\\\u0026#34;:\\\u0026#34;aggTrade\\\u0026#34;\u0026#34;; if (json.","title":"行情数据解析优化最佳实践"},{"content":"","date":null,"permalink":"/tags/http/","section":"Tags","summary":"","title":"Http"},{"content":" 1. 会话恢复简介 #什么是会话恢复？ #TLS会话恢复是TLS协议的一项优化特性，允许客户端和服务器基于之前建立的安全会话快速恢复通信，跳过完整的握手过程。在TLS 1.3中，会话恢复主要通过**PSK（Pre-Shared Key，预共享密钥）**机制实现，而在TLS 1.2及更早版本中，也可以通过Session ID或Session Ticket实现。\n为什么需要会话恢复？ # 性能优化： 完整握手（TLS 1.3）：1-RTT 会话恢复：1-RTT（或0-RTT） 显著减少连接建立时间 资源节省： 降低CPU开销（避免重复密钥交换） 减少网络带宽占用 2. TLS 1.3中的会话恢复机制 #工作流程对比 #完整握手（TLS 1.3） #Client Server | ClientHello | |--------------------\u0026gt;| | ServerHello | | EncryptedExt | | Certificate | | CertVerify | | Finished | |\u0026lt;--------------------| | Finished | |--------------------\u0026gt;| | NewSessionTicket | |\u0026lt;--------------------| RTT：1次往返 服务器在握手后发送NewSessionTicket，包含PSK和有效期信息。 会话恢复（1-RTT） #Client Server | ClientHello | | (with PSK) | |--------------------\u0026gt;| | ServerHello | | Finished | |\u0026lt;--------------------| | Finished | |--------------------\u0026gt;| RTT：1次往返 客户端使用之前保存的PSK直接恢复会话。 0-RTT（可选） #Client Server | ClientHello | | (with PSK + Early Data) | |--------------------\u0026gt;| | ServerHello | | Finished | |\u0026lt;--------------------| | Finished | |--------------------\u0026gt;| RTT：0次往返（早期数据随首次请求发送） 注意：0-RTT有重放攻击风险，仅适用于幂等请求。 3. 实现示例（基于picotls） #数据结构 #typedef struct { ptls_iovec_t session_ticket; // 会话ticket（包含PSK） ptls_save_ticket_t ticket_cb; // ticket保存回调 int is_resumption; // 是否恢复会话 time_t ticket_received_time; // ticket接收时间 } tls_context_t; 保存会话Ticket #static int save_ticket_cb(ptls_save_ticket_t* self, ptls_t* tls, ptls_iovec_t ticket) { tls_context_t* ctx = container_of(self, tls_context_t, ticket_cb); // 释放旧ticket if (ctx-\u0026gt;session_ticket.base) { free(ctx-\u0026gt;session_ticket.base); } // 保存新ticket ctx-\u0026gt;session_ticket.base = malloc(ticket.len); if (!ctx-\u0026gt;session_ticket.base) return -1; // 内存分配失败 memcpy(ctx-\u0026gt;session_ticket.base, ticket.base, ticket.len); ctx-\u0026gt;session_ticket.len = ticket.len; ctx-\u0026gt;ticket_received_time = time(NULL); return 0; } 尝试会话恢复 #ptls_handshake_properties_t props = {0}; if (conn-\u0026gt;tls_ctx.session_ticket.base) { // 检查ticket是否过期（假设有效期24小时） if (time(NULL) - conn-\u0026gt;tls_ctx.ticket_received_time \u0026lt; 24 * 3600) { props.client.session_ticket = conn-\u0026gt;tls_ctx.session_ticket; props.client.max_early_data_size = 16384; // 支持0-RTT conn-\u0026gt;session_info.resumption_attempted = 1; } } ptls_handshake(conn-\u0026gt;tls, \u0026amp;props); 验证恢复结果 #if (conn-\u0026gt;session_info.resumption_attempted) { conn-\u0026gt;session_info.resumption_succeeded = ptls_is_psk_handshake(conn-\u0026gt;tls); if (!conn-\u0026gt;session_info.resumption_succeeded) { // 恢复失败，清理ticket free(conn-\u0026gt;tls_ctx.session_ticket.base); conn-\u0026gt;tls_ctx.session_ticket.base = NULL; conn-\u0026gt;tls_ctx.session_ticket.len = 0; } } 4. 在连接池中的应用 #场景 # 复用连接：检查ticket有效性，优先尝试恢复，失败则完整握手。 新建连接：使用已有ticket尝试恢复，保存新ticket。 连接维护：跟踪ticket有效期，清理过期ticket，统计恢复率。 示例逻辑 #if (pool-\u0026gt;ticket.base \u0026amp;\u0026amp; time(NULL) - pool-\u0026gt;ticket_time \u0026lt; pool-\u0026gt;ticket_lifetime) { // 尝试恢复 props.client.session_ticket = pool-\u0026gt;ticket; if (ptls_handshake(conn-\u0026gt;tls, \u0026amp;props) == 0 \u0026amp;\u0026amp; ptls_is_psk_handshake(conn-\u0026gt;tls)) { pool-\u0026gt;stats.resumption_success++; } else { pool-\u0026gt;stats.resumption_fail++; ptls_handshake(conn-\u0026gt;tls, NULL); // 回退完整握手 } } else { // 完整握手并保存新ticket ptls_handshake(conn-\u0026gt;tls, NULL); } 5. 注意事项 #安全性 # 有效期限制：ticket通常有效数小时，由服务器指定。 存储安全：避免明文保存ticket，建议加密存储。 0-RTT风险：防范重放攻击，仅用于安全场景。 性能优化 # ticket管理：避免过度保存，定期清理无效ticket。 服务器负载：减少频繁发送NewSessionTicket。 监控指标：记录恢复成功率，优化策略。 错误处理 # 优雅降级：恢复失败时切换完整握手。 日志记录：保存失败原因（如ticket过期或拒绝）。 6. 总结 #TLS会话恢复通过PSK机制显著提升连接效率，尤其在高并发场景（如连接池）中效果明显。正确实现需要平衡安全性与性能，关注ticket管理、错误处理和监控。通过1-RTT或0-RTT，客户端和服务器可在毫秒内恢复安全通信，是现代网络优化的关键技术。\n改进亮点 # TLS 1.3准确性：流程图和术语基于TLS 1.3标准。 0-RTT补充：增加了0-RTT的说明和风险提示。 代码健壮性：加入内存分配检查和ticket过期逻辑。 结构优化：分为简介、机制、实现、应用和注意事项，逻辑更清晰。 如果需要进一步调整（如更深入的代码细节或特定场景分析），请告诉我！\n","date":"7 March 2025","permalink":"/blog/2025-06-24-session_resumption_techniques/","section":"Blog","summary":"1. 会话恢复简介 #什么是会话恢复？ #TLS会话恢复是TLS协议的一项优化特性，允许客户端和服务器基于之前建立的安全会话快速恢复通信，跳过完整的握手过程。在TLS 1.3中，会话恢复主要通过**PSK（Pre-Shared Key，预共享密钥）**机制实现，而在TLS 1.2及更早版本中，也可以通过Session ID或Session Ticket实现。\n为什么需要会话恢复？ # 性能优化： 完整握手（TLS 1.3）：1-RTT 会话恢复：1-RTT（或0-RTT） 显著减少连接建立时间 资源节省： 降低CPU开销（避免重复密钥交换） 减少网络带宽占用 2. TLS 1.3中的会话恢复机制 #工作流程对比 #完整握手（TLS 1.3） #Client Server | ClientHello | |--------------------\u0026gt;| | ServerHello | | EncryptedExt | | Certificate | | CertVerify | | Finished | |\u0026lt;--------------------| | Finished | |--------------------\u0026gt;| | NewSessionTicket | |\u0026lt;--------------------| RTT：1次往返 服务器在握手后发送NewSessionTicket，包含PSK和有效期信息。 会话恢复（1-RTT） #Client Server | ClientHello | | (with PSK) | |--------------------\u0026gt;| | ServerHello | | Finished | |\u0026lt;--------------------| | Finished | |--------------------\u0026gt;| RTT：1次往返 客户端使用之前保存的PSK直接恢复会话。 0-RTT（可选） #Client Server | ClientHello | | (with PSK + Early Data) | |--------------------\u0026gt;| | ServerHello | | Finished | |\u0026lt;--------------------| | Finished | |--------------------\u0026gt;| RTT：0次往返（早期数据随首次请求发送） 注意：0-RTT有重放攻击风险，仅适用于幂等请求。 3.","title":"TLS会话恢复（Session Resumption）"},{"content":"我来帮你编写一份详细的技术教程，介绍如何使用 Tailscale 在 MacOS 设备间实现远程连接。\n使用 Tailscale 实现 MacOS 设备远程连接教程 #准备工作 # 确保两台 MacOS 设备都能正常访问互联网 准备一个 Tailscale 账号（可以使用 Google、GitHub 等账号登录） 详细步骤 #第一步：安装 Tailscale #在两台 MacOS 设备上分别安装 Tailscale：\n访问 Tailscale 官网 (https://tailscale.com/download) 下载 MacOS 版本的安装包 打开下载的 .dmg 文件，将 Tailscale 拖入应用程序文件夹 第二步：登录和配置 # 在两台设备上启动 Tailscale 点击菜单栏的 Tailscale 图标 使用相同的账号登录 登录成功后，Tailscale 会自动为设备分配 IP 地址 点击菜单栏图标可以查看分配的 IP 地址（通常格式为 100.xx.xx.xx） 第三步：开启远程访问 #在被控制的 MacOS 设备上：\n打开系统偏好设置 选择\u0026quot;共享\u0026quot; 勾选\u0026quot;远程管理\u0026quot;或\u0026quot;屏幕共享\u0026quot; 配置访问权限，可以选择： 允许所有用户 仅允许特定用户 第四步：建立连接 #在控制端 MacOS 设备上：\n打开访达（Finder） 在菜单栏选择\u0026quot;前往\u0026quot; → \u0026ldquo;连接服务器\u0026rdquo;（或按下 Command + K） 在服务器地址栏输入：vnc://100.xx.xx.xx（使用目标设备的 Tailscale IP） 点击连接 输入目标设备的用户名和密码(平常锁屏时的解锁密码,可以在设置-用户群组-用户 处查看用户名) 注意事项 # 确保两台设备都保持 Tailscale 在线状态 建议在首次连接时进行测试，确保连接正常 如遇连接问题，检查： Tailscale 状态是否在线 目标设备的屏幕共享是否开启 IP 地址是否输入正确 防火墙设置是否允许连接 安全建议 # 定期更新 Tailscale 客户端 使用强密码保护你的 Tailscale 账号 在不需要远程连接时，建议关闭屏幕共享功能 定期检查已授权设备列表，移除不需要的设备 这样设置后，你就可以通过 Tailscale 安全地远程连接和控制其他 MacOS 设备了。Tailscale 使用了安全的加密通信，让远程连接更加安全可靠。\n","date":"19 January 2025","permalink":"/blog/2025-06-24-screen_sharing_techniques/","section":"Blog","summary":"我来帮你编写一份详细的技术教程，介绍如何使用 Tailscale 在 MacOS 设备间实现远程连接。\n使用 Tailscale 实现 MacOS 设备远程连接教程 #准备工作 # 确保两台 MacOS 设备都能正常访问互联网 准备一个 Tailscale 账号（可以使用 Google、GitHub 等账号登录） 详细步骤 #第一步：安装 Tailscale #在两台 MacOS 设备上分别安装 Tailscale：\n访问 Tailscale 官网 (https://tailscale.com/download) 下载 MacOS 版本的安装包 打开下载的 .dmg 文件，将 Tailscale 拖入应用程序文件夹 第二步：登录和配置 # 在两台设备上启动 Tailscale 点击菜单栏的 Tailscale 图标 使用相同的账号登录 登录成功后，Tailscale 会自动为设备分配 IP 地址 点击菜单栏图标可以查看分配的 IP 地址（通常格式为 100.xx.xx.xx） 第三步：开启远程访问 #在被控制的 MacOS 设备上：\n打开系统偏好设置 选择\u0026quot;共享\u0026quot; 勾选\u0026quot;远程管理\u0026quot;或\u0026quot;屏幕共享\u0026quot; 配置访问权限，可以选择： 允许所有用户 仅允许特定用户 第四步：建立连接 #在控制端 MacOS 设备上：\n打开访达（Finder） 在菜单栏选择\u0026quot;前往\u0026quot; → \u0026ldquo;连接服务器\u0026rdquo;（或按下 Command + K） 在服务器地址栏输入：vnc://100.","title":"使用 Tailscale 实现 MacOS 设备远程连接教程"},{"content":"问题现象 #在多进程共享内存通信中，发现读取进程出现异常：\n写入进程（线程3002707）正常写入数据\n读取进程（线程3002791）卡在固定位置：\npage: 0 write_pos: 134209160 read_pos: 134199368 问题定位过程 #1. 初步分析 #首先观察到一个关键现象：\nBinance的读写正常 Bitget的读取卡在固定位置 两个交易所使用相同的共享内存机制 2. 代码分析 #检查共享内存管理的核心类：\n写入机制： template\u0026lt;typename T\u0026gt; bool write(const TypedFrame\u0026lt;T\u0026gt;\u0026amp; frame) { // ... if (write_pos + frame_size \u0026gt; page_size_) { switchToNextPage(); write_pos = current_write_pos_.load(std::memory_order_relaxed); continue; } // ... std::atomic\u0026lt;size_t\u0026gt;* shared_write_pos = reinterpret_cast\u0026lt;std::atomic\u0026lt;size_t\u0026gt;*\u0026gt;(current_page_-\u0026gt;getData()); shared_write_pos-\u0026gt;store(write_pos + frame_size, std::memory_order_release); } 页面切换： void Journal::switchToNextPage() { current_page_ = page_engine_-\u0026gt;getNextPage(); current_write_pos_.store(0, std::memory_order_relaxed); } Page* PageEngine::getNextPage() { current_page_index_++; if (current_page_index_ \u0026gt;= pages_.size()) { addNewPage(); } return pages_[current_page_index_].get(); } 3. 关键发现 #通过分析发现：\n写入位置（write_pos）正确存储在共享内存中 但页面索引（current_page_index_）是进程内变量 导致读取进程无法感知页面切换 根本原因 # 进程隔离： 每个进程有自己的PageEngine实例 current_page_index_是进程内存变量 写进程切换页面时，读进程无法感知 共享机制不完整： 只共享了写入位置（write_pos） 未共享页面切换信息 解决方案 # 设计思路\n设计共享控制结构管理页面状态 在共享内存中维护完整的页面信息 实现页面切换的进程间同步 具体实现 首先，设计共享控制结构：\nstruct alignas(64) SharedPageControl { std::atomic\u0026lt;uint64_t\u0026gt; current_page_index; // 当前页索引 std::atomic\u0026lt;uint64_t\u0026gt; total_pages; // 总页数 std::atomic\u0026lt;uint64_t\u0026gt; write_pos; // 写入位置 char padding[40]; // 保持缓存行对齐 }; 更新页面切换逻辑：\nvoid Journal::switchToNextPage() { if (!is_writer_) return; auto* control = current_page_-\u0026gt;getSharedControl(); size_t new_page_index = control-\u0026gt;current_page_index.load(std::memory_order_relaxed) + 1; // 更新共享控制信息 control-\u0026gt;current_page_index.store(new_page_index, std::memory_order_release); control-\u0026gt;write_pos.store(sizeof(SharedPageControl), std::memory_order_release); // 更新本地状态 current_page_ = page_engine_-\u0026gt;getNextPage(); current_write_pos_.store(sizeof(SharedPageControl), std::memory_order_relaxed); } 3. 优化考虑 # 性能优化： 使用64字节对齐避免false sharing 最小化原子操作次数 保持无锁设计 可靠性保证： 使用原子操作确保线程安全 正确的内存序保证可见性 完整的错误处理 方案实现要点 # 共享控制信息管理\n在共享内存的起始位置放置控制结构 使用原子操作保证更新的可见性 通过内存对齐优化性能 页面切换同步\n写进程负责更新页面状态 读进程通过共享控制信息感知页面切换 保证页面信息的一致性 内存布局优化\n控制信息和数据区域分离 使用缓存行对齐避免false sharing 保持高效的内存访问 技术经验总结 # 多进程通信设计原则\n控制信息必须在进程间共享 使用原子操作保证可见性 注意内存布局和性能优化 问题诊断方法\n观察异常现象的共性和差异 对比正常和异常案例 追踪到根本原因 代码实现建议\n重视共享状态的同步 合理使用内存对齐 注意性能和正确性的平衡 最佳实践要点\n设计时考虑多进程场景 充分测试边界条件 保持代码的可维护性 这个案例展示了在高频交易系统中一个典型的多进程通信问题的完整解决过程。它强调了在共享内存通信中正确管理共享状态的重要性，以及如何通过系统的分析和设计来解决复杂的并发问题。这些经验对于构建稳定、高效的多进程系统具有重要的参考价值。\n","date":"17 January 2025","permalink":"/blog/2025-06-24-fix_shared_page_position/","section":"Blog","summary":"问题现象 #在多进程共享内存通信中，发现读取进程出现异常：\n写入进程（线程3002707）正常写入数据\n读取进程（线程3002791）卡在固定位置：\npage: 0 write_pos: 134209160 read_pos: 134199368 问题定位过程 #1. 初步分析 #首先观察到一个关键现象：\nBinance的读写正常 Bitget的读取卡在固定位置 两个交易所使用相同的共享内存机制 2. 代码分析 #检查共享内存管理的核心类：\n写入机制： template\u0026lt;typename T\u0026gt; bool write(const TypedFrame\u0026lt;T\u0026gt;\u0026amp; frame) { // ... if (write_pos + frame_size \u0026gt; page_size_) { switchToNextPage(); write_pos = current_write_pos_.load(std::memory_order_relaxed); continue; } // ... std::atomic\u0026lt;size_t\u0026gt;* shared_write_pos = reinterpret_cast\u0026lt;std::atomic\u0026lt;size_t\u0026gt;*\u0026gt;(current_page_-\u0026gt;getData()); shared_write_pos-\u0026gt;store(write_pos + frame_size, std::memory_order_release); } 页面切换： void Journal::switchToNextPage() { current_page_ = page_engine_-\u0026gt;getNextPage(); current_write_pos_.store(0, std::memory_order_relaxed); } Page* PageEngine::getNextPage() { current_page_index_++; if (current_page_index_ \u0026gt;= pages_.","title":"共享内存多进程通信中的页面切换同步问题分析与解决"},{"content":"Solana链上交易监控最佳实践：从logsSubscribe到全方位监控 #背景介绍 #在Solana链上开发中，实时监控特定账户的交易活动是一个常见需求，特别是在构建跟单机器人这类对时效性要求较高的应用场景中。最初，我们可能会想到使用Solana提供的logsSubscribe WebSocket API来实现这个功能，因为它看起来是最直接的解决方案。然而，在实际应用中，我们发现这种方案存在一些限制和问题。\n问题发现 #在使用logsSubscribe进行账户监控时，我们发现一个关键问题：某些确实发生的交易并没有被我们的监控系统捕获到。这个问题的发现促使我们深入研究Solana的交易日志机制，并最终设计了一个更全面的监控方案。\n为什么会遗漏交易？ # 日志记录机制的局限性\n程序可能不会在日志中明确记录所有涉及的账户地址 交易可能使用了PDA(Program Derived Address)或其他派生地址 某些DEX采用内部账户映射，而不是直接记录用户地址 mentions过滤器的限制\n只能捕获在日志中明确提到目标地址的交易 无法捕获通过间接方式影响目标账户的交易 解决方案 #针对上述问题，我们设计了一个多维度监控方案，通过组合多种订阅方式来确保不会遗漏任何相关交易。\n1. 三重订阅机制 #pub struct EnhancedTradeWatcher { target_account: Pubkey, ws_client: WebSocketClient, } impl EnhancedTradeWatcher { async fn setup_comprehensive_monitoring(\u0026amp;mut self) -\u0026gt; Result\u0026lt;()\u0026gt; { // 1. logsSubscribe - 捕获显式提及 let logs_sub = json!({ \u0026#34;jsonrpc\u0026#34;: \u0026#34;2.0\u0026#34;, \u0026#34;method\u0026#34;: \u0026#34;logsSubscribe\u0026#34;, \u0026#34;params\u0026#34;: [ { \u0026#34;mentions\u0026#34;: [self.target_account.to_string()], }, { \u0026#34;commitment\u0026#34;: \u0026#34;processed\u0026#34; } ] }); // 2. programSubscribe - 监控DEX程序 let dex_program_sub = json!({ \u0026#34;jsonrpc\u0026#34;: \u0026#34;2.0\u0026#34;, \u0026#34;method\u0026#34;: \u0026#34;programSubscribe\u0026#34;, \u0026#34;params\u0026#34;: [ DEX_PROGRAM_ID, { \u0026#34;encoding\u0026#34;: \u0026#34;jsonParsed\u0026#34;, \u0026#34;commitment\u0026#34;: \u0026#34;processed\u0026#34; } ] }); // 3. accountSubscribe - 监控账户变更 let account_sub = json!({ \u0026#34;jsonrpc\u0026#34;: \u0026#34;2.0\u0026#34;, \u0026#34;method\u0026#34;: \u0026#34;accountSubscribe\u0026#34;, \u0026#34;params\u0026#34;: [ self.target_account.to_string(), { \u0026#34;encoding\u0026#34;: \u0026#34;jsonParsed\u0026#34;, \u0026#34;commitment\u0026#34;: \u0026#34;processed\u0026#34; } ] }); // 发送所有订阅请求 self.ws_client.send(logs_sub).await?; self.ws_client.send(dex_program_sub).await?; self.ws_client.send(account_sub).await?; } } 2. 交易去重机制 #为了避免多个订阅渠道导致的重复处理，我们实现了基于交易签名的去重机制：\nasync fn handle_all_events(\u0026amp;mut self) -\u0026gt; Result\u0026lt;()\u0026gt; { let mut transactions_seen = HashSet::new(); while let Some(event) = self.ws_client.next_message().await? { if !transactions_seen.contains(\u0026amp;event.signature) { self.process_transaction(\u0026amp;event).await?; transactions_seen.insert(event.signature); } } Ok(()) } 3. 相关账户检查 #为了确保捕获所有相关交易，我们实现了全面的账户关联检查：\nfn check_related_accounts(\u0026amp;self, program_info: \u0026amp;ProgramInfo) -\u0026gt; bool { // 检查Token账户 let token_accounts = self.get_associated_token_accounts(\u0026amp;self.target_account); // 检查OpenOrders账户 let open_orders = self.get_open_orders_accounts(\u0026amp;self.target_account); // 检查PDA let pdas = self.get_related_pdas(\u0026amp;self.target_account); program_info.accounts.iter().any(|acc| token_accounts.contains(acc) || open_orders.contains(acc) || pdas.contains(acc) ) } 为什么选择这种方案？ # 完整性保证\n多维度监控确保不会遗漏任何相关交易 通过检查关联账户捕获间接交易 性能优化\n使用缓存减少RPC调用 实现交易去重避免重复处理 采用processed提交级别获得最低延迟 可扩展性\n方案设计支持添加新的DEX监控 可以根据具体需求调整监控策略 可靠性\n多渠道数据源提供数据冗余 降低单点故障风险 性能考虑 #虽然这种多维度监控方案会带来一些额外的系统开销，但在跟单场景中，准确性和完整性的重要性远大于少量的性能损耗。为了优化性能，我们实现了以下机制：\nstruct AccountCache { token_accounts: LruCache\u0026lt;Pubkey, Vec\u0026lt;Pubkey\u0026gt;\u0026gt;, open_orders: LruCache\u0026lt;Pubkey, Vec\u0026lt;Pubkey\u0026gt;\u0026gt;, last_update: HashMap\u0026lt;Pubkey, Instant\u0026gt;, } 结论 #在Solana链上开发中，单一的监控方式往往无法满足复杂业务场景的需求。通过结合多种订阅方式，并配合合理的缓存策略和去重机制，我们可以构建一个既可靠又高效的交易监控系统。这个方案虽然实现较为复杂，但能够提供更好的可靠性和完整性保证，特别适合对实时性和准确性要求较高的跟单场景。\n未来展望 # 支持更多DEX协议 优化缓存策略 添加更多性能监控指标 实现自动化失败重试机制 希望这篇文章能够帮助大家在实现Solana链上监控时避免一些常见陷阱，构建更可靠的监控系统。\n","date":"20 December 2024","permalink":"/blog/2025-06-24-solana_monitoring_system/","section":"Blog","summary":"Solana链上交易监控最佳实践：从logsSubscribe到全方位监控 #背景介绍 #在Solana链上开发中，实时监控特定账户的交易活动是一个常见需求，特别是在构建跟单机器人这类对时效性要求较高的应用场景中。最初，我们可能会想到使用Solana提供的logsSubscribe WebSocket API来实现这个功能，因为它看起来是最直接的解决方案。然而，在实际应用中，我们发现这种方案存在一些限制和问题。\n问题发现 #在使用logsSubscribe进行账户监控时，我们发现一个关键问题：某些确实发生的交易并没有被我们的监控系统捕获到。这个问题的发现促使我们深入研究Solana的交易日志机制，并最终设计了一个更全面的监控方案。\n为什么会遗漏交易？ # 日志记录机制的局限性\n程序可能不会在日志中明确记录所有涉及的账户地址 交易可能使用了PDA(Program Derived Address)或其他派生地址 某些DEX采用内部账户映射，而不是直接记录用户地址 mentions过滤器的限制\n只能捕获在日志中明确提到目标地址的交易 无法捕获通过间接方式影响目标账户的交易 解决方案 #针对上述问题，我们设计了一个多维度监控方案，通过组合多种订阅方式来确保不会遗漏任何相关交易。\n1. 三重订阅机制 #pub struct EnhancedTradeWatcher { target_account: Pubkey, ws_client: WebSocketClient, } impl EnhancedTradeWatcher { async fn setup_comprehensive_monitoring(\u0026amp;mut self) -\u0026gt; Result\u0026lt;()\u0026gt; { // 1. logsSubscribe - 捕获显式提及 let logs_sub = json!({ \u0026#34;jsonrpc\u0026#34;: \u0026#34;2.0\u0026#34;, \u0026#34;method\u0026#34;: \u0026#34;logsSubscribe\u0026#34;, \u0026#34;params\u0026#34;: [ { \u0026#34;mentions\u0026#34;: [self.target_account.to_string()], }, { \u0026#34;commitment\u0026#34;: \u0026#34;processed\u0026#34; } ] }); // 2. programSubscribe - 监控DEX程序 let dex_program_sub = json!","title":"Solana_monitor"},{"content":"Solana链上交易监控技术分析 #1. Solana DEX 交易形式 #1.1 直接 DEX 交易 #用户直接与 DEX 合约交互，交易流程简单直接。\n用户钱包 -\u0026gt; DEX程序 (如Raydium/Orca) -\u0026gt; Token Program 特点：\n交易日志简洁，主要包含单个 DEX 程序的调用 容易识别交易平台和交易对 Token Program 的 transfer 指令较少 1.2 聚合器交易（Jupiter） #通过聚合器路由到单个或多个 DEX。\n用户钱包 -\u0026gt; Jupiter -\u0026gt; DEX1/DEX2/... -\u0026gt; Token Program 特点：\n包含 Jupiter 合约调用 可能涉及多个 DEX 交易日志较长，包含多个内部指令 可能有复杂的代币交换路径 1.3 智能路由交易 #一笔交易通过多个 DEX 串联完成。\n用户钱包 -\u0026gt; 聚合器 -\u0026gt; DEX1 -\u0026gt; DEX2 -\u0026gt; DEX3 -\u0026gt; Token Program 特点：\n交易路径最复杂 涉及多次代币交换 目的是获得最优价格 包含多个 Token Program 的 transfer 指令 2. Solana 链上监控原理 #2.1 为什么可以监控目标账户 #Solana 链上监控的实现基于以下几个关键特性：\n账户模型 Solana 使用账户模型而不是 UTXO 模型： - 每个账户都有唯一的地址 - 所有交易都会涉及账户的状态变更 - 交易日志会记录所有涉及的账户地址 交易日志系统 // 交易日志包含： - 所有程序调用 (Program invoke) - 账户操作记录 (Instruction logs) - 代币转移详情 (Token transfers) - 错误信息 (如果有) RPC 订阅机制 // logsSubscribe 支持多种过滤方式： - mentions: 日志中提到的账户地址 - dataSlice: 选择性获取数据片段 - commitment: 确认级别设置 2.2 监控原理详解 # 账户日志触发机制 在 Solana 中，以下操作会导致账户出现在交易日志中： - 作为交易签名者 - 作为指令中的账户参数 - 发生代币转入/转出 - 账户数据被修改 代币账户变更追踪 // 代币账户变更会记录在 meta 数据中 pub struct TransactionMeta { pub pre_token_balances: Vec\u0026lt;TokenBalance\u0026gt;, // 交易前余额 pub post_token_balances: Vec\u0026lt;TokenBalance\u0026gt;, // 交易后余额 pub log_messages: Vec\u0026lt;String\u0026gt;, // 详细日志 // ... } DEX 交易跟踪 一个 DEX 交易通常涉及： 1. DEX 程序调用 2. Token Program 转账操作 3. 池子账户状态更新 4. 用户代币账户余额变化 2.3 技术实现关键点 # WebSocket 订阅配置详解 // 订阅特定账户的所有相关日志 let subscribe_config = json!({ \u0026#34;jsonrpc\u0026#34;: \u0026#34;2.0\u0026#34;, \u0026#34;id\u0026#34;: 1, \u0026#34;method\u0026#34;: \u0026#34;logsSubscribe\u0026#34;, \u0026#34;params\u0026#34;: [ { \u0026#34;mentions\u0026#34;: [target_address.to_string()], // 目标账户 \u0026#34;commitment\u0026#34;: \u0026#34;confirmed\u0026#34;, // 确认级别 \u0026#34;filters\u0026#34;: [ // 可选过滤器 {\u0026#34;dataSize\u0026#34;: 0}, // 数据大小过滤 {\u0026#34;memcmp\u0026#34;: { // 内存比较过滤 \u0026#34;offset\u0026#34;: 0, \u0026#34;bytes\u0026#34;: \u0026#34;base58_encoded_bytes\u0026#34; }} ] } ] }); 交易数据结构解析 pub struct ParsedTransaction { pub signature: String, pub slot: u64, pub meta: TransactionStatusMeta, pub transaction: EncodedTransaction, } impl TransactionWatcher { fn parse_transaction_data(\u0026amp;self, tx: \u0026amp;ParsedTransaction) -\u0026gt; Option\u0026lt;TradeInfo\u0026gt; { // 1. 检查交易是否涉及目标账户 let involves_target = tx.meta.log_messages.iter() .any(|log| log.contains(\u0026amp;self.target_address.to_string())); if !involves_target { return None; } // 2. 解析代币余额变化 let balance_changes = self.parse_token_balances( \u0026amp;tx.meta.pre_token_balances, \u0026amp;tx.meta.post_token_balances ); // 3. 识别交易类型和方向 let dex_type = self.identify_dex(\u0026amp;tx.meta.log_messages); // 4. 构建交易信息 Some(TradeInfo { // ... 交易详情 }) } } 余额变化分析示例 fn analyze_balance_changes( \u0026amp;self, pre_balances: \u0026amp;[TokenBalance], post_balances: \u0026amp;[TokenBalance], ) -\u0026gt; Vec\u0026lt;(String, f64)\u0026gt; { pre_balances.iter() .filter(|pre| pre.owner == self.target_address.to_string()) .filter_map(|pre| { post_balances.iter() .find(|post| post.mint == pre.mint) .map(|post| { let change = post.ui_token_amount.ui_amount.unwrap_or(0.0) - pre.ui_token_amount.ui_amount.unwrap_or(0.0); (pre.mint.clone(), change) }) }) .collect() } 2.4 监控可靠性保障 # 数据完整性验证 impl TransactionWatcher { fn validate_transaction_data(\u0026amp;self, tx: \u0026amp;ParsedTransaction) -\u0026gt; bool { // 1. 验证交易状态 if tx.meta.err.is_some() { return false; } // 2. 验证代币余额数据完整性 if tx.meta.pre_token_balances.is_empty() || tx.meta.post_token_balances.is_empty() { return false; } // 3. 验证日志完整性 if tx.meta.log_messages.is_empty() { return false; } true } } 错误恢复机制 async fn reconnect_websocket(\u0026amp;mut self) -\u0026gt; Result\u0026lt;()\u0026gt; { let mut retry_count = 0; let max_retries = 3; while retry_count \u0026lt; max_retries { match connect_async(\u0026amp;self.ws_url).await { Ok((ws_stream, _)) =\u0026gt; { self.ws_client = ws_stream; return Ok(()); } Err(e) =\u0026gt; { retry_count += 1; error!(\u0026#34;WebSocket重连失败: {}, 重试 {}/{}\u0026#34;, e, retry_count, max_retries); tokio::time::sleep(Duration::from_secs(2_u64.pow(retry_count))).await; } } } Err(anyhow!(\u0026#34;WebSocket重连失败\u0026#34;)) } 3. 技术要点 #3.1 实时性保障 # 使用 WebSocket 订阅而不是轮询 confirmed commitment 级别的确认 错误重试机制 3.2 数据准确性 # 解析交易前后的余额变化 考虑代币精度 验证交易状态 3.3 性能优化 # 缓存常用池子信息 批量处理交易 异步处理框架 3.4 可靠性保障 # 多个 RPC 节点故障转移 WebSocket 断线重连 交易确认重试 4. 最佳实践 # RPC 节点选择\n使用可靠的私有节点 准备多个备用节点 定期检查节点健康状态 监控配置\n合理设置 commitment 级别 配置适当的重试参数 根据需求调整缓存策略 错误处理\n完善的日志记录 优雅的错误恢复 监控告警机制 5. 潜在问题 # RPC 节点不稳定\n解决：实现节点故障转移 定期健康检查 使用私有节点 交易解析失败\n原因：日志格式变化 解决：版本适配 完善错误处理 性能瓶颈\nWebSocket 连接管理 交易处理队列 缓存优化 ","date":"19 December 2024","permalink":"/blog/2025-06-24-solana_blockchain_analysis/","section":"Blog","summary":"Solana链上交易监控技术分析 #1. Solana DEX 交易形式 #1.1 直接 DEX 交易 #用户直接与 DEX 合约交互，交易流程简单直接。\n用户钱包 -\u0026gt; DEX程序 (如Raydium/Orca) -\u0026gt; Token Program 特点：\n交易日志简洁，主要包含单个 DEX 程序的调用 容易识别交易平台和交易对 Token Program 的 transfer 指令较少 1.2 聚合器交易（Jupiter） #通过聚合器路由到单个或多个 DEX。\n用户钱包 -\u0026gt; Jupiter -\u0026gt; DEX1/DEX2/... -\u0026gt; Token Program 特点：\n包含 Jupiter 合约调用 可能涉及多个 DEX 交易日志较长，包含多个内部指令 可能有复杂的代币交换路径 1.3 智能路由交易 #一笔交易通过多个 DEX 串联完成。\n用户钱包 -\u0026gt; 聚合器 -\u0026gt; DEX1 -\u0026gt; DEX2 -\u0026gt; DEX3 -\u0026gt; Token Program 特点：\n交易路径最复杂 涉及多次代币交换 目的是获得最优价格 包含多个 Token Program 的 transfer 指令 2.","title":"Solana链上交易监控技术分析"},{"content":"一、故障现象 #1.1 单endpoint模式故障 # 单个WebSocket连接时消息接收完全阻塞 日志显示消息处理线程启动后无法接收新消息 [2024-12-12 20:31:29.455] [error] [setThreadAffinity] Error calling pthread_setaffinity_np: 22 [2024-12-12 20:31:29.697] [info] Message thread started for endpoint: OkxPublic // 之后无消息接收日志 1.2 多endpoint模式部分正常 # 多个WebSocket连接时只有一个线程能正常接收消息 日志显示消息处理情况： [20:54:50.542] [thread 91374] Processing message for OkxPublic [20:54:50.640] [thread 91374] Processing message for OkxPublic // 只有一个线程在持续处理消息 二、系统架构分析 #2.1 WebSocket消息接收机制 #void WebSocketClient::receiveMessages(const MessageHandler\u0026amp; handler) { while (true) { try { // 1. 阻塞式接收WebSocket消息 int n = ws_-\u0026gt;receiveFrame(buffer.data(), buffer.size(), flags); // 2. 同步回调处理消息 if (n \u0026gt; 0) { handler(buffer.data(), n); } } catch (const std::exception\u0026amp; e) { break; } } } 2.2 关键技术点 # 阻塞式WebSocket接收\nreceiveFrame是阻塞调用 直到收到消息才会返回 高频交易场景下需要快速响应 同步消息处理\n接收和处理在同一线程中 处理耗时会直接影响下一条消息的接收 适合高频交易的低延迟要求 CPU亲和性设置\nvoid ConnectionPool::setupRealtime(int cpu_core) { cpu_set_t cpuset; CPU_ZERO(\u0026amp;cpuset); CPU_SET(cpu_core, \u0026amp;cpuset); pthread_setaffinity_np(pthread_self(), sizeof(cpu_set_t), \u0026amp;cpuset); struct sched_param param; param.sched_priority = sched_get_priority_max(SCHED_FIFO); pthread_setschedparam(pthread_self(), SCHED_FIFO, \u0026amp;param); } 三、问题分析 #3.1 单endpoint阻塞原因 # CPU亲和性限制： 线程被强制绑定到特定CPU核心 当该核心被其他任务占用时，无法切换到其他核心 导致消息处理线程无法获得CPU时间 阻塞式接收影响： receiveFrame阻塞等待新消息 CPU亲和性限制导致线程无法及时获得CPU时间 即使有新消息也无法及时处理 3.2 多endpoint场景分析 # 为什么只有一个线程正常： 多个线程竞争CPU资源 获得CPU时间片的线程能正常处理消息 其他线程由于CPU亲和性限制无法切换核心，导致阻塞 日志证据： [20:54:50.542] [thread 91374] OkxPublic message processed [20:54:50.640] [thread 91374] OkxPublic message processed // 只有thread 91374持续工作 四、解决方案 #4.1 移除CPU亲和性限制 #void ConnectionPool::setupMessageHandler(context) { context-\u0026gt;message_thread = std::thread([this, context]() { // 移除CPU亲和性设置 // 让系统自动进行线程调度 while (running_) { context-\u0026gt;client-\u0026gt;receiveMessages(...); } }); } 4.2 优化性能监控 #struct ThreadMetrics { std::atomic\u0026lt;uint64_t\u0026gt; messages_processed{0}; std::atomic\u0026lt;uint64_t\u0026gt; processing_time_us{0}; std::atomic\u0026lt;uint64_t\u0026gt; max_processing_time_us{0}; std::atomic\u0026lt;int\u0026gt; current_cpu{-1}; }; 五、经验总结 # 高频交易系统特点 需要低延迟处理 同步处理模式更适合 系统调度策略需要谨慎 CPU亲和性使用原则 避免不必要的限制 让操作系统进行自然调度 需要时要充分测试验证 故障诊断要点 分析线程行为模式 对比不同场景的表现 理解底层技术机制 性能优化方向 保持消息处理的低延迟 添加性能监控指标 系统调度最优化 ","date":"13 December 2024","permalink":"/blog/2025-06-24-message_queue_overstocking_solutions/","section":"Blog","summary":"一、故障现象 #1.1 单endpoint模式故障 # 单个WebSocket连接时消息接收完全阻塞 日志显示消息处理线程启动后无法接收新消息 [2024-12-12 20:31:29.455] [error] [setThreadAffinity] Error calling pthread_setaffinity_np: 22 [2024-12-12 20:31:29.697] [info] Message thread started for endpoint: OkxPublic // 之后无消息接收日志 1.2 多endpoint模式部分正常 # 多个WebSocket连接时只有一个线程能正常接收消息 日志显示消息处理情况： [20:54:50.542] [thread 91374] Processing message for OkxPublic [20:54:50.640] [thread 91374] Processing message for OkxPublic // 只有一个线程在持续处理消息 二、系统架构分析 #2.1 WebSocket消息接收机制 #void WebSocketClient::receiveMessages(const MessageHandler\u0026amp; handler) { while (true) { try { // 1. 阻塞式接收WebSocket消息 int n = ws_-\u0026gt;receiveFrame(buffer.data(), buffer.size(), flags); // 2. 同步回调处理消息 if (n \u0026gt; 0) { handler(buffer.","title":"WebSocket消息处理线程CPU亲和性导致的消息阻塞故障分析"},{"content":"1. 需求背景 #在高频交易系统中，我们面临一个典型场景：需要同时处理三个关联订单（三角套利）。这些订单必须几乎同时发出以确保套利的有效性。\n关键挑战：\n订单必须同时或几乎同时发出 系统需要处理高并发的订单组 需要保证订单处理的稳定性和可靠性 2. 当前使用的两种处理订单的机制 # 无锁队列机制 订单生成后进入一个无锁队列 多个线程从队列中取订单进行处理 订单的发送通过RestClient进行，RestClient负责管理HTTP连接池并发送请求 分片机制 订单生成后根据某种规则分配到不同的分片 每个分片由固定的线程处理 同一组的订单被分配到同一个分片，确保组内订单的处理一致性 RestClient同样负责订单的发送 class OrderShard { private: struct OrderGroup { uint64_t groupId; uint64_t timestamp; std::vector\u0026lt;Order\u0026gt; orders; }; std::queue\u0026lt;OrderGroup\u0026gt; orderQueue_; std::mutex mutex_; std::condition_variable cv_; RestClient restClient_; public: void addOrderGroup(OrderGroup group) { { std::lock_guard\u0026lt;std::mutex\u0026gt; lock(mutex_); orderQueue_.push(std::move(group)); } cv_.notify_one(); } void processOrders() { while (running_) { OrderGroup group; { std::unique_lock\u0026lt;std::mutex\u0026gt; lock(mutex_); cv_.wait(lock, [this] { return !orderQueue_.empty() || !running_; }); if (!running_) break; group = std::move(orderQueue_.front()); orderQueue_.pop(); } // 批量发送同组订单 sendOrderGroup(group); } } private: void sendOrderGroup(const OrderGroup\u0026amp; group) { // 使用同一个连接发送组内所有订单 auto conn = restClient_.getConnection(); for (const auto\u0026amp; order : group.orders) { conn-\u0026gt;sendOrder(order); } } }; 3. 两种机制的执行结果分析 # 无锁队列机制 日志显示组内订单的发送时间差较大，通常在180-220ms之间 存在较大的延迟波动，部分组的最大时间差超过1000ms 总订单组数: 164 存在时间差的组数: 161 最大时间差: 2961.000ms 平均时间差: 308.851ms 分片机制\n日志显示组内订单的发送时间差非常小，基本在0-1ms之间 订单几乎同时发出，延迟波动很小 总订单组数: 416 存在时间差的组数: 166 最大时间差: 425.000ms 平均时间差: 56.991ms 4. 机制差异分析 # 无锁队列机制 所有订单进入同一个队列 多个线程从同一队列取任务，即使是无锁的，仍然存在竞争 同一组的三个订单可能被不同线程处理，导致时间差 线程调度的不确定性导致组内订单的发送时间不一致 分片机制 通过分片将同组订单分配到同一线程，避免了线程间的竞争 固定线程处理同一分片，确保了组内订单的处理顺序和时间一致性 5. 适合需求的最佳方案 # 分片机制 理由：分片机制能够确保同组订单的处理一致性，满足几乎同时发出的需求 通过减少线程竞争和调度不确定性，分片机制提供了更稳定的性能 6. 最佳方案的优化方向 # 优化分片策略 根据订单特性优化分片规则，进一步提高处理效率 调整线程池配置 根据系统负载动态调整线程池大小，确保资源的合理利用 优化RestClient连接池 根据请求并发量调整连接池大小，确保请求的快速发送 监控和调优 持续监控系统性能，识别瓶颈并进行调优 使用性能分析工具识别和优化关键路径 ","date":"12 December 2024","permalink":"/blog/2025-06-24-order_sending_optimization/","section":"Blog","summary":"1. 需求背景 #在高频交易系统中，我们面临一个典型场景：需要同时处理三个关联订单（三角套利）。这些订单必须几乎同时发出以确保套利的有效性。\n关键挑战：\n订单必须同时或几乎同时发出 系统需要处理高并发的订单组 需要保证订单处理的稳定性和可靠性 2. 当前使用的两种处理订单的机制 # 无锁队列机制 订单生成后进入一个无锁队列 多个线程从队列中取订单进行处理 订单的发送通过RestClient进行，RestClient负责管理HTTP连接池并发送请求 分片机制 订单生成后根据某种规则分配到不同的分片 每个分片由固定的线程处理 同一组的订单被分配到同一个分片，确保组内订单的处理一致性 RestClient同样负责订单的发送 class OrderShard { private: struct OrderGroup { uint64_t groupId; uint64_t timestamp; std::vector\u0026lt;Order\u0026gt; orders; }; std::queue\u0026lt;OrderGroup\u0026gt; orderQueue_; std::mutex mutex_; std::condition_variable cv_; RestClient restClient_; public: void addOrderGroup(OrderGroup group) { { std::lock_guard\u0026lt;std::mutex\u0026gt; lock(mutex_); orderQueue_.push(std::move(group)); } cv_.notify_one(); } void processOrders() { while (running_) { OrderGroup group; { std::unique_lock\u0026lt;std::mutex\u0026gt; lock(mutex_); cv_.wait(lock, [this] { return !orderQueue_.empty() || !","title":"高频交易系统中的大吞吐量订单发送机制"},{"content":"1. 背景问题 #1.1 性能挑战 # 高吞吐量订单处理需求 每个订单都需要 HTTP 请求 JWT Token 生成开销大 网络延迟敏感 1.2 主要痛点 # 单个订单发送造成网络请求过多 JWT Token 频繁生成浪费资源 大量订单并发可能导致系统瓶颈 2. 解决方案 #2.1 JWT Token 缓存机制 #class RestClient { private: static constexpr auto JWT_REFRESH_INTERVAL = std::chrono::seconds(110); // 预留刷新窗口 std::string getOrCreateJWT(const std::string\u0026amp; uri) { auto now = std::chrono::steady_clock::now(); if (!cache.token.empty() \u0026amp;\u0026amp; now \u0026lt; cache.expiryTime) { return cache.token; } cache.token = generateJWT(uri); cache.expiryTime = now + JWT_REFRESH_INTERVAL; return cache.token; } }; 优点：\n减少 JWT 生成次数 降低 CPU 使用率 提高请求响应速度 2.2 智能批量处理机制 #void ExecutionEngine::executeOrder(const OrderReadyForExecutionEvent\u0026amp; order) { auto now = std::chrono::steady_clock::now(); if (collector_.orders.empty()) { collector_.firstOrderTime = now; } collector_.orders.push_back(order); bool shouldBatch = collector_.orders.size() \u0026gt;= BATCH_THRESHOLD; bool withinWindow = (now - collector_.firstOrderTime) \u0026lt;= COLLECT_WINDOW; if (shouldBatch || !withinWindow) { if (collector_.orders.size() == 1) { processSingleOrder(collector_.orders[0]); } else { processBatchOrders(); } collector_.orders.clear(); } } 优点：\n自适应处理策略 平衡延迟和吞吐量 优化网络资源使用 2.3 订单配置结构设计 #struct OrderConfig { struct LimitGTC { std::string baseSize; std::string limitPrice; bool postOnly{false}; }; struct MarketIOC { std::string baseSize; }; Type type; std::variant\u0026lt;LimitGTC, MarketIOC\u0026gt; config; std::string orderId; std::string productId; std::string side; }; 优点：\n类型安全 清晰的数据结构 易于维护和扩展 3. 关键设计参数 #3.1 批处理参数 #static constexpr size_t BATCH_THRESHOLD = 20; // 批处理阈值 static constexpr auto COLLECT_WINDOW = std::chrono::microseconds(50); // 收集窗口 3.2 JWT 缓存参数 #static constexpr auto JWT_REFRESH_INTERVAL = std::chrono::seconds(110); // JWT刷新间隔 4. 性能优化点 #4.1 内存优化 #configs.reserve(collector_.orders.size()); // 预分配内存 configs.push_back(std::move(config)); // 使用移动语义 4.2 批处理优化 # 动态判断是否使用批处理 单订单直接处理 批量订单合并请求 4.3 错误处理 #try { auto response = m_RestClient-\u0026gt;batchCreateOrders(configs); // 处理响应... } catch (const std::exception\u0026amp; e) { LOG_ERROR(\u0026#34;Batch processing error: {}\u0026#34;, e.what()); } 5. 方案优势 # 性能提升：\n减少网络请求数量 降低系统资源消耗 优化内存使用 可靠性：\n完善的错误处理 JWT Token 可靠性保证 订单状态追踪 可维护性：\n清晰的代码结构 类型安全的设计 详细的日志记录 灵活性：\n可配置的参数 自适应处理策略 易于扩展 6. 监控建议 # 性能指标：\n订单处理延迟 批处理大小分布 JWT 缓存命中率 系统指标：\nCPU 使用率 内存使用情况 网络请求统计 业务指标：\n订单成功率 批处理效率 错误率统计 7. 最佳实践 # 参数调优：\n根据实际负载调整批处理阈值 监控并优化时间窗口 定期评估性能指标 错误处理：\n实现重试机制 记录详细错误信息 监控异常情况 性能优化：\n使用移动语义 预分配内存 避免不必要的复制 这个方案通过合理的设计和优化，有效解决了高吞吐量订单处理的挑战，同时保证了系统的可靠性和可维护性。\n","date":"6 December 2024","permalink":"/blog/2025-06-24-batch_order_processing/","section":"Blog","summary":"1. 背景问题 #1.1 性能挑战 # 高吞吐量订单处理需求 每个订单都需要 HTTP 请求 JWT Token 生成开销大 网络延迟敏感 1.2 主要痛点 # 单个订单发送造成网络请求过多 JWT Token 频繁生成浪费资源 大量订单并发可能导致系统瓶颈 2. 解决方案 #2.1 JWT Token 缓存机制 #class RestClient { private: static constexpr auto JWT_REFRESH_INTERVAL = std::chrono::seconds(110); // 预留刷新窗口 std::string getOrCreateJWT(const std::string\u0026amp; uri) { auto now = std::chrono::steady_clock::now(); if (!cache.token.empty() \u0026amp;\u0026amp; now \u0026lt; cache.expiryTime) { return cache.token; } cache.token = generateJWT(uri); cache.expiryTime = now + JWT_REFRESH_INTERVAL; return cache.token; } }; 优点：","title":"高性能订单执行系统设计方案1"},{"content":"0. 内存管理优化 #0.1 大页内存 (Huge Pages) #大页内存是一种内存管理优化技术，主要优势：\n减少 TLB (Translation Lookaside Buffer) 缺失 减少页表项数量 提高内存访问效率 系统配置和检查：\n# 检查系统大页配置 cat /proc/meminfo | grep Huge # 配置大页 echo 20 \u0026gt; /proc/sys/vm/nr_hugepages # 分配20个大页 0.2 内存锁定 (Memory Locking) #防止内存被交换到磁盘，确保数据始终在物理内存中：\n# 检查内存锁定限制 ulimit -l # 修改限制（需要root权限） echo \u0026#34;* soft memlock unlimited\u0026#34; \u0026gt;\u0026gt; /etc/security/limits.conf 0.3 内存优化实现 #struct IOBuffer { char* data; size_t size; explicit IOBuffer(size_t s) : size(s) { // 1. 尝试使用大页内存 data = static_cast\u0026lt;char*\u0026gt;(mmap(nullptr, size, PROT_READ | PROT_WRITE, MAP_PRIVATE | MAP_ANONYMOUS | MAP_HUGETLB, -1, 0)); if (data == MAP_FAILED) { // 2. 回退到普通内存 + 预填充 data = static_cast\u0026lt;char*\u0026gt;(mmap(nullptr, size, PROT_READ | PROT_WRITE, MAP_PRIVATE | MAP_ANONYMOUS | MAP_POPULATE, -1, 0)); if (data == MAP_FAILED) { throw std::runtime_error(\u0026#34;Failed to allocate memory\u0026#34;); } } // 3. 尝试锁定内存 if (mlock(data, size) != 0) { LOG_WARN(\u0026#34;Failed to lock memory: {}\u0026#34;, strerror(errno)); } } ~IOBuffer() { if (data != MAP_FAILED \u0026amp;\u0026amp; data != nullptr) { munlock(data, size); munmap(data, size); } } }; 1. io_uring 多路 I/O 优势 #1.1 传统 I/O 模型的问题 #// 传统 epoll 模型 int epoll_fd = epoll_create1(0); struct epoll_event events[MAX_EVENTS]; // 每个 I/O 操作都需要系统调用 read(fd, buffer, len); // 系统调用 write(fd, data, len); // 系统调用 epoll_wait(epoll_fd, events, MAX_EVENTS, timeout); // 系统调用 问题：\n每个 I/O 操作都需要独立的系统调用 上下文切换开销大 数据复制次数多 1.2 io_uring 的改进 #struct io_uring ring; struct io_uring_sqe *sqe; struct io_uring_cqe *cqe; // 批量提交 I/O 请求 for (int i = 0; i \u0026lt; n_requests; i++) { sqe = io_uring_get_sqe(\u0026amp;ring); io_uring_prep_read(sqe, fds[i], buffers[i], len, offset); sqe-\u0026gt;user_data = i; // 标识请求 } // 一次系统调用提交所有请求 io_uring_submit(\u0026amp;ring); 优势：\n批量提交减少系统调用 零拷贝 I/O 异步处理多个 I/O 请求 2. WebSocket 多连接处理实现 #2.1 基础结构 #struct IOContext { int fd; IOBuffer buffer; std::function\u0026lt;void(const char*, size_t)\u0026gt; callback; }; class WebSocketClient { private: struct io_uring ring; std::vector\u0026lt;IOContext\u0026gt; contexts; static constexpr int QUEUE_DEPTH = 256; // ... }; 2.2 多连接 I/O 处理 #void WebSocketClient::processMultipleConnections() { struct io_uring_params params = {}; params.flags = IORING_SETUP_SQPOLL; params.sq_thread_cpu = cpu_core_; // 初始化 io_uring io_uring_queue_init_params(QUEUE_DEPTH, \u0026amp;ring, \u0026amp;params); // 为每个连接提交读请求 for (auto\u0026amp; ctx : contexts) { struct io_uring_sqe *sqe = io_uring_get_sqe(\u0026amp;ring); io_uring_prep_read(sqe, ctx.fd, ctx.buffer.data, ctx.buffer.size, 0); sqe-\u0026gt;user_data = reinterpret_cast\u0026lt;__u64\u0026gt;(\u0026amp;ctx); } // 一次提交所有请求 io_uring_submit(\u0026amp;ring); // 处理完成事件 while (running_) { struct io_uring_cqe *cqe; int ret = io_uring_wait_cqe(\u0026amp;ring, \u0026amp;cqe); if (ret == 0) { IOContext *ctx = reinterpret_cast\u0026lt;IOContext*\u0026gt;(cqe-\u0026gt;user_data); if (cqe-\u0026gt;res \u0026gt; 0) { // 处理数据 ctx-\u0026gt;callback(ctx-\u0026gt;buffer.data, cqe-\u0026gt;res); // 提交新的读请求 struct io_uring_sqe *sqe = io_uring_get_sqe(\u0026amp;ring); io_uring_prep_read(sqe, ctx-\u0026gt;fd, ctx-\u0026gt;buffer.data, ctx-\u0026gt;buffer.size, 0); sqe-\u0026gt;user_data = cqe-\u0026gt;user_data; io_uring_submit(\u0026amp;ring); } io_uring_cqe_seen(\u0026amp;ring, cqe); } } } 2.3 性能优化技巧 #批量提交优化 #void submitBatchRequests() { int pending = 0; for (auto\u0026amp; ctx : contexts) { struct io_uring_sqe *sqe = io_uring_get_sqe(\u0026amp;ring); io_uring_prep_read(sqe, ctx.fd, ctx.buffer.data, ctx.buffer.size, 0); sqe-\u0026gt;user_data = reinterpret_cast\u0026lt;__u64\u0026gt;(\u0026amp;ctx); pending++; // 达到批次大小时提交 if (pending == BATCH_SIZE) { io_uring_submit(\u0026amp;ring); pending = 0; } } // 提交剩余请求 if (pending \u0026gt; 0) { io_uring_submit(\u0026amp;ring); } } 内存对齐和缓存优化 #struct alignas(64) IOContext { // 缓存行对齐 int fd; IOBuffer buffer; std::function\u0026lt;void(const char*, size_t)\u0026gt; callback; char padding[CACHE_LINE_SIZE - sizeof(fd) - sizeof(buffer) - sizeof(callback)]; }; 3. 性能监控和调优 #3.1 性能指标收集 #struct IOStats { std::atomic\u0026lt;uint64_t\u0026gt; total_requests{0}; std::atomic\u0026lt;uint64_t\u0026gt; completed_requests{0}; std::atomic\u0026lt;uint64_t\u0026gt; total_bytes{0}; std::atomic\u0026lt;uint64_t\u0026gt; error_count{0}; void recordRequest() { total_requests++; } void recordCompletion(size_t bytes) { completed_requests++; total_bytes += bytes; } void recordError() { error_count++; } }; 3.2 性能监控示例 #void monitorPerformance() { while (running_) { auto start_stats = io_stats; std::this_thread::sleep_for(std::chrono::seconds(1)); auto end_stats = io_stats; uint64_t requests_per_sec = end_stats.completed_requests - start_stats.completed_requests; uint64_t bytes_per_sec = end_stats.total_bytes - start_stats.total_bytes; LOG_INFO(\u0026#34;IO Stats: {} req/s, {} MB/s\u0026#34;, requests_per_sec, bytes_per_sec / (1024 * 1024)); } } 4. 最佳实践总结 # 批量处理\n合并多个 I/O 请求 减少系统调用次数 提高吞吐量 内存管理\n使用大页内存 内存对齐 避免内存拷贝 CPU 亲和性\n绑定 io_uring 工作线程到特定 CPU 减少 CPU 缓存失效 错误处理\n优雅降级 自动重试机制 详细的错误日志 监控和调优\n实时性能指标 系统资源使用情况 异常情况告警 通过这些技术的组合使用，可以构建一个高性能、可靠的多连接 I/O 处理系统。io_uring 的异步特性和批量处理能力，配合合理的内存管理和监控机制，能够显著提升系统的整体性能。\n","date":"6 December 2024","permalink":"/blog/2025-06-24-io_uring_basics/","section":"Blog","summary":"0. 内存管理优化 #0.1 大页内存 (Huge Pages) #大页内存是一种内存管理优化技术，主要优势：\n减少 TLB (Translation Lookaside Buffer) 缺失 减少页表项数量 提高内存访问效率 系统配置和检查：\n# 检查系统大页配置 cat /proc/meminfo | grep Huge # 配置大页 echo 20 \u0026gt; /proc/sys/vm/nr_hugepages # 分配20个大页 0.2 内存锁定 (Memory Locking) #防止内存被交换到磁盘，确保数据始终在物理内存中：\n# 检查内存锁定限制 ulimit -l # 修改限制（需要root权限） echo \u0026#34;* soft memlock unlimited\u0026#34; \u0026gt;\u0026gt; /etc/security/limits.conf 0.3 内存优化实现 #struct IOBuffer { char* data; size_t size; explicit IOBuffer(size_t s) : size(s) { // 1. 尝试使用大页内存 data = static_cast\u0026lt;char*\u0026gt;(mmap(nullptr, size, PROT_READ | PROT_WRITE, MAP_PRIVATE | MAP_ANONYMOUS | MAP_HUGETLB, -1, 0)); if (data == MAP_FAILED) { // 2.","title":"高性能网络编程：io_uring 与内存优化技术详解"},{"content":"","date":null,"permalink":"/tags/hft-system-design/","section":"Tags","summary":"","title":"HFT System Design"},{"content":"1. 业务背景与挑战 #在高频交易系统中，需要同时维护多个WebSocket连接以订阅不同交易所的行情数据。主要挑战包括：\n需要处理多个交易所的并发连接 对消息处理延迟有严格要求 需要保证数据处理的稳定性 系统资源（CPU、内存）的高效利用 2. 传统方案的局限 #2.1 传统消息队列方案 #// 常见的消息处理流程 WebSocket接收 -\u0026gt; 消息队列 -\u0026gt; 处理线程池 -\u0026gt; 业务处理 存在的问题：\n消息经过队列带来额外延迟 线程切换开销大 内存拷贝次数多 资源竞争导致性能不稳定 3. 优化方案设计 #3.1 核心设计理念 # 零拷贝数据处理 CPU亲和性绑定 预分配内存 每个连接独立处理 3.2 关键组件设计 #struct ConnectionContext { // 连接基础信息 std::shared_ptr\u0026lt;WebSocketClient\u0026gt; client; std::string endpoint_name; // 性能优化相关 int cpu_core{-1}; // CPU核心绑定 char* direct_buffer{nullptr}; // 预分配缓冲区 static constexpr size_t BUFFER_SIZE = 64 * 1024; std::shared_ptr\u0026lt;MessageProcessor\u0026gt; dedicated_processor; // 资源管理 ~ConnectionContext() { if (direct_buffer) { munlock(direct_buffer, BUFFER_SIZE); munmap(direct_buffer, BUFFER_SIZE); } } // 禁用拷贝以保证资源安全 ConnectionContext(const ConnectionContext\u0026amp;) = delete; ConnectionContext\u0026amp; operator=(const ConnectionContext\u0026amp;) = delete; }; 3.3 优化细节 # 内存管理优化 // 使用大页内存和内存锁定 void* buffer = mmap(nullptr, BUFFER_SIZE, PROT_READ | PROT_WRITE, MAP_PRIVATE | MAP_ANONYMOUS, -1, 0); mlock(buffer, BUFFER_SIZE); 原因：\n避免动态内存分配 减少页面错误 提供稳定的内存访问性能 CPU亲和性优化 void setupRealtime(int cpu_core) { cpu_set_t cpuset; CPU_ZERO(\u0026amp;cpuset); CPU_SET(cpu_core, \u0026amp;cpuset); pthread_setaffinity_np(pthread_self(), sizeof(cpu_set_t), \u0026amp;cpuset); } 原因：\n减少线程迁移 提高CPU缓存利用率 降低延迟抖动 消息处理优化 // 直接在IO线程处理数据 context-\u0026gt;client-\u0026gt;receiveMessages([context](const char* data, size_t length) { // 直接使用预分配缓冲区 memcpy(context-\u0026gt;direct_buffer, data, length); context-\u0026gt;dedicated_processor-\u0026gt;processMessage(/*...*/); }); 原因：\n消除线程切换开销 减少数据拷贝次数 提供确定性的处理延迟 4. 性能监控 #struct PerformanceMetrics { std::atomic\u0026lt;uint64_t\u0026gt; message_count{0}; std::atomic\u0026lt;uint64_t\u0026gt; total_latency_ns{0}; std::atomic\u0026lt;uint64_t\u0026gt; max_latency_ns{0}; }; 实现了精确到纳秒级的延迟监控，便于:\n实时监控系统性能 及时发现性能问题 提供优化依据 5. 方案优势 # 延迟优化 从微秒级优化到纳秒级 消除了队列和线程切换开销 提供稳定的处理延迟 资源利用 CPU资源隔离 内存访问优化 减少系统调用 可靠性保证 资源自动释放 连接状态监控 异常处理机制 易于维护 清晰的代码结构 完善的监控指标 模块化设计 6. 实际效果 # 消息处理延迟降低到纳秒级别 CPU利用率更加均衡 系统稳定性显著提升 内存使用更加高效 7. 总结 #本方案通过深入优化系统底层，实现了高性能的多WS连接处理。关键在于：\n合理的内存管理 优秀的CPU亲和性设计 高效的消息处理机制 完善的性能监控体系 这些优化使系统能够满足高频交易对低延迟的严格要求。\n","date":"3 December 2024","permalink":"/blog/2025-06-24-multi_quote_data_processing/","section":"Blog","summary":"1. 业务背景与挑战 #在高频交易系统中，需要同时维护多个WebSocket连接以订阅不同交易所的行情数据。主要挑战包括：\n需要处理多个交易所的并发连接 对消息处理延迟有严格要求 需要保证数据处理的稳定性 系统资源（CPU、内存）的高效利用 2. 传统方案的局限 #2.1 传统消息队列方案 #// 常见的消息处理流程 WebSocket接收 -\u0026gt; 消息队列 -\u0026gt; 处理线程池 -\u0026gt; 业务处理 存在的问题：\n消息经过队列带来额外延迟 线程切换开销大 内存拷贝次数多 资源竞争导致性能不稳定 3. 优化方案设计 #3.1 核心设计理念 # 零拷贝数据处理 CPU亲和性绑定 预分配内存 每个连接独立处理 3.2 关键组件设计 #struct ConnectionContext { // 连接基础信息 std::shared_ptr\u0026lt;WebSocketClient\u0026gt; client; std::string endpoint_name; // 性能优化相关 int cpu_core{-1}; // CPU核心绑定 char* direct_buffer{nullptr}; // 预分配缓冲区 static constexpr size_t BUFFER_SIZE = 64 * 1024; std::shared_ptr\u0026lt;MessageProcessor\u0026gt; dedicated_processor; // 资源管理 ~ConnectionContext() { if (direct_buffer) { munlock(direct_buffer, BUFFER_SIZE); munmap(direct_buffer, BUFFER_SIZE); } } // 禁用拷贝以保证资源安全 ConnectionContext(const ConnectionContext\u0026amp;) = delete; ConnectionContext\u0026amp; operator=(const ConnectionContext\u0026amp;) = delete; }; 3.","title":"高频交易场景下的多WS连接低延时方案设计"},{"content":"","date":null,"permalink":"/tags/others/","section":"Tags","summary":"","title":"Others"},{"content":"HSBC 速记 汇丰私人财富规划\n玺越世家 · 臻享沙龙 上海站\n（速记稿）\n时间：2024 年 11 月 24 日\n地点：上海浦东文华东方酒店 LG1 层东方厅\n主持人：女士们，先生们，各位尊敬的来宾，我是陈佳昊（音），我是汇丰私人财富规划上海分区总经理，我代表上海汇丰私人财富规划欢迎各位的莅临。\n今天有很多新朋友，也有很多老朋友，我在周五的时候问过后台同事报名报了多少了，他告诉我们已经快要接近 200 人了，但从今天的规模来看，我感觉好像今天的人数还要再超过一些。\n当然了，有一些是原先的老客户，也有很多是慕名而来，看到这次邀请的是付鹏先生，所以慕名而来。也有一些新朋友。在付鹏先生上台之前，请允许我对汇丰私人财富规划做简短的介绍。\n汇丰私人财富规划是全球的战略重点之一，老朋友都知道，汇丰私人财富规划成立于 2020 年，距今刚好四年，在四年的过程中集团一直在给我们大力注资，也是集团里最重要的项目之一。\n为什么聚焦在中国市场上？大家很多人都明白，中国中产阶级的人数在世界上占有量是最庞大的，随着中国经济的高速发展，中国人财富管理的需求逐步提升到很高的水准。所以，私人财富规划也会变成汇丰的重要战略之一。\n介绍一下发展历史，从 2020 年汇丰私人财富规划成立，先是在上海和广州，总部离这里不远，汇丰总部就在国金，欢迎大家去坐一坐。逐步进入到杭州、深圳、北京、佛山，今年在苏州、成都开立了分支机构。\n2020 年汇丰私人财富规划才刚刚成立，那汇丰的历史又是怎么样的？汇丰简称叫 HSBC，很多人会问 HSBC 四个字母分别代表着什么，可以跟大家简单介绍一下，H 代表的是香港的意思，S 代表的是上海的意思。很多人印象中以为汇丰是一家外资银行，但其实大家有所不知，其实汇丰在清朝的时候就在外滩已经设立了总部，现在这栋楼交给了浦发银行。1949 年之后，汇丰因为历史的原因退出了中国，在 WTO 之后回到了中国。\n汇丰 1865 年成立至今已经有 100 多年了，那时候还是清朝的同治年间，同时已经在全球的 62 个国家还有 3900 多名客户，这段历史和这么大的分布也是汇丰很多同事内心的骄傲。我们跟很多客户做沟通的时候，经常会把这段历史拿出来跟大家讲一讲，就像这头石狮子，很多人都见过，但很多人都不知道它的历史，很多人在海报、广告、港元大钞上看过这个石狮子，原来在外滩上也有两座，现在放在上海博物馆里，前一阵儿我在博物馆参观的时候还看到了这两只石狮子，上面还有很多历史的痕迹，比如说战争而留下的弹孔，就在人民广场的博物馆里，大家有兴趣的话可以去看一下。\n财富大矩阵与中国内地市场，汇丰集团对于中国私人财富规划业务的重视程度，在大矩阵中承担了很重要的地位。\n每 100 位客户中，会有 87 位客户将汇丰私人财富规划视作为提供财富重要的主要品牌，提出了很多好评，82% 的调研者打出 9-10 分的高分。\n也有一些比较有意思的话，如：“对产品内容的保障满意，公司大有保障；甄汇生活有一定的吸引力，汇丰的产品较贵但也愿意买，因为对汇丰私人财富规划师的认可。”\n这两年提出一句比较新的 Slogan“懂你关心的，给你安心的”。\n今天的活动我们邀请到了一位重量级嘉宾，他曾任职于雷曼兄弟、所罗门投资集团等全球顶尖金融机构，从事对冲基金等相关工作。他就是东北证券首席经济学家付鹏先生。让我们欢迎付鹏先生为我们带来《2024 年年终回顾和 2025 年展望——对冲风险 VS 软着陆》主题分享，有请付鹏先生！\n付鹏：正值年底，虽然刚才汇丰一直强调大家不录音不录像，但大概率你挡不住。我在这儿讲话会谨慎一些，非常小心谨慎，大概率会有人透露出去，放到 YouTube 上，基本上所有见我都说付总我在 YouTube 上看过你的视频，我说那都是盗版的，靠盗版发财的也不少。\n今天和大家分享的内容基本上都是官方的，回顾会多一点，展望不多，因为这个月展望完了之后下个月怎么办？有些话对我来讲我倒觉得很简单，本质上原来我们是做 Hedge Fund 出身，所以我们的逻辑框架整体具有极强的延续性，不是说今年去讨论，或者说明年去讨论。\n惯性思维从 2016 年开始，我一直在跟大家强调这个世界已经完全不一样了。当然经历过过去的几年时间，我相信在座各位应该对这番话的理解变得越发深刻。\n2016 年实际上是美国特朗普的第一次大选，我有一个特点，我的特征是如果我觉得什么地方有投资机会，我可能第一时间去一线调研，我不喜欢看 YouTube，我也不喜欢在网上扒。当然你会说，现在 ChatGPT 很强大了，人工智能好像能帮你解决很多问题，但你们有没有想过，可能广泛流传或者广泛传播的很多信息是错的。这一点在 2012 年当时我从日本做完调研回来之后，我的感悟是最深的。\n当然去日本有一个重要的人物，名字叫本森特，很快大家就会非常熟悉他的，目前来讲应该是特朗普政府提名的美国财长。本森特原来是索罗斯基金实际掌控人，因为索大爷已经年龄很大了，去年的时候才刚刚把基金的业务交给他儿子亚历山大，但在这之前，最主要的几场战役本质上来讲都是本森特在主导。\n2012 年当时我从北京去香港约朋友们吃饭的饭局上，当时斯索罗斯基金在香港办公室跟我说，本森特从这儿去了日本。我说 OK。我经常说一句话 “站在巨人的肩膀上看问题。”\n当然你知道，网民们最可怕的地方是巴菲特 “SB”、索罗斯 “SB”，我最 “牛逼”。你要记住，他们的所有行为一定有很大的变化，很多人可能都不知道，巴菲特第一次去是 2011 年，我们正在讲福岛核电站泄漏，核废水污染以后海鲜不能吃的时候，一个 80 多岁的老头顶着核辐射泄漏去日本吃海鲜了，当然他去日本干吗，这其实很关键。\n之后我们跑到日本做完调研回来之后那几年，我陆陆续续跟很多人讲，日本正在发生变化，日本的利率结构都会随之变化的，当然包括日本的证券市场。今年日本股市终于走出这 35 年了，创下了历史性纪录。\n但网上很多人还在说，我从经济数据里好像没看出什么状况来，这就是我们说的 “信息差”，因为有时候你知道人的理解，对社会的理解，对经济的理解带有惯性思维。前几年我经常普及的一点是关于日本的理解，很多人总在想经济增长，有没有一种可能性经济不增长也很爽呢？比如说中国过去三四十年改革开放之后我们习惯的就是经济要增长，经济不增长我们就很难过，你有没有想过一种不增长还会把蛋糕吃多的方法呢？答案是分配，你怎么老想着分工、努力、工作、干活儿、挣钱、增长，有没有另外一种可能性是进行再分配？\n你对日本的理解为什么要增长？用我的话说在过去 30 年的时间里保持着这块蛋糕没有变，但现在远端利率抬起来的根本原因是因为年轻人可以吃多了，年轻人为什么可以吃多了？你们知道 2012 年日本的死亡数据是什么吗？你有注意过他的人口结构变化吗？到了今天为止，你突然之间发现日本现在招聘怎么会是应聘的在下面坐着，招聘的在上面站着？放心，中国现在不是招聘的问题，是 HR 砍人的问题，这种变化的根因到底来自于什么？其实很多人只是惯性思维，你不一定能看懂世界。\n过去 40 年已经发生翻天覆地的变化了，从 2016 年开始，中国也不再是过去 40 年的中国，美国也不再是过去 40 年的美国，日本也不再是过去 40 年的日本，东南亚也不再是过去 40 年的东南亚，你资本运转的逻辑框架都在发生着巨变，而这种时刻下，如果你保持着过去的思维，你并不能理解我在讲什么。\n我只能说，大家一切看缘分，我不需要完全说 “付总在胡说，我并不认同”，无所谓的，你能听懂你就听懂，你能早理解你就早理解，早理解你就能顺着这条线 Get 到 2016 年之后世界发生的巨变。\n最新的美国大选，特朗普重新上来，但这次上来跟 2016 年又不一样了，因为他比 2016 年变的更加右翼化了，2016 年大的政治转变本质上就是逆全球化和右翼化。2016 年我把我自己的书稿整理过一版，当年也没空没时间把这个东西出版，去年因为我们家孩子回来以后做了传媒公司，原则上来讲我就把书稿送给他作为传媒公司的一部分出版业务去做，这就是大家后来见到的《见证逆潮》。但这本书不完整，全文将近 70 万字，你们拿到手的只有 50 万字，中间差不多有 20 万字被删减掉了，这 20 万字其实非常关键，涉及到我们对世界大类资产顶层逻辑的核心框架，金字塔究竟是什么？底层是我们的所有资产和市场，市场其实是在框架中最底层的，大家天天想问的房价的上涨、股票价格，这实际上是金字塔中最底层的。\n稍微往上一点有人说宏观经济很重要，尤其是中国 2008 年次贷危机结束之后，中国的投资人开始发生巨变，2008 年金融危机之前，中国大部分投资人讲的是 “擒龙大法”，如何抓涨停。但 2008 年的次贷危机，全球的冲击使得很多从事金融交易、资产交易的人开始意识到，原来全球金融市场是这样的，是联动的。自那一刻起，真正意义上的金融才开始在中国慢慢生根发芽。\n稍微有人开始意识到宏观经济的重要性，当然像现在证监会的首经团队中，36 个首经里，我一直说我是那个最不正经的，因为我又不是搞学术的，我也不是搞政府出身的，我是市场一线的，我们对很多问题的理解是完全不同的。\n前两天的时候，在 Fox News 上，本森特和几个诺贝尔经济学家在那儿争吵关于关税作用的时候，你就突然间发现，一个站在市场角度上的人理解关税作为一个手段到底起到什么样的作用，和那帮老学究们去讲的，甚至跟在座各位在新闻联播上看到的关税描述 “美帝国主义打关税，使得老百姓生活在水深火热中”，你会发现好像你聪明点的话就知道好多东西并不对，这就是差异性。这次特朗普组成的第一是实权派，第二是通杀了，可以理解右翼化已经完全没有牵制了，第三上来的全是实干派。你猜后面的结果是什么？这场仗可不好打，比 2016 年那一届还难拿。\n再往上是什么呢？有人说终于讲到了政治，没错，再往上是政治，民主党、共和党、全球政治的变动。但再往上的顶层，金字塔的最核心是什么东西？实际上是意识形态。\n我教很多研究员说你们在研究世界经济的时候别盲目地做简单的对比，我估计很多研究报告都会犯这样的错误，动不动就做对比，和 70 年代、80 年代做对比，这种对比纯粹写报告凑字数的，换成我的角度，我都不会看完直接撕了就扔垃圾桶了，我其实挺心疼这些券商研究员的，为什么呢？是个事儿就得写个报告，写个报告就得好几万字，好辛苦，结果还没人看。\n顶层断代，也就是大家经常讲的周期性的断代到底是什么？你记住一点，顶层的断代是意识形态，社会政治的发展本质上是群体性意识形态的周期，也就是大家学过的思想政治课中的 “左” 和 “右”，左倾右倾、左派右派、左翼右翼、集体个体，这些东西的变动才是世界经济周期的最大变动。\n当前是什么？大领导讲的那句话很对，百年之未有大变局（百年未有之大变局），基本上就是 1929 年大萧条到二战后完整一个周期的结束。全球从二战的极端右翼，慢慢发展到中右，再偏向中左，再到差不多这 20 年左右的极端左翼化，最终导致重新开始右翼，这个世界很有意思，没有任何方向是绝对正确的。\n我一直告诫大家，你们不要在网上争吵，我站在左边，你站在右边，PK，非得讨论出谁最牛逼，谁最正确，没有。就好像你想找个女朋友，又漂亮，腿又长，胸又大，腰又细，还有钱，还特别爱你，你想多了，挑一样就行了，完美主义哪儿有？最终的结果就是在运动的过程中对政治造成影响，进而对经济造成影响，进而对金融市场造成影响，一定记住金字塔的逻辑。\n大部分时间我们不用 Care 顶层，因为在过去百年，顶层的方向是固定的，就是从极端右翼不断向左翼在运动，所以顶层方向不变的情况下就会形成下面的一套运转逻辑。\n比如说以美国为代表，你们应该看到 Ray Dalio 关于债务危机的那本书，里面有个利率曲线，二战前我们的利率就是 0，到 80 年代利率达到顶峰，2008 年次贷危机到疫情期间利率再次达到 0。利率的低点到底是什么意思？为什么在过去的百年历史里，利率的低点总是战争的起点？是有原因的，因为利率的本质实际上就是贫富差距，利率越低的时候，贫富差距越大的，利率越高的时候，贫富差距越小的。\n当然，我们每个人都是屁股决定脑袋，比如大家手上拿着一堆金融资产，拿着一堆杠杆，我可以告诉你，你永远高呼的是低利率是对的，就好像大家对于美国的理解一样，永远认为美国在过去的 40 年的逻辑就是不能加息，加息就崩溃，崩溃就降息，利率永远是低的，美元永远是 Carry 的借贷方，但你从来没有想过这个逻辑会变的。\n几年前我跟很多人讲的时候，我说你记住一点，中国从高利率变低利率，海外从低利率变高利率，所谓的几毛档还动不动说付总说了高利率是多少，4-5 都是高利率。低利率是多少？0、1、2 有多大区别吗？没有的，这是关键的点。有人说非得纠，把我的话直接变成了付总说的，中国不会加息，永远降息，美国永远加息不会降息，降一点加一点，加一点降一点，这不很正常吗？比如特朗普上来了，明年有没有另外一种意外性呢？比如说降了 50，降了 25，大概回到 4.75，5 的水平了，突然间又抬回到 5.25 了，这不正常吗？4 和 5 的纠结不重要，重要的是他不会再回到 0、1、2 了，这是很关键的。\n对于劳动力来讲，利率是很关键的，如果利率的抬升来自于劳动收入的增长，这是好事情。你想想中国，你把时间拨回到 20 年前，利率高不高？那时候你难受吗？不难受。现在利率低不低？低，你难受吗？你难受。为什么？所以你要知道是通过劳动获得收入还是通过资本杠杆获得收入的，你对利率的感觉完全是反的。\n但是整体社会去讨论贫富的时候，贫富主要讨论来自于劳动价值，简单讲，天天外面送外卖跑滴滴的，他们就是失去的一代，在过去的二三十年里，靠劳动力的就是被淘汰的一代，没办法，这是社会发展的必然结果。但是当这种矛盾压力大了，就会转化成社会矛盾，甚至可以通过选票转化成对政治的影响，对意识形态的影响。所以贫富到极端的时候一定会进行修正，无论是极左的贫富还是极右的贫富，都会最终产生矛盾，这就是社会运转的规律。\n过去百年发生的这一轮大周期就是完整的周期，到 2016 年表面上看叫中美贸易战，表面上看是中美两个大国之间所谓的对抗和博弈，其实是全球各个国家内部的矛盾展示，对内是内部的分配，对外是外部的分配。在这个背景下，战争的风险将加大。\n前两天，我们也看到了，人类历史上第一次使用了洲际弹道导弹，无非就是前面没挂核弹头而已，仅此。你觉得这个东西离你还很远吗？我们这一代可以说是幸福的一代，但我们这一代也将经历不常见的百年之未有大变局了。\n很多人在思考这个世界的时候，真的不要以为我们现在还能回到过去，回不去了，那个全世界包容融合的，不断左翼化推进的，全球化不断推进的路径，这个时代彻底在 2016 年已经开始结束了。\n2016 年很多人判断是错误的，总觉得 2016 年只是一场贸易战，那时候我从华盛顿调研两周回来以后，我跟很多人讲，不是贸易战，不是说哪个党派，民主党、共和党上来对中国就会温和的，不会的，两党的共识，他两者之间只存在着我比你左一点，我比你右一点，但咱俩都是往右的。美国政治的变化核心在于不管民主党和共和党，对中国的压力都是一样的，只不过是他俩谁压力多一点谁压力少一点，谁在外交上压力多一点，谁在经济上压力多一点，仅此而已。\n对中国来讲现在也麻烦，在过去的 80 年代、90 年代，西方在不断包容融合右翼化，同时当时的中国也在不断地往右走，当然此处不是指的中国的右倾。注意西方右翼概念和中国完全反的，你们不要觉得是错误的，如果你觉得错误的，你先了解了解什么是左派右派、左倾右倾、左翼右翼。如果你不能理解这个东西，你肯定对左和右在中国和西方的框架里完全是颠倒的。\n中国也是朝着包容、融合的，所以我们才有了非常好的入市、WTO、改革开放等一系列的操作。我经常说家庭中女生是天生右翼，右翼有一个典型特征，右翼可以叫民粹，可以叫国家主义，可以叫爱国主义，极端右翼可以叫纳粹。但右翼的特征很明显，“我没错，都是你的错”，这就是右翼。\n家庭中女同志天生带有，当然我不是歧视各位女士，这是你们 DNA 里带的，两个 X 里带的，如果家庭中男生是左翼，家庭是幸福的，什么意思呢？“老婆，没事儿没事儿，都是我的错”，男生是个左翼，家庭很好，左 + 右。\n如果男女都是左翼，这简直是幸福无比了，男生回家了，女生把拖鞋一放说 “老婆你打会儿游戏，我正做饭呢，一会儿做好了叫你洗手吃饭”，这个男同志真的去打游戏了，吃完饭了说 “你歇一会儿，看会儿剧，我来把碗洗了。” 你可千万别当个大直男，锅一甩又去玩儿了，不行，时间长了，她会右翼化的。\n此时你也表现出左翼特征，你家庭就是融合的。家里如果两个右翼就完蛋了，都是你的错我没错，凭什么说是我的错？就是你的错，就是你做错了。直男碰上女生，一般来讲没啥好组合，两个右翼就是战争，打架到离婚。\n不要认为这是在讨论家庭、婚姻，同样在讨论国家，同样在讨论全球。当国家和国家之间的组合出现统统左翼化的时候，就是包容、融合、全球化共同增长的俱佳的历史环境，当全是右翼化的时候，就是战争。\n我们现在的大麻烦在哪儿？就在这儿。全世界在过去 5、6 年时间里已经陆陆续续都在右翼化了，右翼化的特征，政治的重要性已经体现出来了，选票回归传统的特征已经体现出来了，反移民的特征已经体现出来了。\n我之前说过，这两年全球著名的交易就是 “多美国，空加拿大”，原因就是加拿大的小土豆放了那么多印度裔进来，就完蛋了，加拿大的核心矛盾是什么？经济增长创造了五个蛋糕，原本加拿大的国民可以一人吃一个，现在放了 10 个阿 X 进来，加拿大问题是分配，当分配不够的时候，一定会趋于保守，一定会趋于右翼化，一定会趋于反移民。\n各国都一样，70 年代 80 年代的时候，英国那时候有过排华，现在又开始反穆斯林了，这正常。世界的动荡不是简简单单表现在单纯的资产上，前两天英国又出台了新的政策，你只要非英国国民的，原则上来讲要交遗产税的，英国政府也要收你的遗产税。我之前跟很多富人说，别天天琢磨避税了，包容融合的时候藏点私房钱是没事儿的，当都右翼化的时候，你再藏私房钱你就完蛋了，现在全世界的大麻烦是什么？找个税率低的地方该缴的缴。当年特朗普上来的时候 20% 只要你愿意回流美国，全部合法化，你看有多少资本往回回流？所以你们知道左翼和右翼大概的框架和特征，这才是我那本书里的真正精髓，但被删掉了。\n你把它理解了，你对应的穿透到经济上，穿透到利率上，穿透到资产上，你会门清儿的，这就是大类资产的精髓，真正的精髓。你要问这东西谁创造的，索罗斯那批人，本森特那批人，整体框架大家都是一样的。\n我到底在讲什么？其实我就是在讲回顾，因为从那一刻开始，几个问题就在陆陆续续暴露。美国在进行重构，特朗普上来以后方向继续重构，这里面其实就涉及到民主党为什么是错的，民主党的很多政策为什么是极端左翼，左翼政策不一定是对的，右翼为什么会使得美国进入到增长通胀利率的环境。\n包括有些华人带有意识形态，我只能说一句话，我们作为全球投资人，最佳的选择是没有任何意识形态，对我来讲，我非常清楚左有左的问题，右有右的问题，左有左的好处，右有右的好处，我不会站在任何一侧。我的答案是全世界选择往左走，我就知道我的交易路径是什么，全世界选择往右走，我也同样知道我的交易路径是什么，但我绝对不会站队说谁是绝对正确的，否则的话就会压错宝。世界有时候不一定按照我们的意识走，美国的这次大选也是典型的结果，其实我也没想到，美国右翼化的速度会这么快，推进速度会如此迅速。本来想的是民主党还能够撑一撑，但现在来看基本上是完败的。\n对于中国来讲，当前我们面临的问题不仅仅是外患的问题，还包括了内忧。综合在一起，会有一个非常奇特的答案，之前很多人问我中国到底和日本一不一样？网上这句话炒的纷纷扬扬的，有人说中国就是会走日本的老路，有人说中国不会走日本的老路，你要问我正确的答案，我会告诉你这个问题没有任何意义，为什么？太泛了，如果拆的细一点我能回答你。比如你要问中国的居民部门和日本的居民部门一不一样？我的答案是一样；中国的企业部门跟当年日本的企业部门一样不一样？我的答案是不一样；中国的政府部门和当年日本的政府部门一不一样？我的答案是不一样；中国的金融机构跟当年日本的金融机构不一样？不一样；中国当前面临的国际环境和当时 90 年日本面临的国际环境一不一样？不一样。\n你说最后的答案是什么？如果站在纯居民角度来讲，我可以告诉你 99.99% 可以复刻，但如果站在大的国际环境上来讲，可能得到的是完全不同于日本的最终答案。用我的话说，你是说一样还是不一样呢？没有意义。\n所以我大部分时间给你们分拆的是日本居民部门和中国居民部门的比对。而日本的企业部门、金融部门、发展模型我也给大家分享过，去年你们应该都知道了，巴菲特买三井、三菱、丸红、伊藤忠商社，大笔发行日元债券购入到日本的三井、三菱、丸红、伊藤忠这些资产中，他到底在干吗？\n那时候第一财经找我说付总你去讲讲巴菲特为什么买，我发现很多评论人员单纯在讲三井、三菱、丸红、伊藤忠资产怎么样，稍微聪明点的会讲到当年的商社们是日本的海外资产，是日本 Carry trade 套息交易的主要收入端。再聪明一点的会讲到巴菲特在参与日本过去 40 年存量财富的再分配。\n我可能明年把我们家小儿子送到日本去，我跟他讲的很清楚，我不需要你去学习人工智能、AI，为啥呢？你好像没那么聪明，也不是 IT 技术男，你把日语学好，能考上应庆就不错了，那里面都是一些日本传统贵族的姑娘，你娶一个就行了，最好她们家都是 80 岁 90 岁的，你就躺赢就行，等她们家 80 岁 90 岁的明后年一挂，房是你的，股权是你的，土地是你的，财富是你的，存款是你的，咱们就参与日本存量 40 年财富再分配。巴菲特是用钱去参与，我们用人参与，本质上都一样，你买股票，我把儿子嫁过去，这都是参与财富存量分配。\n你们要明白日本的核心究竟是什么？日本的核心是参与分配，而不是参与增长。很多人不太理解，因为他在国内没参与过分配，永远都是增长处在哪个环节，我距离权力近一点，资源近一点，资本近一点，我就多吃点，卖劳动力的就少吃点。当经济增长增速不够的时候，最底层就没饭吃了。经济增长 5，可能各个阶层的体感是完全不同的，所以网上会有些人说经济数据造假，真的造假了吗？也许没有。5 代表的是整体的蛋糕，而你的体感仅仅代表你的阶层。\n在过去几年中国经济的调研中，我们到底做对了什么？\n第一，在 2020 年疫情后，那时候长白山论坛我跟大家讲的很清楚，我说的非常赤裸裸，中国居民资产负债表出现问题，那时候券商们都很 Happy，因为他们永远需要 Happy，只能做多。但对于我们做 Hedge Fund 出身的来讲，我可不能这么做，我这么做我就完蛋了，我的钱在里头。10 月 8 日之后，有人在里头吗？千万别自己麻醉自己，那都扯淡。网上一般来讲，拿所谓的这种东西蒙蒙别人可以，你自己信了就完蛋了，就跟当时 “6000 点不是梦，1 万点刚起步”，记住那话是说给散户听的，你信了那你就完了。核心是什么？从我们的角度非常明确地大家，大家的预期很高，但现实很残酷。\n那两年跟各家公募基金每个季度做交流的时候，他们没法去理解现在的经济情况，比如说那时候我跟他们讲网约车司机、外卖，那两年我大量的调研样本参数是底层。经济增长消费扩张升级的时候，调研样本是富人先进五星级酒店，富人先买超跑，富人先吃海鲜，然后你的样本参数是下沉的，到最后是老百姓吃上海鲜，老百姓开上汽车，老百姓进五星级酒店。\n但是当经济收缩的时候，倒过来的，第一步先收缩的是底层。我前几年我说每年现在新增几千万的网约车司机，你们都没有想想人从哪儿来的吗？有人说了，农村劳动力进城，我说都啥年代了，还农村劳动力进城，这又不是你当年搞大规模基建城镇化建设的时候缺农民工，把农村劳动力大规模转移过来。现在的农村你去看看，哪儿还有劳动力，除了老弱病残幼以外，还有劳动力吗？你就没想想这两年突然激增的两千万的网约车司机这些人从哪儿来的？答案很简单，中产阶级的陨落。只不过是你的阶层不一样，你看的问题不一样。\n很多人的调研很有问题的，很多人说美国通胀导致美国居民部门水深火热，我问他为什么？他说你看我打电话问了我在美国的朋友，他们都很惨。我说那你美国朋友的样本是个什么状态？他一描述，我说那当然惨了，他们以前爽的时候是老公在中国挣着通胀的钱，老婆在那边花着通缩的钱，享受着社会福利保障体系，还不交税。现在倒过来变成了老公在国内挣不着钱了，海外人家上门给你弄个草皮清理一下要多收你 50 美金一小时，你的钱没增长，花的钱多了，你当然难受。我要是那个铲草皮的，我会告诉你那点通胀算个屁，5 块钱的三明治涨到 7.5 元，翻了 50%，对我来讲不重要，重要的是我从你们家弄个草皮，挣 50 多一个小时，劳动价值提升了。从事劳动的人就很舒服，从事单纯支出的人来讲你就难受了。\n你要调研的样本是一样的，前两年的样本收缩的是时候是底层先吃苦，但对宏观经济数据影响不大，你们记住一点。\n我就说网约车司机，如果你在广州做调研，他们的特征就是有钱没钱，今天都吃龙江猪脚饭。但注意，北京北四环的网约车司机吃的中午盒饭到多少钱吗？15 块钱送瓶水，还带锅包肉，猪肉炖粉条子，耙子肉，嘎嘎香。但你记住一点，千万别问肉多少年，问你就吃不下了，因为基本上都是 80 年陈酿拉菲，一定是冻肉，一定是冻了 20 年、10 年以上的肉，不然怎么那么便宜。所以你们也不要瞧不起预制菜，我觉得预制菜很好，没有预制菜老百姓日子更苦，有预制菜老百姓好一点，为啥？12 块钱能吃饱，还能吃上肉，吃上足够的蛋白质。\n你就记住一点，当你都吃 12 块钱了，你还注意肉多少年吗？现在统计中国在讲需求的时候很有意思，我从来不会用一个数字，从来不会用中国的 CPI，中国的 CPI 一直有一个大的问题，当年宏观经济数据设立的时候中国老百姓第一目标是解决吃穿，解决温饱，所以对我们来讲，物价中的菜价、猪肉价格、粮食价格、油价波动，我们看的比天都大。\n那时候一般来讲，领导们下去做慰问的时候，第一件事儿都是去家里掀锅，动作都很标准，打开锅看看你吃啥，这个动作其实就是因为当年我们的重要问题是解决老百姓的吃穿住行，所以我第一件事情就关注你吃的情况。但改革开放下来以后，到现在为止，吃如果都成问题，那就是大问题了。\n为什么不用数据？因为数据中这部分的波动很大，这部分的波动已经跟需求没关系了。比如说城市里洪涝，那蔬菜价格那几天就会暴涨，那种变动其实影响已经不重要了。我们现在讲需求，比如中国经济从 2019 年获得大问题，非常麻烦，你们不要觉得现在的经济问题是现在，是 2019 年就开始了，在今年是恶化的，所以今年的情况你们都不知道有多严峻，数据里已经告诉你，非常严峻，而在调研的时候更严峻。\n8、9 月份的时候，必须转向，那时候很多人不理解，因为过去的一年大家都养成了一种习惯，这也是右翼化的特征，右翼化的特征就是我没错都是你的错，我不许你说我错。你想想，家里的老婆你敢说她错吗？到最后男生就是出门抽烟，不吭。\n过去几年我们的右翼特征当中体现出来的，大家都有一种习惯，国内经济不许说不行，谁说不行谁就是叛徒，谁说不行谁就是不爱国，谁说不行就网上攻击他。问题是诞生了另外一种生意，什么生意？你们懂得，你只要说这东西遥遥领先，8000 块钱的东西就能卖 18000。\n在我的角度看很简单，这是社会的整体意识形态变动的核心，但是真正可怕的是如果大家都不去讲，到了关键的时间点上，会使得所有的信息反馈形成谬论性错误，最后你们会发现，连决策层都做出错误判断，那就完蛋了。\n到最后谁是那个误国误民的，历史会有正确评价。最上头在关键的时刻该做调研，该让你发声，还是要发声的。8、9 月份到底中国发生了什么事情？大部分人在当时并不了解，8 月 27 日开始，你们关注一下所有的金融峰会和论坛上，全部让你敞开了讲中国经济的核心问题。\n当然只是说我当时在大湾区论坛上讲话时间不够长，大家传播更为广泛，但不是说我胆大讲。当时下午讲完之后，晚上就有朋友发 “付总，这能讲吗？” 我说 “如果能讲，你要想想为啥？”24 小时都不到，大概 12 小时左右，第二天早上易纲同志在上海的外滩金融论坛上马上跟你讲当前中国经济的核心问题，通缩的风险，以及经济有效需求不足，所有人从 8 月份到 9 月份，用的词都是一模一样的，中国经济当前核心问题有效需求不足。其实我想说，有效需求从 2019 年开始就在下降了，而此次的有效需求非常麻烦，可以说是我们改革开放之后的百年之未有大变局。\n我在 9 月初的时候，提的政策建议里，我都没有用 “解决”，我用的是 “对冲”。9 月 11 日我怕大家对这个事儿理解不深刻，当时演讲的原题目是要注意有效需求问题，赶快出政策对冲，9 月 11 日我把东西又给你写出来，再讲了一遍。\n但那时候会发现社会上的整体风气依旧沉浸在 “不许说我们不行，我们挺好的”。到现在为止，现实情况是什么？你觉得资本市场起来这一下，猛冲这一下跟经济好有关系吗？恰恰出现的情况是经济差才来了这么一下，而不是经济好。现在出的所有政策有没有达到目标呢？有一讲一，没有。能不能达到对冲的目标呢？我觉得对冲一点点，解决肯定不可能，因为你如果仔细地了解这次有效需求的复杂性，意思是告诉你这事儿挺难，因为里面掺杂了中短长期的因素。其实在疫情后，我们当时就做出中国国内经济的预期很高但现实很差的根因也来自于有效需求背后的矛盾。\n有谁记得前两年我在各个公开演讲中，一直跟你强调国内经济的核心变量是什么吗？我老跟你们讲到人口的问题，老跟你讲到老龄化的问题，几年前跟你们讲到现在为止，通过金融市场、资本市场、银行背后的数据大概都能看明白到底发生了什么，老龄化对于中国、韩国、日本都不是好事儿。\n西方经济研究中研究移民政策，中国、韩国、日本研究人口出生，因为这几个国家的历史决定了他不会有大规模移民的。你别动不动就来一句，老龄化了人口出生少了，北欧怎么怎么样，好家伙，你这一刀切出去，你玩过《文明》吗？马上《文明》就要上新了，大家可以 Steam 上下载玩一玩，开局资源要素是不一样的，最后你组成的文明和帝国发展战略也是不同的，别动不动做瞎对比，没用的，这就是中国经济的大问题。\n这是所有这次有效需求的组合，包括下面我们对的政策建议，不用看了，意义不大。\n核心的就用这两张图够了，很简单。\n第一，我不会用 CPI 这个数字就是因为里面含了实际上已经跟现在有效需求没太大关系的。用的什么呢？把高清大图放大到 2007 年之后，我们用的是扣除食品和能源以后的通胀。简单讲，我要关心的是老百姓吃饱饭以后没事儿干的价格，你没事儿干的价格高，就说明你的有需求好，你没事儿干的价格低，就说明你有效需求差，就这么简单。至于吃饭这件事情，非常容易解决，龙江猪脚饭。\n说实话，你们点 20 块钱 30 块钱的外卖，成本价格就 4 块钱，4 块钱你都吃的嘎嘎香，可以想想，食品工业发展到现在为止，防腐剂、添加剂一加，成本是很低的。当然了，做不到既要又要还要，既便宜，又好吃，还健康，还得是厨子现割肉现做，你想多了，你要想吃现割肉现做，你掏 50，我去你家做，你就掏 5 块，那就是预制菜。你们要明白，这是解决吃喝拉撒很重要的因素。有的时候，左右两边不可共同都有，健康和便宜不可能同时存在的，所以健康很贵的。\n大家先看放大版的数字，2019 年是整个平台的顶峰期，2019 年后的典型特征是总需求曲线一直在降。10 月份的数字是负的，没有疫情，没有 2008 年金融危机的外需崩塌，从 2002—2024 年，长达 22 年的时间里，在没有任何重大风险的情况下，中国第一次出现了有效需求为负。负数啥意思？非常简单，中产阶级节衣缩食，这个宏观数据就告诉你这个答案。\n刚才我讲了，底层老百姓是一点点往上反馈的，不会很快地作用到宏观经济数据里，所以你们在疫情后看到的这个数字大平台还没有快速往下掉，但当时的底层（网约车司机、送外卖）其实一点点在痛苦，但那时候去金融机构做路演，他们都没有这种感觉。今年所有金融机构都觉得很痛苦的原因是啥？因为他们被裁员了，他们被降本增效了，他们被要求奖金退回了，板子打到了他们的阶层之后，他们开始感受到了痛苦。你知道这代表什么吗？今年经济为什么从 3 月份之后这个数字一路掉下来，答案非常简单，今年的大麻烦是中产阶级陨落。\n别说今年了，这两年底层慢慢 “拼多多”，现在应该是中产阶级开始 “拼多多”，今年最好的样本参数调研应该是隔壁的杭州，其实上海也可以做调研，差不多。3 月份降本裁员裁老张，6 月份降本裁员裁老李，我就问你老王怎么办？回家跟媳妇开个香槟庆祝一下，老张、老李被裁了，我没被裁，是这样吗？现实的情况是回家赶紧跟老婆算账，国际学校多少钱，孩子多少钱，面膜多少钱，健身房多少钱，该花的不该花的多少钱，房贷欠了多少钱，一算账列一个数字，假设被裁员怎么办？算完跟老婆说，你的面膜 SPA 中心别去了，李佳琦的直播间拍一个糊脸上差不多。然后你开始节衣缩食收缩，你的收缩是要命的。记住一点，中产阶级的收缩对整个宏观经济是冲击最大的。\n底层真的是今天干个活儿，跑跑，有钱挣没钱挣都得吃个龙江猪脚饭，反正也不贵。多挣钱了，跑个单王，跟老板说 “龙江猪脚饭加个蛋”，今天各位在外卖平台上给打赏 10 块钱就是龙江猪脚饭加个蛋来个腿。\n我没有太多投资的群，但我会潜伏在全国外卖小哥、网约车司机的群里，因为他们是我广大调研阶层的样本参数。我甚至还有个样本参数是全国最大的美容连锁店的老板，我经常拿他当调研样本，为啥？他背后的 2000 多家店，以及店后面的那些女人们，那就是标准的消费调研样本参数，他的生意好经济就好，他的生意差经济就差。杭州今年上半年应该有 500 家美容店要转让，你们有谁要的我给你们搭个线。你们会要吗？你要知道，不管是正宫娘娘还是非正宫娘娘，都没钱了，她背后的男人们都没钱了。\n你说消费降级吗？其实不仅仅是降级，你们一定要记住，这个大周期的结束很可怕的，因为这是大部分中国投资人里第一次经历这样的周期。\n中国证券市场反应非常精准，不要再看上证综指，那个意义不大的，我们经常讲有结构性行情，一点错没有，结果里对经济、政策的反应非常准确，不是不准确，是非常准确。所以说你真正在这几年对宏观经济的理解，就是告诉你一句话，没有增量，就是结构，对结构怎么把握？这里的结构可不是 40 年前的结构。\n前两天我跟一家公司说了一句话，黑色线是 PPI，相信在座各位都明白，PPI 是什么呢？简单讲就是企业利润，PPI 为负，大家就是在拼命地价格战、竞争，我卷你，你卷我，上游卷完卷下游，下游卷完卷客户，卷到最后就卷到谁能活着，这就是 PPI 为负的答案。\n中国这二十多年来，从 2002 年开始，我们的经济从来没有遇到大问题的根因非常简单，红色线永远存在，上面的红色线存在。\n中国经济的任何供给问题都是有需求在的，有内部需求有外部需求，外部需求是全球化对我们的支撑，内部需求是什么？房地产大佬现在好像在里头踩缝纫机，他当年曾经说过一句经典的话，“什么房地产、供应、需求、土地开发、城镇化，扯淡，就一句话，我们有庞大的 80 后”。我觉得他说的非常诚恳，因为需求内需到底是啥？本质上就是人口收入的债务函数、杠杆函数。\n所以你就知道，中国内需庞大的一代是谁，就是这批 80 后。是 “文革” 之后人口基数最大的那批，可以花 3 个钱的，可以花过去时，上一代人给你留下的 6 个口袋。可以花当下时，你的企业老板给你的收入函数。可以花未来时，金融机构给你们的杠杆。你们是花三代的钱，一代的人口高峰，那就是中国内需的所有底牌。中国经济的任何问题都可以由这部分人买单，所有债务问题、经济问题均由这代人买，那就不会有真正意义上的经济的风险。\n比如说 2008 年，现在也会发现，有些政策跟 2008 年很像，房地产放开、限购放开、购置税减免、消费补贴、刺激消费，但你们都会发现，还能产生 2008 年效果吗？能回去吗？我可以明确告诉你，回不去的。\n你们记住一点，那句忽悠了老百姓这几年的一句话叫 “做内债不是债”，我不知道谁让这句话传出来的，很多人在那儿喊 “内债不是债”，这是我们家祖传对联之一，下句是什么？“内债不是债，只要人还在” 横批 “万税万税万万税”。任何国家的本币债务就是对自己本国居民的征税权，税等于什么呢？税基 × 税率，税基等于人口和收入函数，一叠加就是人口收入 × 税率，这就是税和债务。\n中国现在的化债化什么？要么增加税率，要么增加人口，要么增加收入，人口不增，收入不增，答案只有一个，增税率。那你猜你的遗产税跑得了吗？你猜你的房产税跑得了吗？想啥呢，年轻人不生，咱收不着他们了，那就收老年人的，一样的。你要知道，债务不会像你想的 “内债不是债”，你想多了，本质是税源。\n政府债务驱动的投资行为只要能收到税，所有投资行为理论上都是合理的，2008 年两个经济学家在那儿讨论高铁到底应不应该修，当时他们俩的讨论中我站后者，当时应该修，因为修不修就看能不能征到税，但他俩的计算方式是不一样的。其中一个是按照标准的市场经济去计算，市场经济计算税就是这个项目能不能挣钱，杭州到上海这条高铁修完了，成本核算完，二等票需要 150 元，老百姓能不能承担得起，能承担得起能运营得起就会项目回本。所以他经常挂在嘴边的口号就是如果项目不能挣钱，那原则上高铁是浪费的，就是纯纯的债。他这句话在当年是不对的，因为中国非常奇特，中国的税分为间接和直接的，你刚才所有的成本核算是直接税，但中国的特色是直接税上减免，增收间接税。这就给中国老百姓一种很好的感觉，我们的高铁又快又好还便宜，成本 150 的票价，我们只需要 60 就能坐了，老百姓觉得生活便利。\n你咋那么天真和可爱呢？我就问你，剩下的 65 块钱谁掏？然后就来一句，内债不是债，这钱国家掏。咋可能呢？这钱谁掏？你们知道为什么所有基础设施一定跟着城镇化走吗？一定建在新城吗？一定高铁内新城的土地很便宜，圈完了之后，三通一平做完了，十字格一画，土地一卖，盖上房子，80 后 1 万块钱 2 万块钱买房子，什么意思？这叫间接税，我们是间接收税补直接。\n核心是什么？核心就是只要能收上间接税，所有的投资政府基建全能做，间接税收不下来，项目就完蛋了。你们猜中国以后还会有大规模基建吗？我可以明确告诉你不会有了，只有修修补补，因为最大的税源税基没了，这就是 2015、2016 年中国经济里暴露出来的最大问题。\n知道是什么吗？年轻人，你们咋不生了呢？你不生我咋办？你不生税咋办？当时的人口拐点，大规模老龄化开启，年轻人不再生育，这将是巨大的麻烦，因为我们所有债务的兜底没了，谁给我们兜？此时很简单，去海外收税，所以大家就明白，我们要走国际化，国际化的本质就是向海外征税。政府、企业、金融均向居民征税，记住一点，企业征税就是所谓的商品通胀，1 块钱的东西卖 2 块钱，就是向居民部门收 1 块钱的税，但国内 PPI 持续为负，代表着企业征不上来税，企业恶性循环，企业债务严峻。PPI 为正，代表着企业可以通过通胀、价格转移的方式向居民转移，也就是向你征税。\n只要居民部门在，通过供给端的调整，都会带来周期性的 PPI 恢复，简单讲，供给侧改革一搞，房地产一推，老百姓一买单，企业的债务就不是债务了。所有政府的债务、企业部门的债务、金融部门的债务，只要居民部门能扛得动，都不是债。\n你们知道现在的大问题是什么吗？我现在说了某一个行业，你知道有些人犯的巨大错误是什么？到现在为止跟我讲，付总 PPI 为负很正常，市场化竞争，优胜劣汰，弱者淘汰，强者赢家通吃。我说现在不是的，现在会出现一种情况，都得死。他没懂，在这个图里你们看得懂吗？\n一是，看 2011—2015 年周期里，2008 年 4 万亿基建，加杠杆，把有效需求扩的非常好，那时候政策一出，绝对管用。现在很多官员犯的错误是觉得老百姓还是曾经的老百姓，还用同样的政策。当年的政策是我准备 50 万孝敬丈母娘准备买房子，结果你跟我说首付只需要 40 万，那你知道年轻人怎么做吗？40 万首付，10 万装修，还是花 50 万。现在的情况是什么吗？告诉他不需要 50 万，只需要 499，年轻人说我不缺那 10 万，我现在缺那 40 万。\n杠杆到头，消费是完全两个概念，用加杠杆的方式刺激经济，这个手段将失效，我现在唯一投票的全部是降杠杆刺激经济政策，比如说降低存量房贷利率，这是扎扎实实的降杠杆，说白了是银行吐出钱来给在座各位每个月可以少还 800、1000。但你还是说降低首付比例，大爷快来，加杠杆哦，我可以告诉你，加不动了，这就是核心。当年产能加上去以后供给过剩，主要是旧产能，出现 36 个月 PPI，企业恶性竞争，破产倒闭，银行压力巨大。我们干了供给侧改革，然后行政性出清一部分产能，其实是转移到新产能上去了，使得供需把需求再一刺激，老百姓买单。\n当年的大问题是这儿是一个妹子，白富美，下面是俩小帅哥，俩帅哥在那儿竞争，优胜劣汰，一个把一个淘汰之后，最终迎娶白富美，因为白富美需要一个帅哥，所以你们俩金正，胜者为王。这句话，充分的市场竞争后胜者为王，假设前提是需求不变，经济的这点活儿放到自媒体、网上真的搞坏了。充分的市场竞争后，胜者为王，赢者通吃的假设前提是需求不变，也就是妹子在，你俩竞争。\n知道现在的数字啥意思吗？红色线没了，0，PPI 如果扣掉疫情期间，持续从 2019 年之后为负，两个小伙子在那儿 PK，目标是胜者为王，最后卷完了，剩下一个，摇头一看，妹子呢？你们等着看吧，这件事情必然是两三年后某些行业必然发生的，会真以为是胜者为王？你的大环境是什么环境？是有效需求面临着中长周期的收缩和调整，这种情况下市场如此恶性竞争和卷是没有赢家的，最后会爆发危机的。我把这话送给某些企业的董事长们。\n跟往年不一样，往年任何过剩的市场竞争，最终都胜者通吃的原因是因为居民部门有效需求永远在，永远能加杠杆，永远能为你的企业产能和利润买单。最后只剩下一条路，你们知道是什么路吗？因为这条红色的线只代表着内需，如果国内的姑娘没了，就要迎娶海外白富美，所以他只剩下一条路，出海。这也是网上很多网友们很开心的，我们就是要出海，我们就是要拿下国际市场。\n现在国际环境是什么样？是不是 20 年前、30 年前全球往左翼包容融合的大环境，能让你出去迎娶白富美。当然日本的三井、三菱、丸红、伊藤忠等等，在 90 年之后就是出海迎娶白富美，问题是我们现在能不能？我相信大家心里都有杆秤，“想不想” 跟在未来的大环境上 “能不能” 将形成激烈的碰撞，如果不能，国内没有有效需求给你怎么办？\n现在所有经济问题是两个都存在，供给过剩也存在，有效需求不足也存在，我们需要解决问题，答案是必须提振内需，内需的核心就是进行再分配。政府和居民之间进行再分配，贫富之间进行再分配，债务和杠杆之间进行再分配，如果不做，那我们就是 35 年日本。日本 35 年周期怎么来的，你们最终知道答案了吧？还不知道，我都把儿子送过去了，你们还不懂再分配是什么意思吗？日本战争后获取所有资源要素（岗位、职务、薪资），战争后的第一代和第二代，到了 2012 年开始死亡，代际分配，老同志们死了，年轻人吃的蛋糕就自然多了，就这么简单了。\n你如果能理解到这个的变动，你就能自然地理解到我在说日本经济的核心到底是什么，是代际分配，不是增长，光增长不分配那就是富着恒富、穷着恒穷。这话翻译到股市上你们知道是什么吗？上市公司不分红没增长的话，答案永远是富着恒富、穷着恒穷。00 后指望着炒股来分 60 后的财产，你想多了，你还不如去萧山当个上门女婿来得更快一点，还不用努力，把自己倒腾的帅帅的，嫁进萧山豪门，财富代际再分配，躺赢，何必要天天炒股累死累活的，心里想着我能干掉 60 后，那都活成精了，你能干成他？换手跑的比你还快，一边喊着 “年轻人快上啊，人生唯一一次机会，此次不 all in 梭哈，未来就没机会了。” 他一边 all in 着，咱们一边换着手撤，让他们站在高高的山冈上。\n市场跟经济是一样的，创造增量的同时也要进行分配，不配没有任何意义。有些事儿都是本质一样的，这就是中国经济当前最大的问题。\n2006 年供给侧改革，我提醒大家一句，有几个错误的观点特别强调一下。为啥说错误的？这观点用我的话说一定咱们要知道，不一定要让老百姓知道。\n举个简单的例子，股票市场创造财富效应，用此来改变国运，这是不是外面经常听到的声音？你想啥呢？房子如果没有收入和租金的回报率和住的功能，那换句话说房子不创造自身价值的情况下，纯换手，依旧答案是富着恒富、穷着恒穷，能创造短期财富效应吗？能，就像 2015、2016 年我跟你们举过的例子，满仓 all in 梭哈，融资杠杆伞形都上，然后你发现股票一个涨停一个涨停，账户里全是钱，出门给老婆买了个包，财富效应。\n然后呢？跌停的时候你知道你后悔买啥吗？你后悔股票没卖掉，放心股票卖不掉的，因为开盘就跌停了。你真正后悔的是，我没给老婆买这包就对了。我经常劝他们，辛苦你给你老婆买了包，因为她至少还剩个包。你要当时没买这个包，你连这个包都不剩了。\n这种换手的游戏只有结果是富着恒富、穷着恒穷，而且后果会越来越差，中国就是典型的这个逻辑。不要把泡沫当成家庭财富去忽悠老百姓，这是扯淡的，背离收入的，背离企业增长和股息分红的，这种东西咱都心知肚明，就是为了来换手的，你把它当成家庭财富配置，会死人的，最后对经济会造成今年这种情况。\n换手的时候你看着消费很爽，比如说我这儿给你举的房子，中国的房子上涨幅度最大的真的不是 2008—2015 年，恰恰是 2015、2016 年股灾之后房价是最猛的，那时候北京我记得最清楚，2009 年炒房的时候，亚运村是 17500，2015 年年终的时候亚运村房价是 25000，2019 年亚运村的房价是 10 万，黑色线是同比上涨，70 个大中型城市同比上涨，同比上涨 + 长时间累计就是房价涨幅最大的时候，所以我们就是 2015、2016 年那一波，房价带来了大家的消费预期和希望，但从这张图上你们可以非常明显地看到消费背离了收入，这种消费就是大家讲的建立在财富效应上的，但此时的财富并不是收入支撑起来的。\n问题就来了，四年前五年前我拍过一些短视频，跟平台合作让我拍些短视频，我当时就讲得很清楚，房子如果纯换手，到底是什么东西？我 200 万买的房子，600 万卖给年轻人，我拿走的就是年轻人未来 40 年青春的当期现金折现。我可以为我的未来 40 年潇洒了，他背上这 40 年的债务，他要还的。如果没有收入的增长，他要硬硬地还 40 年，他就是失去的，我替他多活 40 年了，就这么简单。\n股票价格也一样，咱们交换的叫时间价值，我是大股东，我现在拿走，我现在 Happy，把你套在里面套 10 年，10 年后能不能解套呢？也许解套了，你觉得你开心吗？你丢了 10 年。金融资产交换成本你们一定要注意时间函数，没有时间函数都是扯淡。\n问题就在于，如果收入不增长，纯换，短期内创造的财富效应毕竟等于另外一批人短期内累积的债务。换句话说，我们爽了，他的债务达到一定程度的时候会造成全面的坍塌，整个资产不可能持续的，他买的房子 800 万买的时候，指望 800 万买，1000 万卖给下一个年轻人，结果发现没年轻人了，下一个年轻人接不动 1000 万了，此时他的资产负债表就开始恶化了，消费开始出现断崖式的回归，这就是 2019 年后的结果。2019 年后开始逐层断崖往下走，向着真实的收入回归。\n千万不敢跟老百姓讲把股票、房子当成家庭财富，过去有年轻人的时候，他才是我们的财富，不管通过股票还是通过房子，年轻人就是所有人财富的来源。说实话，中国的房地产涨了差不多 20 年的根本原因是啥？有一讲一，凭良心讲，任何的房子年轻人能买走，我们就拿走财富了，我的房子就是他们的负债，咱们所有人在吃的就一条东西，就是居民部门杠杆率，所有人吃的就是这根红色线。\n有些研究员说，中国现在居民部门杠杆率比海外低，因为他单纯的对比数字。我可以告诉你不低了，你知道原因是什么吗？我们的杠杆背后没有高福利，高福利国家 70%，低福利国家 60%，你跟我说 60% 比 70% 低，你自己实际的杠杆压力到底是多少，你心里没点数吗？你的教育、医疗、养老，上有老下有小，你需要花多少钱你心里没点数吗？你是社会福利 70 年代 80 年代建立起来国家的居民部门杠杆率吗？不是的。从各种现象上去观察，这个杠杆率到头了。\n资本市场很聪明，从 2002 年一直到现在，长达 18、20 年左右的时间，交易中国的消费升级，不管上证综指是 3000 点还是 3500，不重要，消费板块大周期就是中国居民部门 80 后加杠杆这一带。\n从 2019、2020 年开始，我对中国消费的所有教育逻辑就是：一是消费会发生结构性的变化，这种变化实际上是代际的变化；二是消费开始降级。\n那时候我演讲中讲的那句话，咖啡不再是喝完 20 喝 30，喝完 30 喝 40，喝完 40 喝 50，而是开始喝 9.9 买一赠一，年轻人开始周四攒个肯德基优惠券。\n年轻人开始发生消费型变化，比如我们家孩子喝茶，我看他们喝茶马上告诉我老婆，把囤的普洱的茶饼全甩卖。为啥？我们家姑娘怎么喝茶知道吗？去她办公室，红茶绿茶普洱，你说红茶，东方树叶拿出来拧开，连水都不用，倒壶里，卡哇伊的小杯子往那儿一放，往那儿一倒，请。她不会给你拿个饼搓半天，然后再一泡，那是上一代人。\n日本当年经济顶峰期的时候，做日本的清酒和威士忌，结果 1990 年之后真正火起来的是三得利、Jim Beam 嗨棒。年轻人说 1000 块钱的酒，啥酒？我要的就是 10 块钱，口味偏甜，RIO 喝起来微醺，能醉吗？能，行了。我的社交场景已经变了，我不会有请客吃饭坐在那儿了，我的消费场景已经变成俩孩子往那儿一坐，看电竞比赛直播。\n这种年代已经到了，你可千万不要以为没到，你囤的邮票、木头，你记住一点，没得传承的，还囤邮票，我家儿子都不知道邮票长啥样？他倒知道小马宝莉值钱，这就是时代的变化，消费在发生结构性变化及总量上的变化。\n中国的资本市场对经济的反应是完全准确的，你们可以看看整个的板块指数，房地产结束了，居民部门的食品、饮料、消费、零售结束了。新能源汽车到底结没结束，我的答案是你们等着瞧。其实现在只有一个板块在扩张，半导体。\n你们看懂啥意思了吗？银行的对联叫 “只做锦上添花，绝不雪中送炭”，横批 “我家银行”。你要是经营不善，我们家第一个干的事儿是抽贷。“内债不是债，只要税还在”，还有一个秘诀是关于中国特色的经济，海外没有的，我们是非市场经济下的 “J、Q、K”。\n“J” 是什么意思？大爷快来。“Q”，大爷，投点钱吧，把钱圈住。“K” 是出去，KO。中国能够高速经济发展实际上和这只手有密切关系，大家炒股票都知道，经济越差的时候，你炒的是这只手动不动，有人问中国的股市跟经济到底有没有关系，我可以告诉你，两头是反向关系，中间是正向关系。\n两头反向关系就是经济越差，这只手会出动，你会有反向关系，中间一定重新回归到跟经济相关，然后再往那头，过于亢奋了，也是行政关系。举个例子，2015、2016 年当时非常典型，我发了个微博，最后一个月，我说你们谁爱玩谁玩，我们要撤了，拜拜了您。当然好处我没做空中国，我做空了香港，我要是做空中国我就回不来了。\n那时候我跑到江浙沪调研当时的伞形信托场外融资配置，证监会一出政策，我说很简单，你们谁爱玩谁玩，老子撤了，根本原因这只手才是关键，你现在的市场什么 6000 点不是梦，10000 点刚起步，你哪一条支持了？债务杠杆再一撤，游戏就崩了，赶紧跑。你们不觉得这玩意要出事？那也是手，不是经济，中间这一段才是正常经济发展，而中国的指数编制将决定了大部分时间经济反映出来的是结构，不是增量。现在出来个 A500，是想试图让 A500 类似标普那样能反映总量加结构。\nJQK 什么意思呢？中国非常奇特，在投资的过程中，只做 JQ 环节，绝不做 K。JQ 环节是什么呢？行业有没有崛起呢？没有。行业有没有国家主义的意义呢？有。你们一定要记住一点，我们是右翼，右翼的产业政策全是偏向于国家主义的，所以说国家主义干任何事情不是要挣钱，要的是有，你们记住这句话。所以 JQ 的所有目的是为了政绩，是为了有。\n老股民都懂，看新闻联播炒股的逻辑是啥，你告诉我你看新闻联播是炒经济吗？你看新闻联播炒股就是炒的他哪儿没有他想要，你就投。这时候是没有证伪的，而他会倾注所有资源给你，倾注土地、税收、地方引导基金，倾注一切资源，股市也是资源，用来融资的，会把一切资源给你，你是最爽的。而你一旦做到了遥遥领先，他的政治目的达到以后，就会把你甩到市场上进行市场经济的 KO，此时你们就会发现 PPI 的秘密。你们就会发现 PPI 的周期性，PPI 的周期性很简单，你如果把 PPI 里的行业再分一下，生产资料、生活资料，再细拆就会发现周期性和政策的关系。\n啥东西呢？我没有，就会让 PPI 为正，我倾注一切资源和方式可以让你在里头挣到钱，而且不需要竞争。但我一旦达到目的，把你扔到市场经济的时候，你们会迅速发展，如果是正式的市场经济，竞争波动会比较温和的，也就是说稍微一有点钱挣，就有人进来了，就会烫平，周期拉的比较长，需要 30 年才能崛起一个大型企业。\n但在中国不是，中国是 5 年，产业链就要做到全球遥遥领先，在 JQ 的保护期内，大家都可以活，但同样会造成很多 “大家”，一旦保护期一到，扔到市场经济的时候，你们就会发现大家就变成了非自己人，开始 PK、竞争、内卷，PPI 开始转负，就这么简单。然后一轮产业过剩，开始淘汰，政府驱动再引导新的产业。用这种前浪推后浪的方式，推动整个产业各个环节的周期缩短到五年，但是代价就是很多行业会以很快的速度进入到 PPI 为负，而所有能到 PPI 为负的产业，最后能活下来都得感谢有自己的负债端，居民部门能买单，一旦内需不够还这么搞，就会出危机，就是现在这种状态。\n老百姓的投资是说你看新能源渗透率到了多少多少了，老百姓开的越来越多了，怎么股票一直跌呢？这就是他的错误，他没有理解政策到底什么时候投资，成熟的时候是不能投的，因为能放给市场的时候一定不那么挣钱了，不放给市场的时候一定是特别挣钱的。\n当然，这里面还牵扯到一点就是当年供给侧改革，你们可能都没有人会想到供给侧改革跟当年的股灾和楼市是有高度关系的。当年周兴涛（音）在市的时候，2015 年底 2016 年初在上海搞了个会议让我讲供给侧改革，我说供给侧改革很简单，1997 年朱镕基总理翻一下，供给侧改革这个词就来自于那儿。中国这一轮所有的起点是 2002 年，PPI 为正，核心 CPI 为正，有效需求为正，持续到了 2012 年，供给开始过剩，但有效需求可以继续加杠杆，在这儿就是供给矛盾，2009 年供需双落，这就是中国这一轮从 2012 年开始的大周期的末端。\n上一次末端是什么时间呢？改革开放一直到 90 年代末，2000 年初。出门京东上 150 块钱买《朱镕基总理答记者问》，三卷本，里面所有的事儿都发生过，房地产泡沫、金融经济危机，鼓励老百姓要有信心。你知道当年怎么鼓励老百姓有信心吗？“心若在，梦就在，大不了从头再来”，刘欢同志从此从那儿活起来的，那就是当年鼓励你们有信心。\n把年轻人的失业率不能拉那么高，你们知道当年怎么做的吗？大学本科扩招，因为我们的年轻失业率统计上是不统计在校生的。把你都赶到大学里，失业率就能往后延三年。\n今年清北附交本硕的比例是多少吗？1 比 3，5000 本科，15000 硕士，当然，当年那种政策最后直接结果造成的什么？曾经本科很值钱，然后本科不值钱。我大概率觉得以后硕士可能也不咋值钱了，如果三年后经济的问题还不基础，我估计开始鼓励你们读博了，读到 30 岁再开始就业吧。在座家里有孩子的，你孩子不是富二代的，就别卷了，富二代就更不用卷了。用我的话说，想清楚了，后面卷学历没什么太大价值了。\n当年还干过啥？银行风险，四大资产管理公司处理不良，供给侧改革，化债，股票和政府化债。如果你想知道股票市场到底用来干啥的，请品品当年。2002—2004 年，经济已经恢复了，A 股跌到 2004 年的原因是啥？都有，所以你们知道债务到底怎么化吗？答案非常简单，所有的债务记住上下联，“只要人还在，啥债都能化”。只要人不在，这债怎么化？税率。量收不上来，就抓率。以前有人说中国是高税率，是居民部门高税率，企业部门低税率，因为各种退税、补贴给你的是低税率的，大家要懂得，该抓税率的时候要抓税率，不然是不够的。这一段大周期到现在为止进入到末端，这就是当前最麻烦的点以及外围环境。\n海外我就不分析了，因为你倒过去就 OK 了，你把过去的 40 年倒过来就是海外正在慢慢发生变化，产业在回归，贸易关系在重塑。我这两天刚从新加坡回来，新加坡、东南亚、马来西亚、印度尼西亚、越南，开句玩笑话说，2016 年之后真的是受益于你俩人打架，因为他们在走正向反馈和循环，他的正向反馈循环就是我们转移的。\n最后送大家一句话，这张图是全球很重要的，当财政需要扩张，当利率在下降，你们可以想想财政花钱短期内挣不到，国内经济的有效需求不够，储蓄过剩，投资回报率下降，利率下降，在这种背景下，汇率就代表着你的实际回报率以及本币购买力，是减弱的。所以新兴市场一般来讲，如果出现这种状况一是利差会推着汇率贬值；二是政府的债务会推着汇率的贬值，会导致资本流出，会导致你需要加息去应对，但一加息，经济崩，资本进一步流出，这就是新兴市场危机。\n注意，新兴市场危机不单纯是美国加息，这是很多人错误的认知。我可以告诉你，中国以前从来不会崩，因为不管老美加到几，中国的投资回报率都远远高于老美的话，我不会有新兴市场危机的，本质上是对内投资回报率和债务。\n大家老是讲一句，老美加息就会收割别人。我开句玩笑话说，你如果没有借那么多钱，且投资回报率很强，他加息对你没影响。有一讲一，老讲成阴谋论，不是左就是右了，是偏颇的，我们站在中立的角度看，左有左的问题，右有右的问题，两边都有。\n全世界在低利率和政府财政扩张情况下能保证汇率稳定的只有欧美、日本，他采用的模式很简单，自己家里不能挣钱，又要保证自己的信用和币值的稳定，答案就是我可以作为资本方到海外挣钱，这也是全球化必然在当年会发生的路径，拿海外的 carry trading 套利资产作为背书，支撑汇率。\n我可不认为《广场协议》把日本打败了，你去日本访问，他们会告诉你日本当年的官员都是偏国家主义的右翼，因为从二战后过来的，他们在写自己所有回忆录的时候都说，我们日本发展的挺好，我们的政策没有错，都怪美帝国主义。你想想，哪个右翼会写本书说我错了，你们家老婆会说自己错了吗？想多了，怎么着都得说你错了。当年你们看日本官员书的时候，就会形成错误的右翼认知，原来是美帝国主义把日本打败的，而他不会去反思自己的问题。日本中立的这批人相反在做历史分析和回顾的时候，那个答案更准确。\n在那个背景下，《广场协议》以后日元大幅度升值，日本形成了低息日元，强势汇率，什么意思？到海外去，购买海外资产吧，日元 carry trading 套息，到海外去形成，这就是日本的三井、三菱、丸红、伊藤忠，他们承载了日本海外所有财富。而更关键的是日本政府债务用于了对那一代居民的补偿，教育、医疗、养老对你们的支撑。当然，没有增长了，所以他的内心是痛苦的，肉体不痛苦，所以日本是内心痛苦，肉体不痛苦，答案就是抑郁症、焦虑症、自杀森林，如果肉体痛苦，那就不是自杀，那是杀别人。\n在这种背景下，财政转向、利率下降，carry trading 套息，用海外资产兜住，形成汇率，这就是当年很著名的，日本只要国内有地震，全球资产就会马上出现抛售潮，日元套息交易会迅速支撑日本，重要的就是这部分资产会回流。\n美国也是一样，1980 年后，美元套息交易，所以全球就形成了低利率美元、借贷美元，永远借贷，美国不能加息，永远降息，美元永远是借贷方，不是投资方。这种背景下，用他的财政扩张和低利率，支撑汇率的就是美元的跨国资产，也就是美股里的所有上市公司和跨国企业。\n但是他造成的结果是，三井、三菱、丸红、伊藤忠富了，老百姓穷了，也不叫穷，就是不增长了。老美也是同样的道理，跨国企业高管富了，老百姓穷了，而老百姓最终会用选票红脖子投出特朗普进行逆转。逆转了游戏就颠倒了，低利率就会回归，套利会回归，产业会回归，债务会下降，汇率会走向，就这个逻辑。\n中国也想这么干，但做不到，我们现在的问题是利率低又不能低到那么低，财政想帮老百姓又不能真的帮到老百姓，汇率升值，我不知道当年有多少人你们咋想的升到 5 块 6 块，不是指 2016 年，是指这两年突然间有人说人民币将来升到 5 块 4 块，你靠啥升？你用什么升？现在说白了就是一种平衡，利率不能那么低，财政不能那么扩，汇率不能那么贬，三者之间找平衡点。比如说 2 的利率，对应的就是 7-7.3 的汇率，财政对应的就是能救救地方政府。如果明年打贸易战，变的更加严峻了，那利率可以更低点，比如破 2，汇率可以往上放一下，7.3-7.6，7.3-7.8，财政可以再扩一点，这是我们现在手上唯一剩下的牌，怎么可能一次给你打出去呢？咋打，打出去以后没有海外市场给你做背书，我们就是新兴市场了。这个游戏就是现在国内的逻辑，止和稳，没有刺激，靠啥刺激？你对国内所有资产理解透了，大概率也就是明年的一些东西了。\n说句实在话，全球这两年战争风险不断加大，某些资产计入的可不是利率、汇率、货币，某些资产在计入的是战争和脱钩，此处请参考俄罗斯。\n时间原因，留几分钟给大家提问。\n现场提问：付总我问一个问题，您刚才也在报告里提到了，这么大的人口国家，也到了 1 万美金以上的人均收入，结构性机会会在哪些行业或者哪些领域里有？\n付鹏：你就记住一点，你现在要么做富的，要么做穷的，放弃中产吧，这就是答案，做富的不受影响。比如说刚才的奢侈品，富的是啥？各位女士们，记住一点，只买爱马仕，香奈儿、LV、GUCCI、Prada 通通放弃，从新品到二手都会崩的，这叫富的。\n穷的怎么做？杭州已经开始了，香奈儿一个包一天租金 25，二奢已经都玩不转了，现在玩的都是租一天 25，干吗呢？租给名媛们拍拍照打打卡，这就是极致的两头。优衣库，要么就往上走，要么就是升级到始祖鸟，要么就往下降级到优衣库，不过现在年轻人更猛，他们都觉得优衣库贵，这都没辙了，我发现优衣库在国内的销售数据在下降。为啥呢？年轻人一问，优衣库挺贵，这没法去理解了，优衣库。就是往两头，中间的部分不做。\n第二，年轻人的生意做，老年人的生意做，40 多岁中年不做，因为他基本上是上有老下有小，还着债，苦哈哈的中年牛马，这部分放弃。年轻人做就是他的兴趣爱好会发生很大的变化，他们不一样，不管有钱没钱，他们的兴趣爱好完全不一样。比如动漫、游戏、二次元，就像年轻人买包，我们家姑娘买啥包？什么 LV、GUCCI、爱马仕统统不买，人家买 “痛包”，知道啥是痛包吗？知道里面塞一堆吧唧是啥东西吗？这就是他们那个时代的消费。\n老年人你们知道做什么吗？老年人记住一点，人生中最后一刻花钱，一定是最后一个房子，ICU 病房。中间 60、70 岁你也没啥好花的，我一直鼓励中国应该尽早退休，趁着这代人有财富的，能花的时候让他花，把岗位职务让出来给年轻人。你真学日本，老着站着，年轻的上不去，你会发现该花的没花还在工作，该挣的挣不着，时间就拖了，中国也一样，放弃中间。\n剩下的是什么呢？国家让你干哪些产业你就干，而且一定干早期，早期干完，后期一定过剩，干早期，达到顶峰的时候就是市场热度极高，要名有名，要利有利的时候，撤。\n现场提问：半导体到了吗？\n付鹏：没到，很简单，我们还没宣布我们 “遥遥领先”，着啥急。会不惜一切代价，哪怕 10 个公司中有 9 个是骗子，他都认，这没办法，你必须明白国家主义的特征。\n现场提问：感谢付总，想问关于海外资产配置的问题，个人在海外资产美股、美债、新兴市场的债券和股票，有什么建议？\n付鹏：新兴市场的债券就是中国，原则上你购买他。比如说你们配置型的会配新兴市场债券，我们投机倒把型的直接在新兴市场放高利贷，任何新兴市场早期第一件事情都进去放贷。2015 年去越南，我原来是做小米总代的，干着干着开始放贷，为什么？中国早些年也是放贷的，新兴市场有个特征，金融体系不完善的情况下，央行和银行的利率并不能正确地反映经济周期的资本供应和需求，所以说银行的利率一定低于实际经济的投资回报率。\n我们的 “搬运工” 直接搬到最高的利率上就可以了，放小贷违约率很低的，现在在中国敢放贷吗？\n举个例子，在座各位 2 分有没有人借钱给我，你们大概第一反应是 2 分，你要我本金的吧？但把时间倒回 10 年前，2 分，开发商问你借，你借不借？那时候你们担心违约吗？不担心，原因跟你放多少息没关系，是对经济的判断，我可以告诉你，新兴市场在这个维度上就是当年的中国。\n人家的员工 00 后就是 00 后，2015 年有一些产业转移跟我们去越南，我跟老板说的很清楚，你不要觉得越南劳动力成本低，不低的。他说这不是挺便宜的，我说加班是要给加班费的，4 点 59 是必须下班的，工会真会罢工的。中国劳动力成本低的原因是啥？牛马可以随便压榨，这才是劳动力低的根因。\n这两年又有些企业想往回迁的原因是啥？越南这两年年轻人动不动就罢工，罢工了人家工会就上，你不给加薪人家就不干的，老板拿着没辙的，中国老板很不适应，就想迁回来。我说你也别迁了，因为中国的 00 后们也很快崛起了，他们整顿职场已经开始了。\n这两天香港闹的很凶的是华为的 HR 招聘，一样的道理，这个时代到了。\n新兴市场的债券，本质上确实是高息的，这很好，美股美债就不用说了，美元资产的投资回报率你自然琢磨去，当然美股明年会有点特殊，因为今年创造奇迹了，今年是美股的奇迹年，你们可能都没注意过回报率、波动率、估值，高估值，低波动，高回报，这三种组合理论上不会同时出现的，但今年同时出现了，是绝对异常的。这就是前几年我跟你讲的，你们必须要明白人工智能已经开始了。前几年我说美股会从人工智能的集中到慢慢扩散，很多人说不可能，他对股市结构的理解不够深刻的。\n当然今年最辉煌的一仗就是英伟达闪崩之前我们明确告诉国内所有的公募基金，你们要注意英伟达场外杠杆，那天晚上崩了，崩完了之后，第二天晚上马上做了将近 8000 人的直播，跟各家公募基金说，这就是波动率的高点，因为经济没有问题，市场没有问题，其实就是过低的波动率带来很多人加了杠杆，把这帮人干掉就完了。\n当时新加坡有些朋友给我打电话说付总这怎么办？我说你今天晚上把保证金补上，明天就能活，补不上死的就是你。这跟英伟达、人工智能没任何关系，妥妥的就是爆杠杆，爆完了就完了，我们也爆完，去完杠杆，英伟达重新回到 3 万亿，现在是这种情况。但这种组合明年应该不会持续，高估值、低波动、高回报，不会。明年要么保持着高估值，保持着低波动，回报率就得下降，要么就是保持着高波动，回报率下降，高估值，要么就是直接杀估值，我目前看还看不到杀估值的路径，杀杠杆是有可能的，杀估值的可能性不大，产业的中期早期估值都是偏高的。到明年去看，美股确实不太一样。\n但我还是那句话，大部分时间你要琢磨琢磨百年美股为啥都是上涨的，是有原因的。对于老百姓来讲，美股不是因为它高的所以高了，是防止高波动就行了，你只要防止高波动，大部分时间就是让你定投了，这没啥选的。\n还有一点，记住，巴菲特是资产管理，不是大散户，老百姓一理解巴菲特持有那么多现金，是不是美股要崩了？现金是啥？是零波动率 4.5% 股息的股票，你去品品这句话就 OK 了，在资产组合中你就是评估资值、波动、回报率，当三者之间异常的时候，你的比重会调到 0 波动率的股息，4.5% 的股票上，这就是现金，千万别把巴菲特搞成大散户，说巴菲特买现金说因为美股要崩，做做短视频蒙蒙老百姓可以，咱们自己就别这么干了，这大概就是美股。\n主持人：感谢付鹏先生的精彩分享。稍后的自由交流时间，各位有任何咨询或问题，可以和您的私人财富规划师继续交流。活动的最后再占用大家两分钟的时间，麻烦大家扫描屏幕上方二维码，填写调查问卷，我们很期待能收到您对于本次活动的反馈，以便我们在以后的活动中更好的服务于您。\n","date":"1 December 2024","permalink":"/blog/2025-06-24-fupeng_trading_system/","section":"Blog","summary":"HSBC 速记 汇丰私人财富规划\n玺越世家 · 臻享沙龙 上海站\n（速记稿）\n时间：2024 年 11 月 24 日\n地点：上海浦东文华东方酒店 LG1 层东方厅\n主持人：女士们，先生们，各位尊敬的来宾，我是陈佳昊（音），我是汇丰私人财富规划上海分区总经理，我代表上海汇丰私人财富规划欢迎各位的莅临。\n今天有很多新朋友，也有很多老朋友，我在周五的时候问过后台同事报名报了多少了，他告诉我们已经快要接近 200 人了，但从今天的规模来看，我感觉好像今天的人数还要再超过一些。\n当然了，有一些是原先的老客户，也有很多是慕名而来，看到这次邀请的是付鹏先生，所以慕名而来。也有一些新朋友。在付鹏先生上台之前，请允许我对汇丰私人财富规划做简短的介绍。\n汇丰私人财富规划是全球的战略重点之一，老朋友都知道，汇丰私人财富规划成立于 2020 年，距今刚好四年，在四年的过程中集团一直在给我们大力注资，也是集团里最重要的项目之一。\n为什么聚焦在中国市场上？大家很多人都明白，中国中产阶级的人数在世界上占有量是最庞大的，随着中国经济的高速发展，中国人财富管理的需求逐步提升到很高的水准。所以，私人财富规划也会变成汇丰的重要战略之一。\n介绍一下发展历史，从 2020 年汇丰私人财富规划成立，先是在上海和广州，总部离这里不远，汇丰总部就在国金，欢迎大家去坐一坐。逐步进入到杭州、深圳、北京、佛山，今年在苏州、成都开立了分支机构。\n2020 年汇丰私人财富规划才刚刚成立，那汇丰的历史又是怎么样的？汇丰简称叫 HSBC，很多人会问 HSBC 四个字母分别代表着什么，可以跟大家简单介绍一下，H 代表的是香港的意思，S 代表的是上海的意思。很多人印象中以为汇丰是一家外资银行，但其实大家有所不知，其实汇丰在清朝的时候就在外滩已经设立了总部，现在这栋楼交给了浦发银行。1949 年之后，汇丰因为历史的原因退出了中国，在 WTO 之后回到了中国。\n汇丰 1865 年成立至今已经有 100 多年了，那时候还是清朝的同治年间，同时已经在全球的 62 个国家还有 3900 多名客户，这段历史和这么大的分布也是汇丰很多同事内心的骄傲。我们跟很多客户做沟通的时候，经常会把这段历史拿出来跟大家讲一讲，就像这头石狮子，很多人都见过，但很多人都不知道它的历史，很多人在海报、广告、港元大钞上看过这个石狮子，原来在外滩上也有两座，现在放在上海博物馆里，前一阵儿我在博物馆参观的时候还看到了这两只石狮子，上面还有很多历史的痕迹，比如说战争而留下的弹孔，就在人民广场的博物馆里，大家有兴趣的话可以去看一下。\n财富大矩阵与中国内地市场，汇丰集团对于中国私人财富规划业务的重视程度，在大矩阵中承担了很重要的地位。\n每 100 位客户中，会有 87 位客户将汇丰私人财富规划视作为提供财富重要的主要品牌，提出了很多好评，82% 的调研者打出 9-10 分的高分。\n也有一些比较有意思的话，如：“对产品内容的保障满意，公司大有保障；甄汇生活有一定的吸引力，汇丰的产品较贵但也愿意买，因为对汇丰私人财富规划师的认可。”\n这两年提出一句比较新的 Slogan“懂你关心的，给你安心的”。\n今天的活动我们邀请到了一位重量级嘉宾，他曾任职于雷曼兄弟、所罗门投资集团等全球顶尖金融机构，从事对冲基金等相关工作。他就是东北证券首席经济学家付鹏先生。让我们欢迎付鹏先生为我们带来《2024 年年终回顾和 2025 年展望——对冲风险 VS 软着陆》主题分享，有请付鹏先生！\n付鹏：正值年底，虽然刚才汇丰一直强调大家不录音不录像，但大概率你挡不住。我在这儿讲话会谨慎一些，非常小心谨慎，大概率会有人透露出去，放到 YouTube 上，基本上所有见我都说付总我在 YouTube 上看过你的视频，我说那都是盗版的，靠盗版发财的也不少。","title":"付鹏HSBC"},{"content":"OrderBook 本地维护方案设计 #一、业务背景 #OrderBook（订单簿）是反映市场深度和流动性的核心数据结构，其维护质量直接影响：\n策略交易决策的准确性 风险控制的有效性 市场定价的及时性 1.1 业务价值 # 价格发现\n实时反映市场供需状态 提供多层次价格信息 展示市场深度分布 交易决策支持\n最优价格确定（NBBO） 流动性评估 交易成本估算 风险管理\n市场异常监控 流动性风险评估 价格波动追踪 二、技术方案 #2.1 核心数据结构 #class LockFreeOrderBook { private: // 基础信息 std::string symbol_; // 状态管理 std::atomic\u0026lt;uint64_t\u0026gt; last_update_time_{0}; std::atomic\u0026lt;uint64_t\u0026gt; last_sequence_{0}; std::atomic\u0026lt;bool\u0026gt; initialized_{false}; // 价格档位存储 using PriceLevelMap = tbb::concurrent_map\u0026lt;double, PriceLevel, std::greater\u0026lt;\u0026gt;\u0026gt;; PriceLevelMap bids_; // 买盘 - 降序 PriceLevelMap asks_; // 卖盘 - 升序 }; // 价格档位结构 struct PriceLevel { double price; double quantity; uint64_t update_time; }; // 深度数据结构 struct DepthData { std::vector\u0026lt;PriceLevel\u0026gt; bids; std::vector\u0026lt;PriceLevel\u0026gt; asks; uint64_t sequence_num; uint64_t timestamp; }; 2.2 核心功能实现 # 快照数据处理 void LockFreeOrderBook::onSnapshot(const QuoteData::L2MarketData\u0026amp; data) { // 序列号检查 if (initialized_ \u0026amp;\u0026amp; data.sequence_num \u0026lt;= last_sequence_) return; // 重建订单簿 bids_.clear(); asks_.clear(); // 批量构建价格档位 for (const auto\u0026amp; event : data.events) { for (const auto\u0026amp; update : event.updates) { if (update.new_quantity \u0026lt;= 0) continue; PriceLevel level(update.price_level, update.new_quantity, data.timestamp); if (update.side == \u0026#34;bid\u0026#34;) { bids_.emplace(update.price_level, level); } else { asks_.emplace(update.price_level, level); } } } // 更新状态 updateState(data); } 增量更新处理 void LockFreeOrderBook::onUpdate(const QuoteData::L2MarketData\u0026amp; data) { // 状态检查 if (!initialized_ || data.sequence_num \u0026lt;= last_sequence_) return; // 处理价格档位更新 for (const auto\u0026amp; event : data.events) { for (const auto\u0026amp; update : event.updates) { PriceLevel level(update.price_level, update.new_quantity, data.timestamp); auto\u0026amp; book = (update.side == \u0026#34;bid\u0026#34;) ? bids_ : asks_; auto [it, inserted] = book.emplace(update.price_level, level); if (!inserted) { it-\u0026gt;second = level; // 更新现有档位 } } } // 更新状态 updateState(data); } 深度数据查询 DepthData LockFreeOrderBook::getDepth(size_t levels) const { DepthData result; result.sequence_num = last_sequence_; result.timestamp = last_update_time_; // 收集有效价格档位 for (const auto\u0026amp; [price, level] : bids_) { if (level.quantity \u0026gt; 0 \u0026amp;\u0026amp; result.bids.size() \u0026lt; levels) { result.bids.push_back(level); } } for (const auto\u0026amp; [price, level] : asks_) { if (level.quantity \u0026gt; 0 \u0026amp;\u0026amp; result.asks.size() \u0026lt; levels) { result.asks.push_back(level); } } return result; } 2.3 技术特点 # 并发安全\n使用 TBB concurrent_map 保证数据一致性 原子操作保证状态更新的安全性 无锁设计减少竞争 性能优化\n最小化锁竞争 高效的数据结构选择 批量处理能力 可靠性保证\n序列号机制确保数据完整性 异常处理机制 状态一致性维护 三、应用场景 #3.1 策略应用 # 做市策略 void MarketMakingStrategy::onOrderBookUpdate() { auto depth = orderbook_-\u0026gt;getDepth(5); // 计算买卖价差 double spread = depth.asks[0].price - depth.bids[0].price; // 评估市场状态 if (isValidSpread(spread)) { updateQuotes(depth); } } 套利策略 void ArbitrageStrategy::checkOpportunity() { auto depth1 = orderbook1_-\u0026gt;getDepth(1); auto depth2 = orderbook2_-\u0026gt;getDepth(1); double spread = calculateSpread(depth1, depth2); if (spread \u0026gt; threshold_) { executeArbitrage(); } } 3.2 风险控制 #void RiskManager::monitorMarket() { auto depth = orderbook_-\u0026gt;getDepth(10); // 检查市场质量 checkMarketQuality(depth); // 监控价格波动 monitorPriceMovement(depth); // 评估流动性 assessLiquidity(depth); } 四、性能指标 # 延迟要求\n更新处理延迟 \u0026lt; 100微秒 查询响应延迟 \u0026lt; 50微秒 批量处理能力 \u0026gt; 10000次/秒 资源消耗\n内存占用 \u0026lt; 1GB/交易对 CPU使用率 \u0026lt; 30% 网络带宽 \u0026lt; 100Mbps 五、后续优化方向 # 性能优化\n引入内存池管理 实现定期清理机制 优化数据结构布局 功能扩展\n添加统计分析功能 实现历史数据回放 支持多市场整合 监控完善\n延迟监控 内存使用监控 异常事件告警 通过这套方案，我们可以高效地维护本地订单簿，为上层策略提供准确、及时的市场数据支持，同时保证系统的可靠性和可扩展性。\n","date":"27 November 2024","permalink":"/blog/2025-06-24-orderbook_implementation/","section":"Blog","summary":"OrderBook 本地维护方案设计 #一、业务背景 #OrderBook（订单簿）是反映市场深度和流动性的核心数据结构，其维护质量直接影响：\n策略交易决策的准确性 风险控制的有效性 市场定价的及时性 1.1 业务价值 # 价格发现\n实时反映市场供需状态 提供多层次价格信息 展示市场深度分布 交易决策支持\n最优价格确定（NBBO） 流动性评估 交易成本估算 风险管理\n市场异常监控 流动性风险评估 价格波动追踪 二、技术方案 #2.1 核心数据结构 #class LockFreeOrderBook { private: // 基础信息 std::string symbol_; // 状态管理 std::atomic\u0026lt;uint64_t\u0026gt; last_update_time_{0}; std::atomic\u0026lt;uint64_t\u0026gt; last_sequence_{0}; std::atomic\u0026lt;bool\u0026gt; initialized_{false}; // 价格档位存储 using PriceLevelMap = tbb::concurrent_map\u0026lt;double, PriceLevel, std::greater\u0026lt;\u0026gt;\u0026gt;; PriceLevelMap bids_; // 买盘 - 降序 PriceLevelMap asks_; // 卖盘 - 升序 }; // 价格档位结构 struct PriceLevel { double price; double quantity; uint64_t update_time; }; // 深度数据结构 struct DepthData { std::vector\u0026lt;PriceLevel\u0026gt; bids; std::vector\u0026lt;PriceLevel\u0026gt; asks; uint64_t sequence_num; uint64_t timestamp; }; 2.","title":"OrderBook 本地维护方案设计"},{"content":"","date":null,"permalink":"/tags/memeoy/","section":"Tags","summary":"","title":"Memeoy"},{"content":"1. 概述 #内存映射（mmap）是一种将文件或设备映射到内存的方法，而零拷贝是一种减少或避免数据在内核空间和用户空间之间不必要复制的技术。这两个概念密切相关，但又有所不同。\n2. mmap 是零拷贝吗？ #答案是：mmap 本身不是零拷贝技术，但它可以实现零拷贝的效果。\n2.1 mmap 的工作原理 # 当调用 mmap 时，操作系统会在虚拟内存中创建一个新的内存区域。 这个内存区域会映射到文件系统缓存（page cache）中的物理页面。 当程序访问这个内存区域时，如果相应的页面不在内存中，会触发缺页中断，操作系统会从磁盘加载数据到内存。 2.2 为什么 mmap 可以实现零拷贝 # 一旦映射建立，用户进程可以直接读写这个内存区域，而无需在用户空间和内核空间之间进行数据复制。 对于读操作，数据从磁盘读入 page cache 后，可以直接被用户进程访问，无需额外复制。 对于写操作，修改直接发生在 page cache 上，操作系统会在适当的时候将修改同步到磁盘。 3. mmap 与传统 I/O 的比较 #3.1 传统 read 系统调用 #char buffer[4096]; ssize_t bytes_read = read(fd, buffer, sizeof(buffer)); 这个过程涉及两次数据拷贝：\n从磁盘到内核缓冲区 从内核缓冲区到用户空间缓冲区 3.2 使用 mmap #void* addr = mmap(NULL, file_size, PROT_READ, MAP_PRIVATE, fd, 0); // 直接访问 addr 指向的内存 mmap 减少了一次数据拷贝，数据直接从磁盘到用户可访问的内存。\n4. mmap 的优势和注意事项 #4.1 优势 # 减少数据拷贝，提高I/O效率 支持随机访问大文件 可以实现进程间通信 4.2 注意事项 # 大文件映射可能导致地址空间碎片 写操作可能触发写时复制（Copy-on-Write），影响性能 需要谨慎处理文件大小变化的情况 5. 真正的零拷贝技术 #zero copy\n虽然 mmap 可以减少拷贝，但真正的零拷贝技术通常指的是：\nsendfile() 系统调用：直接在内核空间完成文件到网络套接字的数据传输。 支持 scatter-gather 的 DMA 传输：允许硬件直接在磁盘和网络接口之间传输数据，完全绕过 CPU。 6. 示例：使用 mmap 实现高效文件复制 ##include \u0026lt;fcntl.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;sys/mman.h\u0026gt; #include \u0026lt;sys/stat.h\u0026gt; #include \u0026lt;iostream\u0026gt; void copy_file(const char* src, const char* dst) { int src_fd = open(src, O_RDONLY); if (src_fd == -1) { std::cerr \u0026lt;\u0026lt; \u0026#34;Error opening source file\u0026#34; \u0026lt;\u0026lt; std::endl; return; } struct stat sb; if (fstat(src_fd, \u0026amp;sb) == -1) { std::cerr \u0026lt;\u0026lt; \u0026#34;Error getting file size\u0026#34; \u0026lt;\u0026lt; std::endl; close(src_fd); return; } void* src_addr = mmap(NULL, sb.st_size, PROT_READ, MAP_PRIVATE, src_fd, 0); if (src_addr == MAP_FAILED) { std::cerr \u0026lt;\u0026lt; \u0026#34;Error mapping source file\u0026#34; \u0026lt;\u0026lt; std::endl; close(src_fd); return; } int dst_fd = open(dst, O_RDWR | O_CREAT | O_TRUNC, 0644); if (dst_fd == -1) { std::cerr \u0026lt;\u0026lt; \u0026#34;Error creating destination file\u0026#34; \u0026lt;\u0026lt; std::endl; munmap(src_addr, sb.st_size); close(src_fd); return; } if (ftruncate(dst_fd, sb.st_size) == -1) { std::cerr \u0026lt;\u0026lt; \u0026#34;Error setting file size\u0026#34; \u0026lt;\u0026lt; std::endl; close(dst_fd); munmap(src_addr, sb.st_size); close(src_fd); return; } void* dst_addr = mmap(NULL, sb.st_size, PROT_WRITE, MAP_SHARED, dst_fd, 0); if (dst_addr == MAP_FAILED) { std::cerr \u0026lt;\u0026lt; \u0026#34;Error mapping destination file\u0026#34; \u0026lt;\u0026lt; std::endl; close(dst_fd); munmap(src_addr, sb.st_size); close(src_fd); return; } memcpy(dst_addr, src_addr, sb.st_size); munmap(dst_addr, sb.st_size); munmap(src_addr, sb.st_size); close(dst_fd); close(src_fd); } int main() { copy_file(\u0026#34;source.txt\u0026#34;, \u0026#34;destination.txt\u0026#34;); return 0; } 这个例子展示了如何使用 mmap 高效地复制文件，避免了传统 read/write 方法中的多次数据拷贝。\n7. 结论 #虽然 mmap 不是严格意义上的零拷贝技术，但它确实能显著减少数据拷贝次数，提高 I/O 效率。在处理大文件或需要频繁随机访问的场景中，mmap 可以成为非常有效的工具。然而，在使用 mmap 时，开发者需要权衡其优势和潜在的复杂性，以确保在特定应用场景中获得最佳性能。\n","date":"22 October 2024","permalink":"/blog/2025-06-24-zero_copy_optimization/","section":"Blog","summary":"1. 概述 #内存映射（mmap）是一种将文件或设备映射到内存的方法，而零拷贝是一种减少或避免数据在内核空间和用户空间之间不必要复制的技术。这两个概念密切相关，但又有所不同。\n2. mmap 是零拷贝吗？ #答案是：mmap 本身不是零拷贝技术，但它可以实现零拷贝的效果。\n2.1 mmap 的工作原理 # 当调用 mmap 时，操作系统会在虚拟内存中创建一个新的内存区域。 这个内存区域会映射到文件系统缓存（page cache）中的物理页面。 当程序访问这个内存区域时，如果相应的页面不在内存中，会触发缺页中断，操作系统会从磁盘加载数据到内存。 2.2 为什么 mmap 可以实现零拷贝 # 一旦映射建立，用户进程可以直接读写这个内存区域，而无需在用户空间和内核空间之间进行数据复制。 对于读操作，数据从磁盘读入 page cache 后，可以直接被用户进程访问，无需额外复制。 对于写操作，修改直接发生在 page cache 上，操作系统会在适当的时候将修改同步到磁盘。 3. mmap 与传统 I/O 的比较 #3.1 传统 read 系统调用 #char buffer[4096]; ssize_t bytes_read = read(fd, buffer, sizeof(buffer)); 这个过程涉及两次数据拷贝：\n从磁盘到内核缓冲区 从内核缓冲区到用户空间缓冲区 3.2 使用 mmap #void* addr = mmap(NULL, file_size, PROT_READ, MAP_PRIVATE, fd, 0); // 直接访问 addr 指向的内存 mmap 减少了一次数据拷贝，数据直接从磁盘到用户可访问的内存。","title":"内存映射（mmap）与零拷贝技术：深入理解和实践"},{"content":"目录 # 简介 仓库结构和分支策略 协作者权限管理 保护主分支 Pull Request 和代码审查流程 持续集成与部署 (CI/CD) 文档和沟通 最佳实践和注意事项 简介 #在没有高级 GitHub 功能的私有仓库中进行协同开发可能具有挑战性，但通过正确的实践和工具，我们可以建立一个高效、安全的开发环境。本指南总结了我们讨论的主要策略和技术。\n仓库结构和分支策略 # 主分支：main（稳定、可部署的代码） 开发分支：main_for_dev（日常开发工作） 特性分支：从 main_for_dev 分出，用于开发新功能 工作流程：\n从 main_for_dev 创建特性分支 在特性分支上开发 完成后，创建 Pull Request 到 main_for_dev 代码审查和测试 合并到 main_for_dev 定期将 main_for_dev 合并到 main 协作者权限管理 #GitHub 私有仓库提供以下权限级别：\nRead Triage Write Maintain Admin 设置步骤：\n进入仓库 \u0026ldquo;Settings\u0026rdquo; \u0026gt; \u0026ldquo;Collaborators and teams\u0026rdquo; 点击 \u0026ldquo;Add people\u0026rdquo; 或 \u0026ldquo;Add teams\u0026rdquo; 输入用户名并选择适当的权限级别 最佳实践：\n遵循最小权限原则 定期审查和更新权限 保护主分支 #由于缺乏高级分支保护功能，我们采用以下策略：\n团队约定：\n禁止直接推送到 main 分支 所有更改通过 PR 进行 Git Hooks： 创建 pre-push hook（.git/hooks/pre-push）：\n#!/bin/sh branch=$(git rev-parse --abbrev-ref HEAD) if [ \u0026#34;$branch\u0026#34; = \u0026#34;main\u0026#34; ]; then echo \u0026#34;Direct push to main branch is not allowed. Please create a Pull Request.\u0026#34; exit 1 fi 设置权限：chmod +x .git/hooks/pre-push\nGitHub Actions： 创建 .github/workflows/protect-main.yml：\nname: Protect Main Branch on: push: branches: - main jobs: check_push: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - name: Check if push was direct run: | if [[ $(git log --format=%B -n 1 ${{ github.sha }}) != *\u0026#34;Merge pull request\u0026#34;* ]]; then echo \u0026#34;::error::Direct push to main branch detected. Please use Pull Requests.\u0026#34; exit 1 fi Pull Request 和代码审查流程 # 创建 PR 模板： 在 .github/pull_request_template.md 中定义模板。\n审查流程：\n至少一名审查者批准 通过所有自动化测试 遵循团队定义的代码规范 合并策略： 使用 \u0026ldquo;Squash and merge\u0026rdquo; 或 \u0026ldquo;Rebase and merge\u0026rdquo; 保持清晰的提交历史。\n持续集成与部署 (CI/CD) #使用 GitHub Actions 进行 CI/CD：\n在 PR 中运行测试和代码质量检查 只从 main 分支进行部署 自动化版本标记和发布流程 文档和沟通 # README.md：项目概述和快速开始指南 CONTRIBUTING.md：详细的贡献指南 代码注释：保持代码自文档化 定期团队会议：讨论项目进展和问题 最佳实践和注意事项 # 定期培训团队成员，确保everyone遵循协作流程 使用 GitHub Issues 进行任务跟踪和 bug 报告 考虑使用项目看板（Project Boards）进行任务管理 定期审查和更新工作流程，适应团队需求 鼓励知识共享和对等编程 重视代码质量，包括单元测试和文档 考虑实施持续反馈机制，不断改进协作流程 通过实施这些策略和最佳实践，即使在私有仓库的限制下，也能建立一个高效、安全的协作环境。记住，成功的协作不仅依赖于工具和流程，更依赖于团队的沟通和相互信任。\n","date":"16 October 2024","permalink":"/blog/2025-06-24-project_management_best_practices/","section":"Blog","summary":"目录 # 简介 仓库结构和分支策略 协作者权限管理 保护主分支 Pull Request 和代码审查流程 持续集成与部署 (CI/CD) 文档和沟通 最佳实践和注意事项 简介 #在没有高级 GitHub 功能的私有仓库中进行协同开发可能具有挑战性，但通过正确的实践和工具，我们可以建立一个高效、安全的开发环境。本指南总结了我们讨论的主要策略和技术。\n仓库结构和分支策略 # 主分支：main（稳定、可部署的代码） 开发分支：main_for_dev（日常开发工作） 特性分支：从 main_for_dev 分出，用于开发新功能 工作流程：\n从 main_for_dev 创建特性分支 在特性分支上开发 完成后，创建 Pull Request 到 main_for_dev 代码审查和测试 合并到 main_for_dev 定期将 main_for_dev 合并到 main 协作者权限管理 #GitHub 私有仓库提供以下权限级别：\nRead Triage Write Maintain Admin 设置步骤：\n进入仓库 \u0026ldquo;Settings\u0026rdquo; \u0026gt; \u0026ldquo;Collaborators and teams\u0026rdquo; 点击 \u0026ldquo;Add people\u0026rdquo; 或 \u0026ldquo;Add teams\u0026rdquo; 输入用户名并选择适当的权限级别 最佳实践：\n遵循最小权限原则 定期审查和更新权限 保护主分支 #由于缺乏高级分支保护功能，我们采用以下策略：\n团队约定：\n禁止直接推送到 main 分支 所有更改通过 PR 进行 Git Hooks： 创建 pre-push hook（.","title":"GitHub私有仓库协同开发指南"},{"content":"","date":null,"permalink":"/tags/project-management/","section":"Tags","summary":"","title":"project management"},{"content":"1. 引言 #Fork是Unix/Linux系统中最基本也是最强大的系统调用之一。它允许一个进程创建一个新的进程,这个新进程是原进程的一个几乎完全相同的副本。本次技术分享将深入探讨fork机制,从基本概念到高级应用。\n2. Fork的基本原理 #2.1 什么是Fork #Fork是一个系统调用,用于创建一个新的进程。新进程（称为子进程）是调用进程（称为父进程）的一个几乎完全相同的副本。\n2.2 Fork的工作原理 #当一个进程调用fork时:\n系统会创建一个新的进程。 新进程是父进程的一个副本,包括代码段、数据段、堆栈等。 子进程获得父进程数据空间、堆和栈的副本。 父进程和子进程继续执行fork调用之后的代码。 2.3 Fork的返回值 #Fork调用会返回两次:\n在父进程中,返回子进程的PID。 在子进程中,返回0。 这允许程序区分父进程和子进程。\npid_t pid = fork(); if (pid \u0026gt; 0) { printf(\u0026#34;父进程\\n\u0026#34;); } else if (pid == 0) { printf(\u0026#34;子进程\\n\u0026#34;); } else { perror(\u0026#34;fork失败\u0026#34;); exit(1); } 3. Fork的高级特性 #3.1 写时复制 (Copy-on-Write) #为了提高效率,现代操作系统使用\u0026quot;写时复制\u0026quot;技术:\n初始时,子进程与父进程共享同一物理内存。 只有当其中一个进程尝试修改内存时,才会创建该部分内存的副本。 这大大减少了fork的开销和内存使用。\n3.2 文件描述符的继承 #子进程继承父进程的文件描述符。这意味着:\n子进程可以访问父进程打开的文件。 父子进程共享文件偏移量。 int fd = open(\u0026#34;example.txt\u0026#34;, O_RDWR); if (fork() == 0) { // 子进程 write(fd, \u0026#34;Hello from child\u0026#34;, 16); } else { // 父进程 write(fd, \u0026#34;Hello from parent\u0026#34;, 17); } 3.3 内存独立性 #虽然子进程初始时与父进程共享内存,但它们的内存空间是独立的:\n一个进程对变量的修改不会影响另一个进程。 这适用于全局变量、堆、栈等所有内存区域。 4. Fork的高级应用 #4.1 多进程并行处理 #Fork常用于创建多个并行工作的进程,例如在服务器程序中:\nfor (int i = 0; i \u0026lt; NUM_WORKERS; i++) { if (fork() == 0) { worker_process(); exit(0); } } 4.2 实现管道 #Fork结合管道可以用于进程间通信:\nint pipefd[2]; pipe(pipefd); if (fork() == 0) { close(pipefd[1]); // 关闭写端 char buf[100]; read(pipefd[0], buf, 100); printf(\u0026#34;子进程读取: %s\\n\u0026#34;, buf); } else { close(pipefd[0]); // 关闭读端 write(pipefd[1], \u0026#34;Hello from parent\u0026#34;, 17); } 4.3 实现Shell命令 #Shell使用fork和exec来执行命令:\nif (fork() == 0) { execl(\u0026#34;/bin/ls\u0026#34;, \u0026#34;ls\u0026#34;, \u0026#34;-l\u0026#34;, NULL); exit(1); // 如果exec失败 } 5. Fork的性能考虑 #5.1 资源消耗 #每次fork都会创建一个新进程,这涉及:\n内存分配 复制进程信息 更新系统表 在资源受限的环境中,过度使用fork可能导致性能问题。\n5.2 上下文切换 #多个进程意味着更多的上下文切换,可能影响性能。在某些情况下,使用线程可能更为高效。\n6. Fork的高级技巧和注意事项 #6.1 信号处理 #Fork后,子进程继承父进程的信号处理程序。但在多线程程序中fork需要特别小心,因为子进程只包含调用fork的线程。\n6.2 清理资源 #在使用fork时,要注意适当地关闭不需要的文件描述符和释放资源,以防止资源泄漏。\n6.3 竞态条件 #需要注意父子进程之间可能的竞态条件,特别是在访问共享资源时。\n7. 实例分析：多重Fork #让我们回到最初的例子:\nvoid test(){ fork() \u0026amp;\u0026amp; fork() \u0026amp;\u0026amp; fork() \u0026amp;\u0026amp; sleep(10); printf(\u0026#34;hello\\n\u0026#34;); exit(0); } 这个例子展示了fork的几个关键特性:\n短路评估: 利用\u0026amp;\u0026amp;操作符的短路特性控制fork的执行。 进程创建: 每个成功的fork都创建一个新进程。 并发执行: 多个进程并发运行,导致多个\u0026quot;hello\u0026quot;输出。 7.1 Fork调用分析 #代码解析 #这段代码的关键在于 fork() \u0026amp;\u0026amp; fork() \u0026amp;\u0026amp; fork() \u0026amp;\u0026amp; sleep(10) 这一行。\nfork() 的行为 # fork() 创建一个新的子进程。 在父进程中，fork() 返回子进程的 PID（非零值）。 在子进程中，fork() 返回 0。 逻辑短路 #由于使用了 \u0026amp;\u0026amp;（逻辑与）操作符，这里涉及到短路评估：\n只有当前面的 fork() 返回非零值（在父进程中）时，后续的 fork() 才会执行。 如果任何 fork() 返回 0（在子进程中），后续的 fork() 和 sleep(10) 都不会执行。 执行流程 # 第一个 fork()：\n创建一个子进程 父进程继续执行下一个 fork() 子进程跳过后续 fork() 和 sleep()，直接打印 \u0026ldquo;hello\u0026rdquo; 第二个 fork()（只在父进程中执行）：\n再创建一个子进程 新的父进程继续执行第三个 fork() 新的子进程跳过后续 fork() 和 sleep()，打印 \u0026ldquo;hello\u0026rdquo; 第三个 fork()（只在最初的父进程中执行）：\n再创建一个子进程 最初的父进程执行 sleep(10) 新的子进程跳过 sleep()，打印 \u0026ldquo;hello\u0026rdquo; 10秒后，最初的父进程也会打印 \u0026ldquo;hello\u0026rdquo;\n进程树 #原始进程 ─── 子进程1 (打印\u0026#34;hello\u0026#34;) │ ├─── 子进程2 (打印\u0026#34;hello\u0026#34;) │ └─── 子进程3 (打印\u0026#34;hello\u0026#34;) │ └─── 父进程 (等待10秒后打印\u0026#34;hello\u0026#34;) 结果 #这段代码会输出 4 个 \u0026ldquo;hello\u0026rdquo;。\n3 个来自立即执行的子进程 1 个来自等待 10 秒后的父进程 8. 结论 #Fork是一个强大而复杂的系统调用,它为Unix/Linux系统提供了创建新进程的基本机制。理解和正确使用fork可以帮助开发者创建高效、可靠的多进程应用。然而,fork也带来了一些挑战,如资源管理和同步问题。在实际应用中,需要根据具体需求权衡使用fork、线程或其他并发机制。\n","date":"15 October 2024","permalink":"/blog/2025-06-24-fork_system_call_analysis/","section":"Blog","summary":"1. 引言 #Fork是Unix/Linux系统中最基本也是最强大的系统调用之一。它允许一个进程创建一个新的进程,这个新进程是原进程的一个几乎完全相同的副本。本次技术分享将深入探讨fork机制,从基本概念到高级应用。\n2. Fork的基本原理 #2.1 什么是Fork #Fork是一个系统调用,用于创建一个新的进程。新进程（称为子进程）是调用进程（称为父进程）的一个几乎完全相同的副本。\n2.2 Fork的工作原理 #当一个进程调用fork时:\n系统会创建一个新的进程。 新进程是父进程的一个副本,包括代码段、数据段、堆栈等。 子进程获得父进程数据空间、堆和栈的副本。 父进程和子进程继续执行fork调用之后的代码。 2.3 Fork的返回值 #Fork调用会返回两次:\n在父进程中,返回子进程的PID。 在子进程中,返回0。 这允许程序区分父进程和子进程。\npid_t pid = fork(); if (pid \u0026gt; 0) { printf(\u0026#34;父进程\\n\u0026#34;); } else if (pid == 0) { printf(\u0026#34;子进程\\n\u0026#34;); } else { perror(\u0026#34;fork失败\u0026#34;); exit(1); } 3. Fork的高级特性 #3.1 写时复制 (Copy-on-Write) #为了提高效率,现代操作系统使用\u0026quot;写时复制\u0026quot;技术:\n初始时,子进程与父进程共享同一物理内存。 只有当其中一个进程尝试修改内存时,才会创建该部分内存的副本。 这大大减少了fork的开销和内存使用。\n3.2 文件描述符的继承 #子进程继承父进程的文件描述符。这意味着:\n子进程可以访问父进程打开的文件。 父子进程共享文件偏移量。 int fd = open(\u0026#34;example.txt\u0026#34;, O_RDWR); if (fork() == 0) { // 子进程 write(fd, \u0026#34;Hello from child\u0026#34;, 16); } else { // 父进程 write(fd, \u0026#34;Hello from parent\u0026#34;, 17); } 3.","title":"Fork机制详解：从基础到高级应用"},{"content":"","date":null,"permalink":"/tags/system-programming/","section":"Tags","summary":"","title":"System Programming"},{"content":"","date":null,"permalink":"/tags/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/","section":"Tags","summary":"","title":"性能优化"},{"content":"1. 基础概念 #1.1 二进制表示 # 计算机使用二进制（0和1）存储和处理数据 1 byte = 8 bits 32位整数可以表示从 0 到 2^32 - 1 的数值 1.2 位操作基础 # 与操作 (\u0026amp;): 两位都为1时结果为1，否则为0 或操作 (|): 至少一位为1时结果为1，否则为0 异或操作 (^): 两位不同时结果为1，相同时为0 非操作 (~): 将每一位取反 左移 (\u0026laquo;): 将所有位向左移动，右侧补0 右移 (\u0026raquo;): 将所有位向右移动，左侧补0或符号位 示例：\nunsigned int a = 5; // 0101 unsigned int b = 3; // 0011 unsigned int and_result = a \u0026amp; b; // 0001 (1) unsigned int or_result = a | b; // 0111 (7) unsigned int xor_result = a ^ b; // 0110 (6) unsigned int not_result = ~a; // 11111111111111111111111111111010 (-6 in 2\u0026#39;s complement) unsigned int left_shift = a \u0026lt;\u0026lt; 1; // 1010 (10) unsigned int right_shift = a \u0026gt;\u0026gt; 1;// 0010 (2) 2. 掩码（Mask） #2.1 掩码定义 #掩码是用于选择或修改特定位的二进制模式\n2.2 常见掩码操作 # 提取位：value \u0026amp; mask 设置位：value | mask 清除位：value \u0026amp; ~mask 切换位：value ^ mask 示例：\nunsigned int value = 0xA5; // 10100101 unsigned int mask = 0x0F; // 00001111 unsigned int extract = value \u0026amp; mask; // 00000101 (5) unsigned int set = value | mask; // 10101111 (175) unsigned int clear = value \u0026amp; ~mask; // 10100000 (160) unsigned int toggle = value ^ mask; // 10101010 (170) 3. 位域（Bit Fields） #3.1 概念 #将较大的数据类型分割成多个小的字段，每个字段占用特定数量的位\n3.2 优势 # 内存效率：在一个整数中存储多个值 性能：位操作通常比其他操作更快 原子性：可以在一个操作中读取或修改多个字段 3.3 位域布局示例 #32-bit integer layout: [Instrument (29 bits)][Offset (2 bits)][Direction (1 bit)] 31 3 1 0 4. 实现技术 #4.1 定义掩码和偏移 ##define DIRECTION_BITS_MASK 0x1 #define DIRECTION_BITS_OFFSET 0x0 #define OFFSET_BITS_MASK 0x3 #define OFFSET_BITS_OFFSET 0x1 #define INSTRUMENT_BITS_MASK 0x1FFFFFFF #define INSTRUMENT_BITS_OFFSET 0x3 4.2 获取字段值 #int get_Field(int\u0026amp; value, int mask, int offset) { return (value \u0026gt;\u0026gt; offset) \u0026amp; mask; } // 具体实现示例 int get_Direction(int\u0026amp; value) { return (value \u0026gt;\u0026gt; DIRECTION_BITS_OFFSET) \u0026amp; DIRECTION_BITS_MASK; } 4.3 设置字段值 #根据字段的位置和大小，设置函数可能有不同的实现：\n// 对于最低位的单位字段（如 Direction） int set_Direction(int\u0026amp; value, int new_direction) { if (new_direction != 1 \u0026amp;\u0026amp; new_direction != 0) { return -1; } value = (value \u0026amp; ~DIRECTION_BITS_MASK) | new_direction; return 0; } // 对于非最低位的多位字段（如 Offset） int set_Offset(int\u0026amp; value, int new_offset) { if (new_offset \u0026lt; 3 \u0026amp;\u0026amp; new_offset \u0026gt; 0) { value = (value \u0026amp; ~(OFFSET_BITS_MASK \u0026lt;\u0026lt; OFFSET_BITS_OFFSET)) | (new_offset \u0026lt;\u0026lt; OFFSET_BITS_OFFSET); return 0; } return -1; } 注意 set_Direction 和 set_Offset 的区别：\nset_Direction 直接使用掩码，因为它操作的是最低位 set_Offset 需要将掩码和新值左移，因为它操作的位不在最低位置 4.4 通用设置函数 #int set_Field(int\u0026amp; value, int new_field_value, int mask, int offset) { value = (value \u0026amp; ~(mask \u0026lt;\u0026lt; offset)) | (new_field_value \u0026lt;\u0026lt; offset); return 0; } 5. 在高频交易（HFT）系统中的应用 #高频交易系统对性能和延迟极其敏感，位操作在这里发挥着关键作用。\n5.1 订单编码 #在HFT系统中，订单信息需要快速处理和传输。使用位域可以将订单的多个属性打包到一个整数中：\n#define ORDER_TYPE_MASK 0x03 #define SIDE_MASK 0x04 #define QUANTITY_MASK 0xFFFFF8 #define PRICE_MASK 0xFFF00000 #define ORDER_TYPE_OFFSET 0 #define SIDE_OFFSET 2 #define QUANTITY_OFFSET 3 #define PRICE_OFFSET 20 typedef unsigned int OrderInfo; OrderInfo createOrder(unsigned char type, bool isBuy, unsigned int quantity, unsigned int price) { return (type \u0026amp; ORDER_TYPE_MASK) | ((isBuy ? 1 : 0) \u0026lt;\u0026lt; SIDE_OFFSET) | ((quantity \u0026amp; (QUANTITY_MASK \u0026gt;\u0026gt; QUANTITY_OFFSET)) \u0026lt;\u0026lt; QUANTITY_OFFSET) | ((price \u0026amp; (PRICE_MASK \u0026gt;\u0026gt; PRICE_OFFSET)) \u0026lt;\u0026lt; PRICE_OFFSET); } unsigned char getOrderType(OrderInfo order) { return order \u0026amp; ORDER_TYPE_MASK; } bool isBuyOrder(OrderInfo order) { return (order \u0026amp; SIDE_MASK) != 0; } unsigned int getQuantity(OrderInfo order) { return (order \u0026amp; QUANTITY_MASK) \u0026gt;\u0026gt; QUANTITY_OFFSET; } unsigned int getPrice(OrderInfo order) { return (order \u0026amp; PRICE_MASK) \u0026gt;\u0026gt; PRICE_OFFSET; } 5.2 市场数据压缩 #HFT系统需要处理大量的市场数据。使用位操作可以压缩数据，减少网络传输和存储需求：\nstruct CompressedQuote { unsigned long long timestamp : 48; // 微秒级时间戳 unsigned int symbol : 24; // 股票代码 unsigned int bidPrice : 32; // 买入价 unsigned int askPrice : 32; // 卖出价 unsigned int bidSize : 24; // 买入量 unsigned int askSize : 24; // 卖出量 unsigned int flags : 8; // 各种标志 }; 5.3 快速比较和匹配 #位操作可用于实现快速的订单匹配和比较：\nbool isMatchingOrder(OrderInfo order1, OrderInfo order2) { return (getOrderType(order1) == getOrderType(order2)) \u0026amp;\u0026amp; (isBuyOrder(order1) != isBuyOrder(order2)) \u0026amp;\u0026amp; ((isBuyOrder(order1) \u0026amp;\u0026amp; getPrice(order1) \u0026gt;= getPrice(order2)) || (!isBuyOrder(order1) \u0026amp;\u0026amp; getPrice(order1) \u0026lt;= getPrice(order2))); } 5.4 风险管理和合规检查 #位操作可以用于快速执行风险检查和合规验证：\n#define RISK_CHECK_MASK 0xF0000000 bool passesRiskCheck(OrderInfo order) { return (order \u0026amp; RISK_CHECK_MASK) == 0; } 5.5 性能优化 # 缓存友好：紧凑的数据表示有助于更好地利用CPU缓存。\nSIMD操作：某些位操作可以利用SIMD（单指令多数据）指令进行并行处理。\n// 使用SIMD指令并行处理多个订单 void processOrdersSIMD(OrderInfo* orders, int count) { // 使用 AVX2 指令集 __m256i orderVector = _mm256_loadu_si256((__m256i*)orders); __m256i typeMask = _mm256_set1_epi32(ORDER_TYPE_MASK); __m256i types = _mm256_and_si256(orderVector, typeMask); // 进一步处理... } 网络优化：压缩的数据格式减少了网络传输量，降低延迟。\n5.6 HFT系统中的注意事项 # 可读性 vs 性能：在HFT系统中，通常会牺牲一定的可读性来换取极致的性能。 正确性验证：由于位操作容易出错，需要严格的单元测试和集成测试。 文档和注释：详细的文档和注释对于维护这类高度优化的代码至关重要。 硬件考虑：某些位操作可能在特定硬件上更高效，需要针对目标平台优化。 6. 结论 #位操作和位域是强大的编程技术，在需要高性能和内存效率的场景中尤其有用。在高频交易系统中，这些技术能够显著提升数据处理速度、减少内存使用和网络延迟。然而，使用这些技术需要在性能、可读性和可维护性之间取得平衡。随着金融技术的不断发展，掌握和巧妙运用这些基础但强大的技术将继续在高性能计算领域，特别是在HFT系统中发挥重要作用。\n","date":"13 October 2024","permalink":"/blog/2025-06-24-bit_field_compression_techniques/","section":"Blog","summary":"1. 基础概念 #1.1 二进制表示 # 计算机使用二进制（0和1）存储和处理数据 1 byte = 8 bits 32位整数可以表示从 0 到 2^32 - 1 的数值 1.2 位操作基础 # 与操作 (\u0026amp;): 两位都为1时结果为1，否则为0 或操作 (|): 至少一位为1时结果为1，否则为0 异或操作 (^): 两位不同时结果为1，相同时为0 非操作 (~): 将每一位取反 左移 (\u0026laquo;): 将所有位向左移动，右侧补0 右移 (\u0026raquo;): 将所有位向右移动，左侧补0或符号位 示例：\nunsigned int a = 5; // 0101 unsigned int b = 3; // 0011 unsigned int and_result = a \u0026amp; b; // 0001 (1) unsigned int or_result = a | b; // 0111 (7) unsigned int xor_result = a ^ b; // 0110 (6) unsigned int not_result = ~a; // 11111111111111111111111111111010 (-6 in 2\u0026#39;s complement) unsigned int left_shift = a \u0026lt;\u0026lt; 1; // 1010 (10) unsigned int right_shift = a \u0026gt;\u0026gt; 1;// 0010 (2) 2.","title":"高频交易系统中的位域压缩技术"},{"content":"1. 背景介绍 #在高频交易系统中，市场数据的快速读取和处理是关键性能指标之一。我们的系统使用共享内存来存储和访问实时市场数据，其中 MarketDataStore 类负责管理这些数据。本文将讨论如何优化 MarketDataStore 中的 readLatestData 函数，以提高数据读取的效率。\n2. 初始实现 #最初的 readLatestData 函数实现如下：\nstd::optional\u0026lt;MappedTickerData\u0026gt; MarketDataStore::readLatestData(const std::string\u0026amp; symbol) const { std::shared_lock\u0026lt;std::shared_mutex\u0026gt; lock(mutex); size_t offset = calculateOffset(symbol); MappedTickerData data; if (dataFile-\u0026gt;read(\u0026amp;data, offset, sizeof(MappedTickerData))) { if (data.timestamp != 0 \u0026amp;\u0026amp; std::string(data.product_id) == symbol) { return data; } else { LOG_WARN(\u0026#34;readLatestData symbol = {} failed\u0026#34;, symbol); return std::nullopt; } } else { LOG_ERROR(\u0026#34;Failed to read data for symbol = {}\u0026#34;, symbol); return std::nullopt; } } 这个实现存在几个性能瓶颈：\n使用共享锁可能导致并发读取的性能下降。 字符串比较效率低下，特别是创建临时 std::string 对象。 没有利用现代 CPU 的 SIMD 指令集。 3. 优化过程 #3.1 字符串比较优化 #首先，我们优化了字符串比较逻辑：\nstatic inline bool compareProductId(const char* product_id, const std::string\u0026amp; symbol) { size_t symbolLength = symbol.length(); if (symbolLength \u0026gt; sizeof(MappedTickerData::product_id) - 1) { return false; } if (memcmp(product_id, symbol.data(), symbolLength) != 0) { return false; } return product_id[symbolLength] == \u0026#39;\\0\u0026#39;; } 这个优化避免了创建临时字符串对象，并使用了更高效的 memcmp 函数。\n3.2 SIMD 指令优化 #为了进一步提高性能，我们引入了 SIMD 指令来并行化字符串比较：\nstatic inline bool compareProductIdSIMD(const char* product_id, const std::string\u0026amp; symbol) { size_t symbolLength = symbol.length(); if (symbolLength \u0026gt; 15) { return false; } __m128i prod_id = _mm_loadu_si128(reinterpret_cast\u0026lt;const __m128i*\u0026gt;(product_id)); char mask[16] = {0}; memcpy(mask, symbol.data(), symbolLength); __m128i symbol_mask = _mm_loadu_si128(reinterpret_cast\u0026lt;const __m128i*\u0026gt;(mask)); __m128i cmp_result = _mm_cmpeq_epi8(prod_id, symbol_mask); int match_mask = _mm_movemask_epi8(cmp_result); int should_match = (1 \u0026lt;\u0026lt; symbolLength) - 1; if ((match_mask \u0026amp; should_match) != should_match) { return false; } return (match_mask \u0026amp; (1 \u0026lt;\u0026lt; symbolLength)) != 0; } 这个实现利用 SSE 指令集同时比较 16 个字节，显著提高了比较速度。\n3.3 无锁读取 #考虑到 readLatestData 函数被频繁调用，我们探讨了使用无锁读取技术：\nstd::optional\u0026lt;MappedTickerData\u0026gt; MarketDataStore::readLatestData(const std::string\u0026amp; symbol) const { size_t offset = calculateOffset(symbol); MappedTickerData data; std::atomic_thread_fence(std::memory_order_acquire); memcpy(\u0026amp;data, static_cast\u0026lt;char*\u0026gt;(mappedMemory) + offset, sizeof(MappedTickerData)); std::atomic_thread_fence(std::memory_order_acquire); if (data.timestamp != 0 \u0026amp;\u0026amp; compareProductIdSIMD(data.product_id, symbol)) { return data; } return std::nullopt; } 这个版本移除了共享锁，使用内存屏障确保数据一致性。\n4. 最终优化版本 #综合以上优化，我们的最终版本如下：\nclass MarketDataStore { private: void* mappedMemory; size_t memorySize; std::unordered_map\u0026lt;std::string_view, size_t\u0026gt; symbolOffsets; static inline bool compareProductIdSIMD(const char* product_id, const std::string\u0026amp; symbol) { // SIMD 比较实现（如前所示） } public: inline std::optional\u0026lt;MappedTickerData\u0026gt; readLatestData(std::string_view symbol) const noexcept { auto it = symbolOffsets.find(symbol); if (it == symbolOffsets.end()) { return std::nullopt; } size_t offset = it-\u0026gt;second; if (offset + sizeof(MappedTickerData) \u0026gt; memorySize) { return std::nullopt; } MappedTickerData data; std::atomic_thread_fence(std::memory_order_acquire); memcpy(\u0026amp;data, static_cast\u0026lt;char*\u0026gt;(mappedMemory) + offset, sizeof(MappedTickerData)); std::atomic_thread_fence(std::memory_order_acquire); if (data.timestamp == 0) { return std::nullopt; } if (compareProductIdSIMD(data.product_id, std::string(symbol))) { return data; } return std::nullopt; } }; 5. 性能考虑和注意事项 # SIMD 指令：确保目标平台支持使用的 SIMD 指令集。 内存对齐：考虑将 MappedTickerData 结构体对齐到缓存线边界。 预计算偏移量：使用 symbolOffsets 哈希表预存储偏移量，避免重复计算。 无锁读取：在多线程环境中需要仔细考虑内存一致性问题。 字符串视图：使用 std::string_view 减少不必要的字符串拷贝。 6. 结论 #通过这一系列优化，我们显著提高了 MarketDataStore 的读取性能。主要改进包括：\n使用 SIMD 指令加速字符串比较 实现无锁读取减少线程竞争 优化内存访问模式提高缓存效率 这些优化对于高频交易系统的整体性能有重要影响。然而，在实际部署前，务必进行全面的基准测试和压力测试，以确保在实际工作负载下的性能提升。\n7. 未来工作 # 探索使用更高级的 SIMD 指令集（如 AVX-512）进一步优化。 实现自适应策略，根据数据特征动态选择最佳的比较方法。 考虑引入预取技术，进一步减少内存访问延迟。 持续监控和分析系统性能，识别新的优化机会。 ","date":"29 September 2024","permalink":"/blog/2025-06-24-efficient_reading_techniques/","section":"Blog","summary":"1. 背景介绍 #在高频交易系统中，市场数据的快速读取和处理是关键性能指标之一。我们的系统使用共享内存来存储和访问实时市场数据，其中 MarketDataStore 类负责管理这些数据。本文将讨论如何优化 MarketDataStore 中的 readLatestData 函数，以提高数据读取的效率。\n2. 初始实现 #最初的 readLatestData 函数实现如下：\nstd::optional\u0026lt;MappedTickerData\u0026gt; MarketDataStore::readLatestData(const std::string\u0026amp; symbol) const { std::shared_lock\u0026lt;std::shared_mutex\u0026gt; lock(mutex); size_t offset = calculateOffset(symbol); MappedTickerData data; if (dataFile-\u0026gt;read(\u0026amp;data, offset, sizeof(MappedTickerData))) { if (data.timestamp != 0 \u0026amp;\u0026amp; std::string(data.product_id) == symbol) { return data; } else { LOG_WARN(\u0026#34;readLatestData symbol = {} failed\u0026#34;, symbol); return std::nullopt; } } else { LOG_ERROR(\u0026#34;Failed to read data for symbol = {}\u0026#34;, symbol); return std::nullopt; } } 这个实现存在几个性能瓶颈：","title":"高频交易系统中的市场数据存储优化"},{"content":"高频交易系统中的重连机制最佳实践 #背景 #在高频交易系统中，网络连接的稳定性至关重要。然而，由于网络波动或其他原因，连接可能会中断。为了确保系统的连续性和可靠性，需要实现一个高效的重连机制。然而，频繁的重连检查和处理可能导致重复重连，影响系统性能。\n问题描述 #在现有实现中，主循环频繁检查 m_client-\u0026gt;needsReconnection()，如果需要重连，则调用 handleReconnect()。然而，由于主循环速度很快，可能在 resetReconnectionFlag() 生效前再次检查 needsReconnection()，导致重复调用 handleReconnect()。\n解决方案 #通过使用原子操作和双重检查机制，确保重连过程的原子性和一致性，避免重复重连。\n1. 定义连接状态管理 #使用原子变量来管理连接状态，确保线程安全。\nclass WebSocketClient { private: std::atomic\u0026lt;bool\u0026gt; isReconnecting{false}; std::atomic\u0026lt;bool\u0026gt; needsReconnection{false}; public: bool needsReconnection() const { return needsReconnection.load(std::memory_order_acquire); } bool tryInitiateReconnection() { bool expected = false; return isReconnecting.compare_exchange_strong(expected, true, std::memory_order_acq_rel); } void setNeedsReconnection(bool value) { needsReconnection.store(value, std::memory_order_release); } void resetReconnectionFlag() { needsReconnection.store(false, std::memory_order_release); isReconnecting.store(false, std::memory_order_release); } }; 2. 修改主循环 #在主循环中使用双重检查机制，确保重连过程的原子性。\nvoid StrategyAndTrading::run() { initializeConnection(); marketDataReader-\u0026gt;start(); positionManager-\u0026gt;updatePositionsThread(); m_commonLib-\u0026gt;getConfigManager().configWatcher(); while (running_) { if (m_client-\u0026gt;needsReconnection() \u0026amp;\u0026amp; m_client-\u0026gt;tryInitiateReconnection()) { handleReconnect(); } // 执行其他高频交易逻辑 std::this_thread::sleep_for(std::chrono::microseconds(100)); // 微秒级的睡眠 } } 3. 实现重连处理 #确保重连过程的原子性和一致性。\nvoid StrategyAndTrading::handleReconnect() { LOG_INFO(\u0026#34;Initiating reconnection process\u0026#34;); int retryCount = 0; const int MAX_RETRIES = 3; while (retryCount \u0026lt; MAX_RETRIES) { LOG_INFO(\u0026#34;retryCount: {} RECONNECTING\u0026#34;, retryCount); if (establishConnection(true)) { LOG_INFO(\u0026#34;Reconnection successful\u0026#34;); m_client-\u0026gt;resetReconnectionFlag(); return; } retryCount++; LOG_WARN(\u0026#34;Reconnection attempt {} failed, retrying...\u0026#34;, retryCount); std::this_thread::sleep_for(std::chrono::seconds(5 * retryCount)); } LOG_ERROR(\u0026#34;Reconnection failed after {} attempts\u0026#34;, MAX_RETRIES); m_client-\u0026gt;setNeedsReconnection(true); // 保持重连需求 m_client-\u0026gt;resetReconnectionFlag(); // 允许下一次重连尝试 } 设计理由 # 原子操作：使用 std::atomic 确保线程安全，避免数据竞争。 双重检查：通过 needsReconnection() 和 tryInitiateReconnection() 的组合，避免重复进入重连流程。 状态一致性：resetReconnectionFlag() 同时重置两个标志，确保状态一致。 性能优化：主循环中的睡眠时间可以调整到微秒级，保持高响应性。 简单直接：相比复杂的多线程或状态机方案，这个解决方案更加直接地解决了您描述的问题。 可扩展性：这个设计易于扩展，可以添加更多的连接状态和相应的处理逻辑。 错误恢复：如果重连失败，系统会保持重连需求，允许在下一个循环中再次尝试。 compare_exchange_strong 的使用 #用法 #compare_exchange_strong 是 C++ 标准库中 std::atomic 提供的一种原子操作，用于实现无锁编程。它的作用是比较并交换（Compare and Swap, CAS），确保在多线程环境下对变量的更新是原子的。\n函数签名 #bool compare_exchange_strong(T\u0026amp; expected, T desired, std::memory_order order = std::memory_order_seq_cst) noexcept; 参数 # expected：一个引用，表示预期的旧值。如果当前值与 expected 相等，则将其更新为 desired，否则将当前值写入 expected。 desired：要设置的新值。 order：内存序（memory order），控制内存操作的顺序。常用的有 std::memory_order_acquire、std::memory_order_release 和 std::memory_order_acq_rel。 查看更多内存序相关内容 返回值 # 如果当前值与 expected 相等，则返回 true，并将当前值更新为 desired。 如果当前值与 expected 不相等，则返回 false，并将当前值写入 expected。 在新方案中的作用 #在新方案中，compare_exchange_strong 用于确保只有一个线程可以成功启动重连过程，避免多个线程同时进入重连过程。\n代码示例 #bool tryInitiateReconnection() { bool expected = false; return isReconnecting.compare_exchange_strong(expected, true, std::memory_order_acq_rel); } 解释 # 初始化 expected：expected 被初始化为 false，表示预期的旧值是 false。 调用 compare_exchange_strong： 如果 isReconnecting 当前值等于 expected（即 false），则将 isReconnecting 更新为 true，并返回 true。 如果 isReconnecting 当前值不等于 expected（即已经有其他线程将其设置为 true），则将 isReconnecting 的当前值写入 expected，并返回 false。 内存序 # std::memory_order_acq_rel：确保在获取和释放内存时的顺序性，保证在重连过程中对内存的访问是有序的。 具体应用 #在主循环中，通过 tryInitiateReconnection 方法来检查并启动重连过程：\nvoid StrategyAndTrading::run() { while (running_) { if (m_client-\u0026gt;needsReconnection() \u0026amp;\u0026amp; m_client-\u0026gt;tryInitiateReconnection()) { handleReconnect(); } std::this_thread::sleep_for(std::chrono::microseconds(100)); // 微秒级的睡眠 } } 解释 # 检查 needsReconnection：首先检查是否需要重连。 尝试启动重连：如果需要重连，调用 tryInitiateReconnection。 如果 tryInitiateReconnection 返回 true，表示当前线程成功启动了重连过程。 如果 tryInitiateReconnection 返回 false，表示已经有其他线程在进行重连，当前线程不需要重复启动重连过程。 实施注意事项 # 确保线程安全：所有涉及连接状态的操作都应是线程安全的。 调整睡眠时间：根据系统需求调整主循环中的睡眠时间，在响应性和系统负载之间找到平衡。 添加日志和监控：适当的日志记录和监控有助于跟踪重连过程和系统状态。 扩展性：可以根据需要在 WebSocketClient 中实现更复杂的状态管理逻辑，如处理部分连接、认证失败等状态。 总结 #通过这个最佳实践，您可以有效管理高频交易系统中的重连过程，避免重复重连，同时保持系统的高性能和可靠性。这个设计方案不仅解决了当前的问题，还为未来的扩展和维护提供了良好的基础。\n","date":"27 September 2024","permalink":"/blog/2025-06-24-atomic_operations_reconnection_mechanism/","section":"Blog","summary":"高频交易系统中的重连机制最佳实践 #背景 #在高频交易系统中，网络连接的稳定性至关重要。然而，由于网络波动或其他原因，连接可能会中断。为了确保系统的连续性和可靠性，需要实现一个高效的重连机制。然而，频繁的重连检查和处理可能导致重复重连，影响系统性能。\n问题描述 #在现有实现中，主循环频繁检查 m_client-\u0026gt;needsReconnection()，如果需要重连，则调用 handleReconnect()。然而，由于主循环速度很快，可能在 resetReconnectionFlag() 生效前再次检查 needsReconnection()，导致重复调用 handleReconnect()。\n解决方案 #通过使用原子操作和双重检查机制，确保重连过程的原子性和一致性，避免重复重连。\n1. 定义连接状态管理 #使用原子变量来管理连接状态，确保线程安全。\nclass WebSocketClient { private: std::atomic\u0026lt;bool\u0026gt; isReconnecting{false}; std::atomic\u0026lt;bool\u0026gt; needsReconnection{false}; public: bool needsReconnection() const { return needsReconnection.load(std::memory_order_acquire); } bool tryInitiateReconnection() { bool expected = false; return isReconnecting.compare_exchange_strong(expected, true, std::memory_order_acq_rel); } void setNeedsReconnection(bool value) { needsReconnection.store(value, std::memory_order_release); } void resetReconnectionFlag() { needsReconnection.store(false, std::memory_order_release); isReconnecting.store(false, std::memory_order_release); } }; 2. 修改主循环 #在主循环中使用双重检查机制，确保重连过程的原子性。\nvoid StrategyAndTrading::run() { initializeConnection(); marketDataReader-\u0026gt;start(); positionManager-\u0026gt;updatePositionsThread(); m_commonLib-\u0026gt;getConfigManager().configWatcher(); while (running_) { if (m_client-\u0026gt;needsReconnection() \u0026amp;\u0026amp; m_client-\u0026gt;tryInitiateReconnection()) { handleReconnect(); } // 执行其他高频交易逻辑 std::this_thread::sleep_for(std::chrono::microseconds(100)); // 微秒级的睡眠 } } 3.","title":"高频交易系统中的重连机制最佳实践"},{"content":"1. 初始问题：数据读取效率 #最初，我们关注的是市场数据读取器本身的效率问题。\n1.1 轮询方式（初始状态） #void MarketDataReader::readingLoop() { while (running) { for (const auto\u0026amp; symbol : symbols_) { processSymbol(symbol); } std::this_thread::sleep_for(std::chrono::milliseconds(100)); } } 问题：持续轮询即使在没有新数据时也会消耗资源。\n1.2 条件控制方式 #void MarketDataReader::readingLoop() { while (running) { std::unique_lock\u0026lt;std::mutex\u0026gt; lock(conditionMutex); dataCondition.wait(lock, [this] { return !running || !symbols_.empty(); }); for (const auto\u0026amp; symbol : symbols_) { processSymbol(symbol); } } } 改进：减少了不必要的CPU使用，但可能会在高频数据更新时引入延迟。\n思考转变：这个阶段，我们主要关注如何提高单个组件（数据读取器）的效率。\n2. 扩展考虑：数据读取对其他系统组件的影响 #随着对系统的深入思考，我们开始考虑数据读取器的行为如何影响整个系统，特别是订单流的执行效率。\n2.1 资源竞争问题 #观察：尽管我们优化了数据读取器的效率，但数据读取线程占据太多的计算资源，也会进而影响订单处理的性能。即使在没有新数据可读时，频繁的检查也会占用宝贵的计算资源。\n思考：\n数据读取和订单处理是否在竞争同样的系统资源（CPU、内存、I/O）？ 如何在保证数据及时性的同时，不影响订单处理的响应速度？ 如何协调各个线程，使系统达到最低的时延？ 2.2 自适应间隔机制 #引入动态调整处理间隔的机制，以平衡数据读取和系统资源使用。\nvoid MarketDataReader::readingLoop() { while (running) { auto start = std::chrono::steady_clock::now(); for (const auto\u0026amp; symbol : symbols_) { processSymbol(symbol); } auto end = std::chrono::steady_clock::now(); auto duration = std::chrono::duration_cast\u0026lt;std::chrono::microseconds\u0026gt;(end - start); if (duration \u0026lt; currentInterval) { std::this_thread::sleep_for(currentInterval - duration); } adjustInterval(); } } 思考转变：从单纯的效率优化转向了资源使用的平衡，考虑到了系统的整体性能。\n3. 系统级优化：负载均衡 #随着对系统整体的思考，我们意识到需要从更高的层面来优化性能和资源分配。\n3.1 多线程数据读取 #将数据读取任务分散到多个线程，以提高并行处理能力。\nclass BalancedMarketDataReader { private: std::vector\u0026lt;std::thread\u0026gt; readerThreads; std::vector\u0026lt;std::vector\u0026lt;std::string\u0026gt;\u0026gt; symbolGroups; public: void start() { for (int i = 0; i \u0026lt; numThreads; ++i) { readerThreads.emplace_back(\u0026amp;BalancedMarketDataReader::readingLoop, this, i); } } }; 思考：如何最有效地分配交易品种给不同的线程，以平衡负载？\n3.2 动态负载均衡 #实现能够根据实时负载情况动态调整工作分配的机制。\nclass DynamicLoadBalancer { private: std::vector\u0026lt;std::atomic\u0026lt;int\u0026gt;\u0026gt; threadLoads; std::mutex symbolsMutex; std::vector\u0026lt;std::string\u0026gt; symbols; public: void balancerLoop() { while (running) { rebalanceLoad(); std::this_thread::sleep_for(std::chrono::seconds(10)); } } }; 思考：如何在数据读取和订单处理之间动态分配系统资源，以实现最佳的整体性能？\n3.3 工作窃取算法 #引入更复杂的负载均衡策略，允许空闲线程从繁忙线程\u0026quot;窃取\u0026quot;工作。\nclass WorkStealingBalancer { private: std::vector\u0026lt;std::unique_ptr\u0026lt;WorkStealingQueue\u0026gt;\u0026gt; queues; bool stealWork(int threadId) { for (size_t i = 0; i \u0026lt; queues.size(); ++i) { if (i == threadId) continue; std::string symbol; if (queues[i]-\u0026gt;steal(symbol)) { processSymbol(symbol); queues[threadId]-\u0026gt;push(symbol); return true; } } return false; } }; 思考转变：从单一组件的优化，发展到了整个系统的资源分配和负载均衡策略。\n思考过程的演进 # 局部到全局：从优化单一数据读取器的效率，扩展到考虑整个系统的性能平衡。 单线程到多线程：认识到多线程处理在提高系统整体吞吐量方面的重要性。 静态分配到动态平衡：从固定的处理策略，转向能够适应实时负载变化的动态系统。 资源使用的权衡：深入思考如何在关键组件（如数据读取和订单处理）之间合理分配资源。 性能指标的全面性：从仅关注数据读取的速度，扩展到考虑系统整体的响应时间、吞吐量和资源利用率。 跨组件影响的认识：理解到一个组件的优化可能会对其他组件产生意料之外的影响，需要从整体角度进行评估。 结论 #这个思考探究过程展示了如何从解决具体问题逐步扩展到系统层面的优化。它强调了在高频交易这样的复杂系统中，局部优化虽然重要，但必须放在整体系统性能和资源平衡的大背景下来考虑。\n这种思维方式的转变不仅适用于市场数据读取器的优化，也可以应用于其他复杂系统的性能优化过程。它提醒我们，在进行任何优化时，都需要考虑：\n这个优化如何影响系统的其他部分？ 我们是否在正确的层面上解决问题？ 局部的高效是否会导致全局的低效？ 如何设计一个能够适应变化和自我调节的系统？ 通过这样的思考过程，我们不仅解决了最初的数据读取效率问题，还提出了更全面、更有弹性的系统优化方案，为构建一个高性能、高可靠性的高频交易系统奠定了基础。\n","date":"25 September 2024","permalink":"/blog/2025-06-24-datareader_design_patterns/","section":"Blog","summary":"1. 初始问题：数据读取效率 #最初，我们关注的是市场数据读取器本身的效率问题。\n1.1 轮询方式（初始状态） #void MarketDataReader::readingLoop() { while (running) { for (const auto\u0026amp; symbol : symbols_) { processSymbol(symbol); } std::this_thread::sleep_for(std::chrono::milliseconds(100)); } } 问题：持续轮询即使在没有新数据时也会消耗资源。\n1.2 条件控制方式 #void MarketDataReader::readingLoop() { while (running) { std::unique_lock\u0026lt;std::mutex\u0026gt; lock(conditionMutex); dataCondition.wait(lock, [this] { return !running || !symbols_.empty(); }); for (const auto\u0026amp; symbol : symbols_) { processSymbol(symbol); } } } 改进：减少了不必要的CPU使用，但可能会在高频数据更新时引入延迟。\n思考转变：这个阶段，我们主要关注如何提高单个组件（数据读取器）的效率。\n2. 扩展考虑：数据读取对其他系统组件的影响 #随着对系统的深入思考，我们开始考虑数据读取器的行为如何影响整个系统，特别是订单流的执行效率。\n2.1 资源竞争问题 #观察：尽管我们优化了数据读取器的效率，但数据读取线程占据太多的计算资源，也会进而影响订单处理的性能。即使在没有新数据可读时，频繁的检查也会占用宝贵的计算资源。\n思考：\n数据读取和订单处理是否在竞争同样的系统资源（CPU、内存、I/O）？ 如何在保证数据及时性的同时，不影响订单处理的响应速度？ 如何协调各个线程，使系统达到最低的时延？ 2.2 自适应间隔机制 #引入动态调整处理间隔的机制，以平衡数据读取和系统资源使用。\nvoid MarketDataReader::readingLoop() { while (running) { auto start = std::chrono::steady_clock::now(); for (const auto\u0026amp; symbol : symbols_) { processSymbol(symbol); } auto end = std::chrono::steady_clock::now(); auto duration = std::chrono::duration_cast\u0026lt;std::chrono::microseconds\u0026gt;(end - start); if (duration \u0026lt; currentInterval) { std::this_thread::sleep_for(currentInterval - duration); } adjustInterval(); } } 思考转变：从单纯的效率优化转向了资源使用的平衡，考虑到了系统的整体性能。","title":"高频交易系统优化：从数据读取到系统平衡的思考过程"},{"content":"高性能低延迟交易系统设计：技术分享 update #在高频交易和实时金融系统中，性能和延迟是关键因素。本文将分享一些设计和实现高性能低延迟交易系统的关键技术和策略。\n1. 数据结构优化 #1.1 内存映射（Memory-Mapped）文件 #使用内存映射文件可以显著提高I/O性能，减少系统调用，并允许快速的进程间通信。\nclass MmapOrderBook { // 使用内存映射文件存储订单簿数据 }; 1.2 自定义内存池 #实现自定义内存池可以减少内存分配和释放的开销，提高内存使用效率。\ntemplate\u0026lt;typename T, size_t MaxSize\u0026gt; class MemoryPool { // 实现高效的内存分配和回收 }; 2. 并发控制 #2.1 细粒度锁 #使用细粒度锁可以减少锁竞争，提高并发性能。\nstd::array\u0026lt;std::shared_mutex, MAX_POSITIONS\u0026gt; m_positionMutexes; 2.2 无锁数据结构 #在关键路径上使用无锁数据结构可以进一步减少同步开销。\nstd::atomic\u0026lt;double\u0026gt; quantity; std::atomic\u0026lt;double\u0026gt; averagePrice; 3. 高效的更新策略 #3.1 增量更新 vs 全量更新 #根据具体场景选择合适的更新策略。增量更新适合频繁的小幅度变化，全量更新适合大幅度变化或定期同步。\nvoid updatePosition(const char* instId, AssetType type, PositionSide side, double quantityDelta, double price); void syncPositionWithExchange(const char* instId, AssetType type, PositionSide side, double quantity, double price); 3.2 原子操作 #使用原子操作可以在不使用锁的情况下实现线程安全的更新。\natomicUpdate(positionPtr-\u0026gt;averagePrice, [newQuantity, quantityDelta, price](double oldAvgPrice) { return (oldAvgPrice * (newQuantity - quantityDelta) + price * quantityDelta) / newQuantity; }); 4. 代码优化 #4.1 内联函数 #使用内联函数可以减少函数调用开销。\ninline void updateAvailable(double delta) { available.fetch_add(delta, std::memory_order_relaxed); } 4.2 分支预测优化 #减少难以预测的分支，利用现代CPU的分支预测功能。\n// 避免复杂的嵌套条件判断 if (type == AssetType::SPOT) { // SPOT 逻辑 } else { // 其他类型逻辑 } 5. 系统架构 #5.1 职责分离 #将不同功能模块分离，如将订单管理和持仓管理分开，可以提高系统的可维护性和可扩展性。\nclass OrderManager { /* ... */ }; class PositionManager { /* ... */ }; 5.2 最小化跨模块调用 #减少模块间的频繁调用，可以降低系统复杂度和延迟。\n6. 性能监控和日志 #6.1 高效日志 #使用异步日志和日志级别控制，确保日志不会成为性能瓶颈。\nLOG_INFO(\u0026#34;Position updated: instId={}, type={}, side={}\u0026#34;, instId, static_cast\u0026lt;int\u0026gt;(type), static_cast\u0026lt;int\u0026gt;(side)); 6.2 性能指标监控 #实时监控关键性能指标，如更新延迟、吞吐量等，以便及时发现和解决性能问题。\n结论 #构建高性能低延迟的交易系统需要在多个层面进行优化，包括数据结构、并发控制、更新策略、代码优化和系统架构等。通过综合运用这些技术，可以显著提升系统的性能和响应速度，满足高频交易和实时金融系统的严格要求。\n","date":"20 September 2024","permalink":"/blog/2025-06-24-high_performance_computing_principles/","section":"Blog","summary":"高性能低延迟交易系统设计：技术分享 update #在高频交易和实时金融系统中，性能和延迟是关键因素。本文将分享一些设计和实现高性能低延迟交易系统的关键技术和策略。\n1. 数据结构优化 #1.1 内存映射（Memory-Mapped）文件 #使用内存映射文件可以显著提高I/O性能，减少系统调用，并允许快速的进程间通信。\nclass MmapOrderBook { // 使用内存映射文件存储订单簿数据 }; 1.2 自定义内存池 #实现自定义内存池可以减少内存分配和释放的开销，提高内存使用效率。\ntemplate\u0026lt;typename T, size_t MaxSize\u0026gt; class MemoryPool { // 实现高效的内存分配和回收 }; 2. 并发控制 #2.1 细粒度锁 #使用细粒度锁可以减少锁竞争，提高并发性能。\nstd::array\u0026lt;std::shared_mutex, MAX_POSITIONS\u0026gt; m_positionMutexes; 2.2 无锁数据结构 #在关键路径上使用无锁数据结构可以进一步减少同步开销。\nstd::atomic\u0026lt;double\u0026gt; quantity; std::atomic\u0026lt;double\u0026gt; averagePrice; 3. 高效的更新策略 #3.1 增量更新 vs 全量更新 #根据具体场景选择合适的更新策略。增量更新适合频繁的小幅度变化，全量更新适合大幅度变化或定期同步。\nvoid updatePosition(const char* instId, AssetType type, PositionSide side, double quantityDelta, double price); void syncPositionWithExchange(const char* instId, AssetType type, PositionSide side, double quantity, double price); 3.","title":"实现高性能低延迟的交易系统设计"},{"content":"在高频交易系统的开发中，我们经常面临着性能和正确性之间的权衡。最近，我们在优化订单处理流程时，发现了一个有趣的问题：是否需要在高层组件中实现锁定？本文将深入探讨这个问题，分析其必要性，并展示优化前后的实现。\n背景 我们的系统主要由以下组件构成：\nMmapOrderBook：核心数据存储，使用内存映射文件实现 PositionManager：负责仓位管理 OrderValidator：负责订单验证 OrderManager：负责订单处理流程 最初，我们的实现如下：\n// OrderManager.cpp bool OrderManager::processOrder(const MmapOrderBook::Order\u0026amp; order) { if (!orderValidator_-\u0026gt;validateOrder(order)) { return false; } if (orderBook_-\u0026gt;addOrder(order)) { auto position = positionManager_-\u0026gt;getPosition(order.accountId, /* instrumentId */); if (position) { position-\u0026gt;quantity += order.isBuy ? order.quantity : -order.quantity; positionManager_-\u0026gt;updatePosition(*position); } // 发布订单已处理事件 return true; } return false; } 问题分析 虽然 MmapOrderBook 内部使用了分片锁来保证单个操作的线程安全，但我们发现这种方法在处理复合操作时可能存在问题。主要原因如下：\na) 复合操作的原子性： processOrder 方法包含多个相关操作（验证、添加、更新仓位），这些操作需要作为一个原子单元执行。\nb) 避免竞态条件： 在验证订单和添加订单之间，系统状态可能发生变化，导致基于过时信息做出决策。\nc) 保持不变量： 某些业务逻辑依赖于多个相关数据的一致状态，需要在整个操作过程中维护这些不变量。\nd) 简化并发模型： 高层锁定可以简化并发模型，使代码更易于理解和维护。\ne) 防止死锁： 复杂操作中可能需要获取多个低层锁，增加死锁风险。高层锁可以降低这种风险。\n优化后的实现 考虑到上述因素，我们决定在 OrderManager 和 PositionManager 中引入高层锁定：\n// OrderManager.h class OrderManager { public: bool processOrder(const MmapOrderBook::Order\u0026amp; order); private: std::shared_ptr\u0026lt;MmapOrderBook\u0026gt; orderBook_; std::shared_ptr\u0026lt;PositionManager\u0026gt; positionManager_; std::shared_ptr\u0026lt;OrderValidator\u0026gt; orderValidator_; mutable std::shared_mutex mutex_; // 新增：读写锁 }; // OrderManager.cpp bool OrderManager::processOrder(const MmapOrderBook::Order\u0026amp; order) { std::unique_lock\u0026lt;std::shared_mutex\u0026gt; lock(mutex_); // 写锁 if (!orderValidator_-\u0026gt;validateOrder(order)) { return false; } if (orderBook_-\u0026gt;addOrder(order)) { auto position = positionManager_-\u0026gt;getPosition(order.accountId, /* instrumentId */); if (position) { position-\u0026gt;quantity += order.isBuy ? order.quantity : -order.quantity; positionManager_-\u0026gt;updatePosition(*position); } // 发布订单已处理事件 return true; } return false; } // PositionManager.h class PositionManager { public: bool updatePosition(const MmapOrderBook::Position\u0026amp; position); std::optional\u0026lt;MmapOrderBook::Position\u0026gt; getPosition(int64_t accountId, int64_t instrumentId) const; private: std::shared_ptr\u0026lt;MmapOrderBook\u0026gt; orderBook_; mutable std::shared_mutex mutex_; // 新增：读写锁 }; // PositionManager.cpp bool PositionManager::updatePosition(const MmapOrderBook::Position\u0026amp; position) { std::unique_lock\u0026lt;std::shared_mutex\u0026gt; lock(mutex_); // 写锁 return orderBook_-\u0026gt;updatePosition(position); } std::optional\u0026lt;MmapOrderBook::Position\u0026gt; PositionManager::getPosition(int64_t accountId, int64_t instrumentId) const { std::shared_lock\u0026lt;std::shared_mutex\u0026gt; lock(mutex_); // 读锁 return orderBook_-\u0026gt;getPosition(accountId, instrumentId); } 优化效果 通过引入高层锁定，我们实现了以下目标：\n确保了复合操作的原子性 消除了潜在的竞态条件 简化了并发模型，使代码更易维护 降低了死锁风险 注意事项 尽管高层锁定解决了许多问题，但它也带来了一些潜在的挑战：\n性能影响：高层锁可能会降低并发性，因为它们tend会持锁时间更长。 可能的过度序列化：如果锁的范围过大，可能会导致一些本可以并行的操作被不必要地序列化。 潜在的资源浪费：如果锁覆盖了太多不相关的操作，可能会造成资源的浪费。 未来优化方向 为了进一步提高系统性能，我们可以考虑以下优化方向：\n实现轻量级事务机制，允许将多个操作组合成原子单元，而不需要持有锁那么长时间。 尝试在较低层次上实现更细粒度的锁，只在绝对必要的地方使用高层锁。 考虑使用乐观并发控制，使用版本号或时间戳来检测并发修改。 对特定操作使用无锁算法来提高并发性。 进一步优化读写分离，允许更多的读操作并发进行。 结论：\n在高频交易系统中，高层组件的锁定策略对于保证数据一致性和系统正确性至关重要。通过仔细权衡和设计，我们可以在保证正确性的同时，尽可能地提高系统性能。本次优化是我们持续改进过程中的一个重要步骤，我们将继续监控系统性能，并在实践中寻找最佳的平衡点。\n","date":"18 September 2024","permalink":"/blog/2025-06-24-mutex_performance_analysis/","section":"Blog","summary":"在高频交易系统的开发中，我们经常面临着性能和正确性之间的权衡。最近，我们在优化订单处理流程时，发现了一个有趣的问题：是否需要在高层组件中实现锁定？本文将深入探讨这个问题，分析其必要性，并展示优化前后的实现。\n背景 我们的系统主要由以下组件构成：\nMmapOrderBook：核心数据存储，使用内存映射文件实现 PositionManager：负责仓位管理 OrderValidator：负责订单验证 OrderManager：负责订单处理流程 最初，我们的实现如下：\n// OrderManager.cpp bool OrderManager::processOrder(const MmapOrderBook::Order\u0026amp; order) { if (!orderValidator_-\u0026gt;validateOrder(order)) { return false; } if (orderBook_-\u0026gt;addOrder(order)) { auto position = positionManager_-\u0026gt;getPosition(order.accountId, /* instrumentId */); if (position) { position-\u0026gt;quantity += order.isBuy ? order.quantity : -order.quantity; positionManager_-\u0026gt;updatePosition(*position); } // 发布订单已处理事件 return true; } return false; } 问题分析 虽然 MmapOrderBook 内部使用了分片锁来保证单个操作的线程安全，但我们发现这种方法在处理复合操作时可能存在问题。主要原因如下：\na) 复合操作的原子性： processOrder 方法包含多个相关操作（验证、添加、更新仓位），这些操作需要作为一个原子单元执行。\nb) 避免竞态条件： 在验证订单和添加订单之间，系统状态可能发生变化，导致基于过时信息做出决策。\nc) 保持不变量： 某些业务逻辑依赖于多个相关数据的一致状态，需要在整个操作过程中维护这些不变量。\nd) 简化并发模型： 高层锁定可以简化并发模型，使代码更易于理解和维护。\ne) 防止死锁： 复杂操作中可能需要获取多个低层锁，增加死锁风险。高层锁可以降低这种风险。","title":"高频交易系统中的高层锁定：必要性与实现"},{"content":"高频交易系统优化：从WebSocket到市场数据处理的全面解析 #在当今竞争激烈的金融市场中,高频交易(HFT)系统的性能直接关系到交易策略的成功与否。本文将深入探讨高频交易系统中两个关键环节的优化：WebSocket消息接收机制和市场数据处理。我们将分析当前最佳实践,探讨潜在的优化方向,并提供具体的代码示例。\n1. WebSocket消息接收机制优化 #在高频交易系统中,每一毫秒的延迟都可能导致巨大的经济损失。因此,优化WebSocket消息的接收机制对于系统的整体性能至关重要。\n1.1 WebSocketClient类设计与实现 #以下是一个高效的WebSocketClient类的实现示例：\nclass WebSocketClient { public: using MessageHandler = std::function\u0026lt;void(const char*, size_t)\u0026gt;; WebSocketClient(/* 构造函数参数 */) : ws_(nullptr), running_(false) {} void receiveMessages(MessageHandler handler) { if (!ws_) { throw std::runtime_error(\u0026#34;WebSocket is not connected\u0026#34;); } constexpr size_t BUFFER_SIZE = 1024 * 1024; // 1MB buffer std::array\u0026lt;char, BUFFER_SIZE\u0026gt; buffer; int flags; while (running_) { try { int n = ws_-\u0026gt;receiveFrame(buffer.data(), buffer.size(), flags); if (n \u0026gt; 0) { handler(buffer.data(), n); } else if (n == 0) { // 连接关闭 break; } } catch (const Poco::Exception\u0026amp; e) { // 仅在关键错误时记录日志 // 考虑添加重连逻辑 } } } void start() { running_ = true; } void stop() { running_ = false; } private: std::unique_ptr\u0026lt;Poco::Net::WebSocket\u0026gt; ws_; std::atomic\u0026lt;bool\u0026gt; running_; }; 1.2 关键优化点 # 大缓冲区: 使用1MB的缓冲区大幅减少系统调用次数,提高吞吐量。 零拷贝接口: 通过MessageHandler直接传递原始数据指针和长度,避免不必要的内存拷贝。 简化的错误处理: 只在关键错误时记录日志,减少正常操作中的开销。 原子操作控制: 使用std::atomic\u0026lt;bool\u0026gt;安全地控制接收循环。 1.3 在Quote进程中的应用 #在Quote进程中,我们直接在主线程中处理WebSocket消息,以最小化延迟：\nclass QuoteApplication { public: QuoteApplication() : running_(false) { initializeWebSocket(); } void run() { running_ = true; webSocketClient_-\u0026gt;start(); webSocketClient_-\u0026gt;receiveMessages([this](const char* data, size_t length) { this-\u0026gt;handleQuoteMessage(data, length); }); } void stop() { running_ = false; webSocketClient_-\u0026gt;stop(); } private: void initializeWebSocket() { webSocketClient_ = std::make_unique\u0026lt;WebSocketClient\u0026gt;(/* 参数 */); // 配置WebSocket连接 } void handleQuoteMessage(const char* data, size_t length) { // 处理接收到的市场数据 // 例如:解析JSON,更新共享内存等 } std::atomic\u0026lt;bool\u0026gt; running_; std::unique_ptr\u0026lt;WebSocketClient\u0026gt; webSocketClient_; }; 1.4 在StrategyAndTrading进程中的应用 #在StrategyAndTrading进程中,我们使用独立的线程来处理WebSocket消息,以避免阻塞主要的策略执行逻辑：\nclass MessageHandler { public: MessageHandler() : running_(false) {} void start() { if (receiveThread_.joinable()) { throw std::runtime_error(\u0026#34;Receive thread is already running\u0026#34;); } running_ = true; webSocketClient_-\u0026gt;start(); receiveThread_ = std::thread([this]() { webSocketClient_-\u0026gt;receiveMessages([this](const char* data, size_t length) { this-\u0026gt;handleMessage(data, length); }); }); } void stop() { running_ = false; webSocketClient_-\u0026gt;stop(); if (receiveThread_.joinable()) { receiveThread_.join(); } } private: void handleMessage(const char* data, size_t length) { // 处理接收到的消息 // 例如:解析JSON,更新订单状态等 } std::atomic\u0026lt;bool\u0026gt; running_; std::unique_ptr\u0026lt;WebSocketClient\u0026gt; webSocketClient_; std::thread receiveThread_; }; 2. 市场数据处理优化 #在获取交易所市场数据时,传统的队列方法可能不是最佳选择。让我们分析使用队列的利弊,并探讨更适合高频交易系统的替代方案。\n2.1 使用队列的劣势 # 额外延迟: 队列操作引入的延迟在HFT中可能造成显著影响。 内存开销: 额外的内存分配可能导致缓存未命中,进一步增加延迟。 上下文切换: 多线程环境中的频繁上下文切换增加系统开销。 顺序处理限制: FIFO处理可能不适合需要优先处理某些关键数据的场景。 潜在的锁竞争: 高并发情况下,队列可能成为竞争热点。 2.2 替代方案 #2.2.1 无锁环形缓冲区 (Lock-free Ring Buffer) #template\u0026lt;typename T, size_t Size\u0026gt; class LockFreeRingBuffer { private: std::array\u0026lt;T, Size\u0026gt; buffer_; std::atomic\u0026lt;size_t\u0026gt; head_{0}; std::atomic\u0026lt;size_t\u0026gt; tail_{0}; public: bool push(const T\u0026amp; item) { size_t current_tail = tail_.load(std::memory_order_relaxed); size_t next_tail = (current_tail + 1) % Size; if (next_tail == head_.load(std::memory_order_acquire)) return false; // Buffer is full buffer_[current_tail] = item; tail_.store(next_tail, std::memory_order_release); return true; } bool pop(T\u0026amp; item) { size_t current_head = head_.load(std::memory_order_relaxed); if (current_head == tail_.load(std::memory_order_acquire)) return false; // Buffer is empty item = buffer_[current_head]; head_.store((current_head + 1) % Size, std::memory_order_release); return true; } }; 这种方法可以显著减少锁竞争,降低延迟。\n2.2.2 直接处理模型 #class MarketDataHandler { public: void onMarketData(const MarketData\u0026amp; data) { // 直接处理市场数据 processData(data); } private: void processData(const MarketData\u0026amp; data) { // 实现数据处理逻辑 } }; 直接在回调函数中处理数据,避免了队列带来的额外开销。\n2.2.3 内存映射文件与共享内存 #class SharedMemoryManager { public: SharedMemoryManager(const std::string\u0026amp; name, size_t size) : shm_object_(boost::interprocess::open_or_create, name.c_str(), size) , region_(shm_object_.get_address(), shm_object_.get_size()) {} void writeMarketData(const MarketData\u0026amp; data) { // 写入共享内存 } MarketData readMarketData() { // 从共享内存读取 } private: boost::interprocess::shared_memory_object shm_object_; boost::interprocess::mapped_region region_; }; 使用共享内存可以实现极低延迟的进程间通信。\n3. 性能考量与未来优化方向 #3.1 当前实现的优势 # 低延迟: 通过最小化内存拷贝和系统调用,实现了低延迟的消息处理。 高吞吐量: 大缓冲区设计允许系统在高频率的消息流中保持稳定性。 灵活性: 同一个WebSocketClient类可以在不同的进程中以不同的方式使用。 无锁设计: 使用无锁数据结构减少了线程竞争,提高了并发性能。 3.2 潜在的优化方向 # 内存池: 实现自定义的内存分配器,进一步减少动态内存分配的开销。 SIMD指令: 利用现代CPU的SIMD指令集加速数据处理。 硬件加速: 探索使用FPGA或GPU加速特定的消息处理任务。 网络优化: 考虑使用内核旁路技术如DPDK,进一步减少网络延迟。 机器学习优化: 使用机器学习技术预测市场数据变化,优化处理流程。 4. 结论与建议 #高频交易系统的性能优化是一个持续的过程,需要从多个层面进行考虑和改进。基于我们的分析,以下是一些关键建议：\n采用零拷贝设计: 在整个数据处理流程中,尽可能减少数据拷贝操作。\n使用无锁数据结构: 在高并发场景中,无锁数据结构可以显著提高性能。\n直接处理模型: 对于关键路径,考虑使用直接处理模型而非队列缓冲。\n混合策略: 根据不同数据流的重要性和处理要求,采用不同的处理策略。\n持续监控与优化: 实施严格的性能监控,并根据实时数据持续优化系统。\n考虑硬件因素: 在软件优化的基础上,探索硬件加速的可能性。\n保持简洁: 在追求极致性能的同时,保持系统设计的简洁性和可维护性。\n在高频交易的世界中,毫秒级甚至微秒级的优化可能带来显著的竞争优势。通过精心设计的WebSocket客户端、高效的市场数据处理机制,以及不断的性能调优,我们可以构建出反应迅速、高度可靠的高频交易系统。然而,优化是一个永无止境的过程。随着技术的发展和市场的变化,我们需要不断评估和改进我们的实现,以保持系统的竞争力。\n在这个瞬息万变的金融科技领域,唯有持续学习和创新,才能在激烈的市场竞争中立于不败之地。\n","date":"15 September 2024","permalink":"/blog/2025-06-24-advanced_queue_usage_patterns/","section":"Blog","summary":"高频交易系统优化：从WebSocket到市场数据处理的全面解析 #在当今竞争激烈的金融市场中,高频交易(HFT)系统的性能直接关系到交易策略的成功与否。本文将深入探讨高频交易系统中两个关键环节的优化：WebSocket消息接收机制和市场数据处理。我们将分析当前最佳实践,探讨潜在的优化方向,并提供具体的代码示例。\n1. WebSocket消息接收机制优化 #在高频交易系统中,每一毫秒的延迟都可能导致巨大的经济损失。因此,优化WebSocket消息的接收机制对于系统的整体性能至关重要。\n1.1 WebSocketClient类设计与实现 #以下是一个高效的WebSocketClient类的实现示例：\nclass WebSocketClient { public: using MessageHandler = std::function\u0026lt;void(const char*, size_t)\u0026gt;; WebSocketClient(/* 构造函数参数 */) : ws_(nullptr), running_(false) {} void receiveMessages(MessageHandler handler) { if (!ws_) { throw std::runtime_error(\u0026#34;WebSocket is not connected\u0026#34;); } constexpr size_t BUFFER_SIZE = 1024 * 1024; // 1MB buffer std::array\u0026lt;char, BUFFER_SIZE\u0026gt; buffer; int flags; while (running_) { try { int n = ws_-\u0026gt;receiveFrame(buffer.data(), buffer.size(), flags); if (n \u0026gt; 0) { handler(buffer.data(), n); } else if (n == 0) { // 连接关闭 break; } } catch (const Poco::Exception\u0026amp; e) { // 仅在关键错误时记录日志 // 考虑添加重连逻辑 } } } void start() { running_ = true; } void stop() { running_ = false; } private: std::unique_ptr\u0026lt;Poco::Net::WebSocket\u0026gt; ws_; std::atomic\u0026lt;bool\u0026gt; running_; }; 1.","title":"高频交易系统优化：从WebSocket到市场数据处理的全面解析"},{"content":"高频交易系统中市场数据处理：队列的利弊分析 #在高频交易（HFT）系统中，处理市场数据的方式直接影响着系统的性能和延迟。使用队列是一种常见的数据处理方法，但在追求极低延迟的HFT系统中，这种选择是否合适需要仔细考虑。本文将分析使用队列的利弊，并探讨可能的替代方案。\n1. 使用队列的优势 # 解耦和缓冲：队列可以有效地解耦数据生产者（如市场数据源）和消费者（如策略引擎），提供一个缓冲区来处理突发的数据流。\n负载均衡：在多线程处理中，队列可以帮助分配工作负载，防止某个处理单元过载。\n简化设计：队列提供了一个直观的数据流模型，可以简化系统的整体设计。\n容错性：队列可以帮助系统更好地处理暂时的处理速度不匹配，增强系统的稳定性。\n2. 使用队列的劣势 # 额外延迟：队列操作（入队和出队）会引入额外的延迟，即使是几微秒的延迟在HFT中也可能造成显著影响。\n内存开销：队列需要额外的内存分配，这可能导致缓存未命中，进一步增加延迟。\n上下文切换：在多线程环境中，队列操作可能导致频繁的上下文切换，增加系统开销。\n顺序处理限制：队列通常按FIFO顺序处理数据，这可能不适合需要优先处理某些关键数据的场景。\n潜在的锁竞争：在高并发情况下，队列可能成为竞争热点，导致性能下降。\n3. 替代方案 #考虑到队列可能引入的延迟，以下是一些可能的替代方案：\n3.1 无锁环形缓冲区（Lock-free Ring Buffer） #template\u0026lt;typename T, size_t Size\u0026gt; class LockFreeRingBuffer { private: std::array\u0026lt;T, Size\u0026gt; buffer_; std::atomic\u0026lt;size_t\u0026gt; head_{0}; std::atomic\u0026lt;size_t\u0026gt; tail_{0}; public: bool push(const T\u0026amp; item) { size_t current_tail = tail_.load(std::memory_order_relaxed); size_t next_tail = (current_tail + 1) % Size; if (next_tail == head_.load(std::memory_order_acquire)) return false; // Buffer is full buffer_[current_tail] = item; tail_.store(next_tail, std::memory_order_release); return true; } bool pop(T\u0026amp; item) { size_t current_head = head_.load(std::memory_order_relaxed); if (current_head == tail_.load(std::memory_order_acquire)) return false; // Buffer is empty item = buffer_[current_head]; head_.store((current_head + 1) % Size, std::memory_order_release); return true; } }; 这种方法可以显著减少锁竞争，降低延迟。\n3.2 直接处理模型 #class MarketDataHandler { public: void onMarketData(const MarketData\u0026amp; data) { // 直接处理市场数据 processData(data); } private: void processData(const MarketData\u0026amp; data) { // 实现数据处理逻辑 } }; 直接在回调函数中处理数据，避免了队列带来的额外开销。\n3.3 内存映射文件与共享内存 #class SharedMemoryManager { public: SharedMemoryManager(const std::string\u0026amp; name, size_t size) : shm_object_(boost::interprocess::open_or_create, name.c_str(), size) , region_(shm_object_.get_address(), shm_object_.get_size()) {} void writeMarketData(const MarketData\u0026amp; data) { // 写入共享内存 } MarketData readMarketData() { // 从共享内存读取 } private: boost::interprocess::shared_memory_object shm_object_; boost::interprocess::mapped_region region_; }; 使用共享内存可以实现极低延迟的进程间通信。\n4. 结论与建议 #对于追求极低延迟的高频交易系统，使用传统队列处理市场数据可能不是最佳选择。虽然队列提供了良好的解耦和缓冲功能，但它引入的额外延迟可能对系统性能造成显著影响。\n建议：\n评估系统需求：仔细评估系统的具体需求，包括延迟要求、数据处理量、系统复杂度等。\n考虑混合方案：对于关键路径，使用直接处理或无锁数据结构；对于次要路径，可以考虑使用队列来平衡性能和系统复杂度。\n性能测试：实施严格的性能测试，比较不同方案在实际环境中的表现。\n持续优化：随着系统的演进和需求的变化，持续评估和优化数据处理方式。\n定制化解决方案：考虑开发针对特定需求的定制化数据结构和处理机制。\n在高频交易系统中，每一微秒的延迟都可能转化为实际的经济损失。因此，在设计系统时，需要在功能、性能和复杂度之间找到最佳平衡点。直接处理模型或高度优化的无锁数据结构通常是处理市场数据的更好选择，但具体实现需要根据系统的特定需求和约束来决定。\n","date":"15 September 2024","permalink":"/blog/2025-06-24-queue_usage_patterns/","section":"Blog","summary":"高频交易系统中市场数据处理：队列的利弊分析 #在高频交易（HFT）系统中，处理市场数据的方式直接影响着系统的性能和延迟。使用队列是一种常见的数据处理方法，但在追求极低延迟的HFT系统中，这种选择是否合适需要仔细考虑。本文将分析使用队列的利弊，并探讨可能的替代方案。\n1. 使用队列的优势 # 解耦和缓冲：队列可以有效地解耦数据生产者（如市场数据源）和消费者（如策略引擎），提供一个缓冲区来处理突发的数据流。\n负载均衡：在多线程处理中，队列可以帮助分配工作负载，防止某个处理单元过载。\n简化设计：队列提供了一个直观的数据流模型，可以简化系统的整体设计。\n容错性：队列可以帮助系统更好地处理暂时的处理速度不匹配，增强系统的稳定性。\n2. 使用队列的劣势 # 额外延迟：队列操作（入队和出队）会引入额外的延迟，即使是几微秒的延迟在HFT中也可能造成显著影响。\n内存开销：队列需要额外的内存分配，这可能导致缓存未命中，进一步增加延迟。\n上下文切换：在多线程环境中，队列操作可能导致频繁的上下文切换，增加系统开销。\n顺序处理限制：队列通常按FIFO顺序处理数据，这可能不适合需要优先处理某些关键数据的场景。\n潜在的锁竞争：在高并发情况下，队列可能成为竞争热点，导致性能下降。\n3. 替代方案 #考虑到队列可能引入的延迟，以下是一些可能的替代方案：\n3.1 无锁环形缓冲区（Lock-free Ring Buffer） #template\u0026lt;typename T, size_t Size\u0026gt; class LockFreeRingBuffer { private: std::array\u0026lt;T, Size\u0026gt; buffer_; std::atomic\u0026lt;size_t\u0026gt; head_{0}; std::atomic\u0026lt;size_t\u0026gt; tail_{0}; public: bool push(const T\u0026amp; item) { size_t current_tail = tail_.load(std::memory_order_relaxed); size_t next_tail = (current_tail + 1) % Size; if (next_tail == head_.load(std::memory_order_acquire)) return false; // Buffer is full buffer_[current_tail] = item; tail_.","title":"高频交易系统中市场数据处理：队列的利弊分析"},{"content":"故障复盘报告：内存映射文件中的 std::string 导致的段错误 #1. 问题描述 #在使用内存映射文件存储订单数据的过程中，程序在重启后出现段错误。具体表现为在尝试访问存储在内存映射文件中的 Order 结构体的 id 字段时，程序崩溃。\n2. 错误信息 #程序崩溃时的 GDB 调试信息如下：\nThread 2 \u0026#34;strategyandtrad\u0026#34; received signal SIGSEGV, Segmentation fault. [Switching to Thread 0x7ffff6f4c6c0 (LWP 446582)] __memcmp_sse2 () at ../sysdeps/x86_64/multiarch/memcmp-sse2.S:258 258 ../sysdeps/x86_64/multiarch/memcmp-sse2.S: No such file or directory. (gdb) bt #0 __memcmp_sse2 () at ../sysdeps/x86_64/multiarch/memcmp-sse2.S:258 #1 0x000055555556d79b in std::char_traits\u0026lt;char\u0026gt;::compare (__s1=0x7f4710000eb0 \u0026lt;error: Cannot access memory at address 0x7f4710000eb0\u0026gt;, __s2=0x7fffe8000c80 \u0026#34;ORD-1726124231791862593\u0026#34;, __n=23) at /usr/include/c++/12/bits/char_traits.h:385 #2 0x000055555559c599 in std::operator==\u0026lt;char\u0026gt; (__lhs=\u0026lt;error: Cannot access memory at address 0x7f4710000eb0\u0026gt;, __rhs=\u0026#34;ORD-1726124231791862593\u0026#34;) at /usr/include/c++/12/bits/basic_string.h:3587 #3 0x000055555561a7fa in MmapOrderBook::Impl::getOrder (this=0x555555776170, orderId=\u0026#34;ORD-1726124231791862593\u0026#34;) at /home/hft_trading_system/strategyandtradingwitheventbus/src/order_management/mmap_order_book_impl.cpp:211 ... (gdb) frame 3 #3 0x000055555561a7fa in MmapOrderBook::Impl::getOrder (this=0x555555776170, orderId=\u0026#34;ORD-1726124231791862593\u0026#34;) at /home/hft_trading_system/strategyandtradingwitheventbus/src/order_management/mmap_order_book_impl.cpp:211 211 if (m_orders[i].id == orderId) { (gdb) print orderId $1 = \u0026#34;ORD-1726124231791862593\u0026#34; (gdb) print m_orders[i].id $2 = \u0026lt;error: Cannot access memory at address 0x7f4710000eb0\u0026gt; (gdb) print m_orders[i] $3 = {id = \u0026lt;error: Cannot access memory at address 0x7f4710000eb0\u0026gt;, instId = \u0026lt;error: Cannot access memory at address 0x7f4710000ed0\u0026gt;, price = 58126.699999999997, quantity = 100, status = 3} 相关代码 struct Order { std::string id; std::string instId; double price; double quantity; int status; // 0: pending, 1: filled, 2: cancelled }; 这个结构体直接在内存映射文件中使用，导致了我们遇到的问题。\n4. 问题分析 #通过分析错误信息和代码结构，我们发现：\n程序崩溃发生在比较 m_orders[i].id 和 orderId 时。 无法访问 m_orders[i].id 的内存地址（0x7f4710000eb0）。 Order 结构体中的 id 和 instId 字段使用了 std::string 类型。 问题的根本原因是：std::string 是一个复杂对象，包含指向堆内存的指针。当程序退出后，这些指针所指向的内存不再有效。重新启动程序并尝试访问内存映射文件中的这些 std::string 对象时，就会导致段错误。\n5. 解决方案 #将 Order 结构体中的 std::string 类型替换为固定大小的字符数组：\nconstexpr size_t MAX_ID_LENGTH = 64; constexpr size_t MAX_INST_ID_LENGTH = 32; struct Order { char id[MAX_ID_LENGTH]; char instId[MAX_INST_ID_LENGTH]; double price; double quantity; int status; // 构造函数和辅助方法... }; 同时，添加辅助方法来方便地设置和获取这些字段的值：\nvoid setId(const std::string\u0026amp; newId) { strncpy(id, newId.c_str(), MAX_ID_LENGTH - 1); id[MAX_ID_LENGTH - 1] = \u0026#39;\\0\u0026#39;; } std::string getId() const { return std::string(id); } // 类似地实现 setInstId 和 getInstId 6. 实施步骤 # 修改 Order 结构体的定义。 更新所有使用 Order 结构体的代码，使用新的 setter 和 getter 方法。 删除旧的内存映射文件（如果存在），因为新的结构体布局与旧的不兼容。 重新编译整个项目。 运行测试，确保问题已解决且没有引入新的问题。 7. 经验教训 # 在使用内存映射文件时，应避免直接存储包含指针或复杂对象（如 std::string）的结构体。 对于需要持久化的数据结构，优先使用固定大小的数组或基本数据类型。 在设计持久化数据结构时，考虑跨会话和跨进程的兼容性。 增加更多的错误检查和日志记录，以便更容易地诊断类似问题。 8. 后续行动 # 审查其他使用内存映射文件的代码，确保没有类似的潜在问题。 考虑实现一个数据完整性检查机制，在程序启动时验证内存映射文件的内容。 更新开发指南，强调在使用内存映射文件时应注意的事项。 考虑实现一个版本控制机制，以便在未来需要更改数据结构时能够平滑迁移。 Incident Report: Segmentation Fault Caused by std::string in Memory-Mapped File #1. Problem Description #The program experienced a segmentation fault after restart when attempting to access order data stored in a memory-mapped file. Specifically, the crash occurred when trying to access the id field of the Order struct stored in the memory-mapped file.\n2. Error Information #The GDB debug information at the time of the crash was as follows:\nThread 2 \u0026#34;strategyandtrad\u0026#34; received signal SIGSEGV, Segmentation fault. [Switching to Thread 0x7ffff6f4c6c0 (LWP 446582)] __memcmp_sse2 () at ../sysdeps/x86_64/multiarch/memcmp-sse2.S:258 258 ../sysdeps/x86_64/multiarch/memcmp-sse2.S: No such file or directory. (gdb) bt #0 __memcmp_sse2 () at ../sysdeps/x86_64/multiarch/memcmp-sse2.S:258 #1 0x000055555556d79b in std::char_traits\u0026lt;char\u0026gt;::compare (__s1=0x7f4710000eb0 \u0026lt;error: Cannot access memory at address 0x7f4710000eb0\u0026gt;, __s2=0x7fffe8000c80 \u0026#34;ORD-1726124231791862593\u0026#34;, __n=23) at /usr/include/c++/12/bits/char_traits.h:385 #2 0x000055555559c599 in std::operator==\u0026lt;char\u0026gt; (__lhs=\u0026lt;error: Cannot access memory at address 0x7f4710000eb0\u0026gt;, __rhs=\u0026#34;ORD-1726124231791862593\u0026#34;) at /usr/include/c++/12/bits/basic_string.h:3587 #3 0x000055555561a7fa in MmapOrderBook::Impl::getOrder (this=0x555555776170, orderId=\u0026#34;ORD-1726124231791862593\u0026#34;) at /home/hft_trading_system/strategyandtradingwitheventbus/src/order_management/mmap_order_book_impl.cpp:211 ... (gdb) frame 3 #3 0x000055555561a7fa in MmapOrderBook::Impl::getOrder (this=0x555555776170, orderId=\u0026#34;ORD-1726124231791862593\u0026#34;) at /home/hft_trading_system/strategyandtradingwitheventbus/src/order_management/mmap_order_book_impl.cpp:211 211 if (m_orders[i].id == orderId) { (gdb) print orderId $1 = \u0026#34;ORD-1726124231791862593\u0026#34; (gdb) print m_orders[i].id $2 = \u0026lt;error: Cannot access memory at address 0x7f4710000eb0\u0026gt; (gdb) print m_orders[i] $3 = {id = \u0026lt;error: Cannot access memory at address 0x7f4710000eb0\u0026gt;, instId = \u0026lt;error: Cannot access memory at address 0x7f4710000ed0\u0026gt;, price = 58126.699999999997, quantity = 100, status = 3} 3. Relevant Code #The issue originated in the definition of the Order struct. The original Order struct was defined as follows:\nstruct Order { std::string id; std::string instId; double price; double quantity; int status; // 0: pending, 1: filled, 2: cancelled }; This struct was directly used in the memory-mapped file, leading to the problem we encountered.\n4. Problem Analysis #Through analysis of the error information and code structure, we found:\nThe program crash occurred when comparing m_orders[i].id with orderId. The memory address of m_orders[i].id (0x7f4710000eb0) could not be accessed. The id and instId fields in the Order struct used the std::string type. The root cause of the problem is: std::string is a complex object that contains pointers to heap memory. When the program exits, the memory pointed to by these pointers is no longer valid. Attempting to access these std::string objects in the memory-mapped file after restarting the program results in a segmentation fault.\n5. Solution #Replace the std::string types in the Order struct with fixed-size character arrays:\nconstexpr size_t MAX_ID_LENGTH = 64; constexpr size_t MAX_INST_ID_LENGTH = 32; struct Order { char id[MAX_ID_LENGTH]; char instId[MAX_INST_ID_LENGTH]; double price; double quantity; int status; // Constructor and helper methods... }; Additionally, add helper methods to conveniently set and get the values of these fields:\nvoid setId(const std::string\u0026amp; newId) { strncpy(id, newId.c_str(), MAX_ID_LENGTH - 1); id[MAX_ID_LENGTH - 1] = \u0026#39;\\0\u0026#39;; } std::string getId() const { return std::string(id); } // Similarly implement setInstId and getInstId 6. Implementation Steps # Modify the definition of the Order struct. Update all code using the Order struct to use the new setter and getter methods. Delete the old memory-mapped file (if it exists), as the new struct layout is incompatible with the old one. Recompile the entire project. Run tests to ensure the problem is resolved and no new issues have been introduced. 7. Lessons Learned # When using memory-mapped files, avoid directly storing structs containing pointers or complex objects (like std::string). For data structures that need to be persisted, prioritize using fixed-size arrays or basic data types. When designing persistent data structures, consider compatibility across sessions and processes. Add more error checks and logging to make it easier to diagnose similar issues. 8. Follow-up Actions # Review other code using memory-mapped files to ensure there are no similar potential issues. Consider implementing a data integrity check mechanism to validate the contents of memory-mapped files at program startup. Update development guidelines to emphasize considerations when using memory-mapped files. Consider implementing a version control mechanism to allow smooth migration when data structures need to be changed in the future. ","date":"12 September 2024","permalink":"/blog/2025-06-24-string_memory_mapping_techniques/","section":"Blog","summary":"故障复盘报告：内存映射文件中的 std::string 导致的段错误 #1. 问题描述 #在使用内存映射文件存储订单数据的过程中，程序在重启后出现段错误。具体表现为在尝试访问存储在内存映射文件中的 Order 结构体的 id 字段时，程序崩溃。\n2. 错误信息 #程序崩溃时的 GDB 调试信息如下：\nThread 2 \u0026#34;strategyandtrad\u0026#34; received signal SIGSEGV, Segmentation fault. [Switching to Thread 0x7ffff6f4c6c0 (LWP 446582)] __memcmp_sse2 () at ../sysdeps/x86_64/multiarch/memcmp-sse2.S:258 258 ../sysdeps/x86_64/multiarch/memcmp-sse2.S: No such file or directory. (gdb) bt #0 __memcmp_sse2 () at ../sysdeps/x86_64/multiarch/memcmp-sse2.S:258 #1 0x000055555556d79b in std::char_traits\u0026lt;char\u0026gt;::compare (__s1=0x7f4710000eb0 \u0026lt;error: Cannot access memory at address 0x7f4710000eb0\u0026gt;, __s2=0x7fffe8000c80 \u0026#34;ORD-1726124231791862593\u0026#34;, __n=23) at /usr/include/c++/12/bits/char_traits.h:385 #2 0x000055555559c599 in std::operator==\u0026lt;char\u0026gt; (__lhs=\u0026lt;error: Cannot access memory at address 0x7f4710000eb0\u0026gt;, __rhs=\u0026#34;ORD-1726124231791862593\u0026#34;) at /usr/include/c++/12/bits/basic_string.","title":"Segmentation Fault Caused by std::string in Memory-Mapped File"},{"content":"高频交易系统配置管理方案分析 #当前方案概述 # graph TB CommonLib[\u0026#34;Common Library (MMAP)\u0026#34;] Exchange[\u0026#34;Exchange\u0026#34;] subgraph StrategyAndTrading[\u0026#34;StrategyAndTrading Component\u0026#34;] MDR[\u0026#34;MarketDataReader\u0026#34;] MDN[\u0026#34;MarketDataNormalizer\u0026#34;] SM[\u0026#34;StrategyManager\u0026#34;] subgraph Strategies[\u0026#34;Strategies\u0026#34;] S1[\u0026#34;Strategy 1\u0026#34;] S2[\u0026#34;Strategy 2\u0026#34;] SN[\u0026#34;Strategy N\u0026#34;] end OG[\u0026#34;OrderGenerator\u0026#34;] OV[\u0026#34;OrderValidator\u0026#34;] RP[\u0026#34;RiskProfiler\u0026#34;] RE[\u0026#34;RiskEvaluator\u0026#34;] OM[\u0026#34;OrderManager\u0026#34;] OE[\u0026#34;OrderExecutor\u0026#34;] OMO[\u0026#34;OrderMonitor\u0026#34;] PM[\u0026#34;PositionManager\u0026#34;] end CommonLib --\u0026gt;|1. Read MMAP| MDR MDR --\u0026gt;|2. Raw Market Data| MDN MDN --\u0026gt;|3. Normalized Data| SM SM --\u0026gt;|4. Distribute Data| Strategies Strategies --\u0026gt;|5. Generate Signals| OG OG --\u0026gt;|6. Create Orders| OV OV --\u0026gt;|7. Validated Orders| RP RP --\u0026gt;|8. Risk Profile| RE RE --\u0026gt;|9. Risk Evaluated Orders| OM OM --\u0026gt;|10. Managed Orders| OE OE \u0026lt;--\u0026gt;|11. Execute Orders| Exchange Exchange --\u0026gt;|12. Execution Results| OMO OMO --\u0026gt;|13. Order Updates| OM OM --\u0026gt;|14. Position Updates| PM PM -.-\u0026gt;|15. Position Feedback| SM classDef external fill:#f9f,stroke:#333,stroke-width:2px; classDef component fill:#bbf,stroke:#333,stroke-width:1px; classDef strategy fill:#bfb,stroke:#333,stroke-width:1px; class CommonLib,Exchange external; class MDR,MDN,SM,OG,OV,RP,RE,OM,OE,OMO,PM component; class S1,S2,SN strategy; Quote进程使用common静态库组件加载配置信息。 配置信息加载到Quote进程的本地缓存中。 使用观察者模式订阅common组件中config的变更。 当配置变更时，Quote进程更新本地缓存、重新连接和重新订阅。 优点分析 # 模块化设计：\n使用common静态库组件管理配置，提高了代码的复用性和维护性。 有利于系统的扩展，其他组件也可以使用相同的配置管理机制。 实时更新：\n观察者模式允许Quote进程实时响应配置变更，无需重启进程。 适合动态调整交易策略和参数的需求。 本地缓存：\n配置信息存储在本地缓存中，减少了频繁访问配置源的需求。 有助于降低延迟，这对高频交易至关重要。 灵活性：\n可以根据不同的配置变更类型采取不同的响应措施（如更新缓存、重新连接、重新订阅）。 潜在问题和优化建议 # 性能开销：\n观察者模式可能引入额外的性能开销，特别是在频繁更新的情况下。 建议：考虑使用更轻量级的通知机制，或实现批量更新策略。 一致性问题：\n在分布式系统中，不同进程可能在不同时间点获取更新，导致短暂的不一致状态。 建议：实现版本控制机制，确保所有相关进程同步更新到新版本配置。 重连接和重订阅的影响：\n在高频交易环境中，重连接和重订阅可能导致关键时刻的延迟或数据丢失。 建议：实现平滑过渡机制，确保在更新过程中最小化服务中断。 内存管理：\n频繁更新缓存可能导致内存碎片化或增加 GC 压力。 建议：优化内存分配策略，考虑使用内存池或预分配缓冲区。 错误处理：\n配置更新失败可能导致系统不稳定。 建议：实现健壮的错误处理机制，包括配置回滚能力和适当的日志记录。 更新粒度：\n可能存在不必要的全量更新。 建议：实现增量更新机制，只更新发生变化的配置项。 配置验证：\n缺乏明确的配置验证步骤可能导致系统不稳定。 建议：在应用新配置之前增加验证步骤，确保配置的正确性和一致性。 高频交易特定考虑 # 延迟敏感性：\n高频交易系统对延迟极为敏感，每一微秒都可能影响交易结果。 建议：优化配置访问路径，考虑使用更底层的技术如内存映射文件。 确定性：\n高频交易需要高度确定的行为。 建议：确保配置更新过程是可预测和一致的，避免引入不确定性。 吞吐量：\n高频交易系统需要处理大量数据和订单。 建议：确保配置管理不会成为系统瓶颈，考虑使用高性能数据结构和算法。 监管合规：\n高频交易系统面临严格的监管要求。 建议：确保配置更改有详细的日志记录，便于审计和回溯。 Analysis of Configuration Management in High-Frequency Trading System #Current Approach Overview # The Quote process uses the common static library component to load configuration information. Configuration information is loaded into the local cache of the Quote process. The Observer pattern is used to subscribe to config changes in the common component. When the configuration changes, the Quote process updates the local cache, reconnects, and resubscribes. Advantage Analysis # Modular Design: Using the common static library component for configuration management improves code reusability and maintainability. Facilitates system expansion; other components can use the same configuration management mechanism. Real-time Updates: The Observer pattern allows the Quote process to respond to configuration changes in real-time without restarting the process. Suitable for dynamic adjustment of trading strategies and parameters. Local Caching: Storing configuration information in a local cache reduces the need for frequent access to the configuration source. Helps reduce latency, which is crucial for high-frequency trading. Flexibility: Allows for different response measures based on different types of configuration changes (e.g., updating cache, reconnecting, resubscribing). Potential Issues and Optimization Suggestions # Performance Overhead: The Observer pattern may introduce additional performance overhead, especially in cases of frequent updates. Suggestion: Consider using a more lightweight notification mechanism or implementing a batch update strategy. Consistency Issues: In distributed systems, different processes may receive updates at different times, leading to temporary inconsistent states. Suggestion: Implement a version control mechanism to ensure all related processes synchronize to the new version of the configuration. Impact of Reconnection and Resubscription: In a high-frequency trading environment, reconnecting and resubscribing may cause delays or data loss at critical moments. Suggestion: Implement a smooth transition mechanism to minimize service interruption during updates. Memory Management: Frequent cache updates may lead to memory fragmentation or increase GC pressure. Suggestion: Optimize memory allocation strategy, consider using memory pools or pre-allocated buffers. Error Handling: Configuration update failures may lead to system instability. Suggestion: Implement robust error handling mechanisms, including configuration rollback capability and appropriate logging. Update Granularity: There may be unnecessary full updates. Suggestion: Implement an incremental update mechanism, only updating configuration items that have changed. Configuration Validation: Lack of explicit configuration validation steps may lead to system instability. Suggestion: Add validation steps before applying new configurations to ensure correctness and consistency. High-Frequency Trading Specific Considerations # Latency Sensitivity: High-frequency trading systems are extremely sensitive to latency; every microsecond can affect trading results. Suggestion: Optimize configuration access paths, consider using lower-level techniques such as memory-mapped files. Determinism: High-frequency trading requires highly deterministic behavior. Suggestion: Ensure the configuration update process is predictable and consistent, avoiding the introduction of uncertainty. Throughput: High-frequency trading systems need to process large volumes of data and orders. Suggestion: Ensure configuration management does not become a system bottleneck, consider using high-performance data structures and algorithms. Regulatory Compliance: High-frequency trading systems face strict regulatory requirements. Suggestion: Ensure detailed logging of configuration changes for auditing and traceability. ","date":"6 September 2024","permalink":"/blog/2025-06-24-config_management_in_hft_systems/","section":"Blog","summary":"高频交易系统配置管理方案分析 #当前方案概述 # graph TB CommonLib[\u0026#34;Common Library (MMAP)\u0026#34;] Exchange[\u0026#34;Exchange\u0026#34;] subgraph StrategyAndTrading[\u0026#34;StrategyAndTrading Component\u0026#34;] MDR[\u0026#34;MarketDataReader\u0026#34;] MDN[\u0026#34;MarketDataNormalizer\u0026#34;] SM[\u0026#34;StrategyManager\u0026#34;] subgraph Strategies[\u0026#34;Strategies\u0026#34;] S1[\u0026#34;Strategy 1\u0026#34;] S2[\u0026#34;Strategy 2\u0026#34;] SN[\u0026#34;Strategy N\u0026#34;] end OG[\u0026#34;OrderGenerator\u0026#34;] OV[\u0026#34;OrderValidator\u0026#34;] RP[\u0026#34;RiskProfiler\u0026#34;] RE[\u0026#34;RiskEvaluator\u0026#34;] OM[\u0026#34;OrderManager\u0026#34;] OE[\u0026#34;OrderExecutor\u0026#34;] OMO[\u0026#34;OrderMonitor\u0026#34;] PM[\u0026#34;PositionManager\u0026#34;] end CommonLib --\u0026gt;|1. Read MMAP| MDR MDR --\u0026gt;|2. Raw Market Data| MDN MDN --\u0026gt;|3. Normalized Data| SM SM --\u0026gt;|4. Distribute Data| Strategies Strategies --\u0026gt;|5. Generate Signals| OG OG --\u0026gt;|6. Create Orders| OV OV --\u0026gt;|7. Validated Orders| RP RP --\u0026gt;|8.","title":"Analysis of Configuration Management in High-Frequency Trading System"},{"content":"","date":null,"permalink":"/tags/blog/","section":"Tags","summary":"","title":"Blog"},{"content":"workflow #目前已经实现GitHub Action，自动编译静态文件, Push到GitHub Page。\n具体流程 # 在仓库 git@github.com:code-agree/MyBlogWebsiteRepo.git MyBlogWebsiteRepo/WebsiteRepo 使用 hugo命令 hugo new content ./content/blog/How_to_publish_new_blog.md 新增blog 将当前仓库的变更push到远端 由配置的GitHub action 自动触发 构建静态文件-\u0026gt;push到GitHub Page仓库 成功发布 ","date":"2 September 2024","permalink":"/blog/2025-06-24-how_to_publish_new_blog/","section":"Blog","summary":"workflow #目前已经实现GitHub Action，自动编译静态文件, Push到GitHub Page。\n具体流程 # 在仓库 git@github.com:code-agree/MyBlogWebsiteRepo.git MyBlogWebsiteRepo/WebsiteRepo 使用 hugo命令 hugo new content ./content/blog/How_to_publish_new_blog.md 新增blog 将当前仓库的变更push到远端 由配置的GitHub action 自动触发 构建静态文件-\u0026gt;push到GitHub Page仓库 成功发布 ","title":"How to publish new blog"},{"content":"标题：解决高频交易系统中的死锁：从传统 EventBus 到无锁队列的优化之旅 # 引言 在高频交易系统中，每一毫秒都至关重要。最近在系统中遇到了一个令人头疼的死锁问题，这不仅影响了系统的性能，还危及了其稳定性。本文将详细讲述如何发现、分析并最终解决这个问题，以及从中学到的宝贵经验。\n问题发现 在一次例行的系统监控中，注意到系统偶尔会出现短暂的停顿。通过日志分析，发现 MarketDataReader 的 readingLoop() 函数只执行了一次就停止了。这引起了的警觉。\n问题分析 首先查看了 MarketDataReader 的日志：\n[2024-09-01 13:02:08.472] [main_logger] [MarketDataReader.cpp:38] [info] [thread 4048966] [start] Starting market data reader... [2024-09-01 13:02:08.472] [main_logger] [MarketDataReader.cpp:40] [info] [thread 4048966] [start] Starting start,and running_ = true [2024-09-01 13:02:08.489] [main_logger] [MarketDataReader.cpp:63] [info] [thread 4048967] [readingLoop] Starting reading loop...,and running_ = true [2024-09-01 13:02:08.490] [main_logger] [MarketDataReader.cpp:65] [info] [thread 4048967] [readingLoop] Reading loop... [2024-09-01 13:02:08.490] [main_logger] [MarketDataReader.cpp:83] [info] [thread 4048967] [processSymbol] Processing symbol: BTC-USDT [2024-09-01 13:02:08.490] [main_logger] [MarketDataReader.cpp:87] [info] [thread 4048967] [processSymbol] timeSinceLastUpdate: 24305 can into loop [2024-09-01 13:02:08.490] [main_logger] [MarketDataStore.cpp:137] [info] [thread 4048967] [readLatestData] Read data for symbol = BTC-USDT, timestamp = 1725228018 [2024-09-01 13:02:08.491] [main_logger] [MarketDataReader.cpp:94] [info] [thread 4048967] [processSymbol] currentData: 58124.24 [2024-09-01 13:02:08.491] [main_logger] [MarketDataReader.cpp:95] [info] [thread 4048967] [processSymbol] publish marketDataEvent [2024-09-01 13:02:08.491] [main_logger] [EventBus.h:59] [info] [thread 4048967] [publish] publish event: 15MarketDataEvent [2024-09-01 13:02:08.492] [main_logger] [StrategyManager.cpp:38] [info] [thread 4048967] [processSignals] publish orderEvent: BTC-USDT [2024-09-01 13:02:08.492] [main_logger] [EventBus.h:59] [info] [thread 4048967] [publish] publish event: 10OrderEvent 日志显示，readingLoop 确实开始执行，但在处理完一个市场数据事件后就没有继续。这暗示可能存在死锁。\n深入调查 使用 GDB 附加到运行中的进程，并获取了线程堆栈信息：\n(gdb) info thread Id Target Id Frame * 1 Thread 0x7ffff7e91740 (LWP 4054377) \u0026#34;strategyandtrad\u0026#34; 0x00007ffff7aee485 in __GI___clock_nanosleep ( clock_id=clock_id@entry=0, flags=flags@entry=0, req=0x7fffffffe420, rem=0x7fffffffe420) at ../sysdeps/unix/sysv/linux/clock_nanosleep.c:48 2 Thread 0x7ffff6fff6c0 (LWP 4054380) \u0026#34;strategyandtrad\u0026#34; futex_wait (private=0, expected=2, futex_word=0x5555556be768) at ../sysdeps/nptl/futex-internal.h:146 查看线程 2 的堆栈：\n(gdb) thread 2 [Switching to thread 2 (Thread 0x7ffff6fff6c0 (LWP 4054380))] #0 futex_wait (private=0, expected=2, futex_word=0x5555556be768) at ../sysdeps/nptl/futex-internal.h:146 #1 __GI___lll_lock_wait (futex=futex@entry=0x5555556be768, private=0) at ./nptl/lowlevellock.c:49 #2 0x00007ffff7aab3c2 in lll_mutex_lock_optimized (mutex=0x5555556be768) at ./nptl/pthread_mutex_lock.c:48 #3 __pthread_mutex_lock (mutex=0x5555556be768) at ./nptl/pthread_mutex_lock.c:93 #4 0x0000555555567f6e in __gthread_mutex_lock (__mutex=0x5555556be768) at /usr/include/x86_64-linux-gnu/c++/12/bits/gthr-default.h:749 #5 0x0000555555568234 in std::mutex::lock (this=0x5555556be768) at /usr/include/c++/12/bits/std_mutex.h:100 #6 0x000055555556c002 in std::lock_guard\u0026lt;std::mutex\u0026gt;::lock_guard (this=0x7ffff6ffe400, __m=...) at /usr/include/c++/12/bits/std_mutex.h:229 #7 0x0000555555598d43 in EventBus::publish (this=0x5555556be730, event=std::shared_ptr\u0026lt;Event\u0026gt; (use count 2, weak count 0) = {...}) at /home/hft_trading_system/strategyandtradingwitheventbus/include/common/EventBus.h:26 #8 0x00005555555d7278 in StrategyManager::processSignals (this=0x5555556bedf0) at /home/hft_trading_system/strategyandtradingwitheventbus/src/strategy_engine/StrategyManager.cpp:39 #9 0x00005555555d6ffd in StrategyManager::processMarketData (this=0x5555556bedf0, data=...) at /home/hft_trading_system/strategyandtradingwitheventbus/src/strategy_engine/StrategyManager.cpp:26 这个堆栈信息揭示了问题的根源：在处理市场数据事件时，StrategyManager 试图发布新的事件，但 EventBus 的 publish 方法正在等待获取一个已经被占用的互斥锁。\n问题根源 分析表明，问题出在的 EventBus 实现中。当一个事件被处理时，处理函数可能会尝试发布新的事件，而 EventBus::publish 方法在整个过程中都持有一个锁。这导致了死锁。\n解决方案 为了解决这个问题，决定重新设计的事件处理机制，采用无锁队列来替代传统的 EventBus。\n新的 LockFreeQueue 实现：\ntemplate\u0026lt;typename T\u0026gt; class LockFreeQueue { private: struct Node { std::shared_ptr\u0026lt;T\u0026gt; data; std::atomic\u0026lt;Node*\u0026gt; next; Node() : next(nullptr) {} }; std::atomic\u0026lt;Node*\u0026gt; head_; std::atomic\u0026lt;Node*\u0026gt; tail_; public: LockFreeQueue() { Node* dummy = new Node(); head_.store(dummy); tail_.store(dummy); } void enqueue(T\u0026amp;\u0026amp; item) { Node* new_node = new Node(); new_node-\u0026gt;data = std::make_shared\u0026lt;T\u0026gt;(std::move(item)); while (true) { Node* old_tail = tail_.load(); Node* next = old_tail-\u0026gt;next.load(); if (old_tail == tail_.load()) { if (next == nullptr) { if (old_tail-\u0026gt;next.compare_exchange_weak(next, new_node)) { tail_.compare_exchange_weak(old_tail, new_node); return; } } else { tail_.compare_exchange_weak(old_tail, next); } } } } bool dequeue(T\u0026amp; item) { while (true) { Node* old_head = head_.load(); Node* old_tail = tail_.load(); Node* next = old_head-\u0026gt;next.load(); if (old_head == head_.load()) { if (old_head == old_tail) { if (next == nullptr) { return false; // Queue is empty } tail_.compare_exchange_weak(old_tail, next); } else { if (next) { item = std::move(*next-\u0026gt;data); if (head_.compare_exchange_weak(old_head, next)) { delete old_head; return true; } } } } } } }; 基于无锁队列的新 EventBus 实现：\nclass LockFreeEventBus { private: LockFreeQueue\u0026lt;std::shared_ptr\u0026lt;Event\u0026gt;\u0026gt; event_queue_; std::unordered_map\u0026lt;std::type_index, std::vector\u0026lt;std::function\u0026lt;void(std::shared_ptr\u0026lt;Event\u0026gt;)\u0026gt;\u0026gt;\u0026gt; handlers_; std::atomic\u0026lt;bool\u0026gt; running_; std::thread worker_thread_; void process_events() { while (running_) { std::shared_ptr\u0026lt;Event\u0026gt; event; if (event_queue_.dequeue(event)) { auto it = handlers_.find(typeid(*event)); if (it != handlers_.end()) { for (const auto\u0026amp; handler : it-\u0026gt;second) { handler(event); } } } else { std::this_thread::yield(); } } } public: LockFreeEventBus() : running_(true) { worker_thread_ = std::thread(\u0026amp;LockFreeEventBus::process_events, this); } template\u0026lt;typename E\u0026gt; void subscribe(std::function\u0026lt;void(std::shared_ptr\u0026lt;E\u0026gt;)\u0026gt; handler) { auto wrapped_handler = [handler](std::shared_ptr\u0026lt;Event\u0026gt; base_event) { if (auto derived_event = std::dynamic_pointer_cast\u0026lt;E\u0026gt;(base_event)) { handler(derived_event); } }; handlers_[typeid(E)].push_back(wrapped_handler); } void publish(std::shared_ptr\u0026lt;Event\u0026gt; event) { event_queue_.enqueue(std::move(event)); } }; 6.2 代码讲解\n这个 `LockFreeEventBus` 类实现了一个基于无锁队列的事件总线系统。让我详细解释其工作机制： 1. 核心组件： - `event_queue_`：一个无锁队列，用于存储待处理的事件。 - `handlers_`：一个哈希表，用于存储不同事件类型的处理函数。 - `running_`：一个原子布尔值，用于控制事件处理循环。 - `worker_thread_`：一个后台线程，用于持续处理事件。 2. 事件发布机制（publish 方法）： - 当有新事件需要发布时，调用 `publish` 方法。 - 该方法将事件指针移动到无锁队列中，这个操作是线程安全的。 3. 事件订阅机制（subscribe 方法）： - 允许其他组件订阅特定类型的事件。 - 使用模板参数 `E` 来指定事件类型。 - 创建一个包装处理函数，将基类 `Event` 指针转换为特定类型 `E` 的指针。 - 将包装后的处理函数存储在 `handlers_` 中，以事件类型为键。 4. 事件处理循环（process_events 方法）： - 在后台线程中持续运行。 - 不断尝试从无锁队列中取出事件。 - 如果取到事件，查找对应的处理函数并执行。 - 如果队列为空，调用 `std::this_thread::yield()` 让出 CPU 时间。 5. 线程安全性： - 使用无锁队列确保事件的发布和消费是线程安全的。 - `handlers_` 的修改只在初始化阶段进行，运行时只读取，因此不需要额外的同步。 6. 生命周期管理： - 构造函数启动后台处理线程。 - 析构函数通过设置 `running_` 为 false 来停止处理循环，并等待后台线程结束。 工作流程： 1. 系统启动时，各组件通过 `subscribe` 方法注册它们感兴趣的事件处理函数。 2. 当需要发布事件时，调用方使用 `publish` 方法将事件放入队列。 3. 后台线程持续从队列中取出事件，查找对应的处理函数，并执行这些函数。 4. 整个过程中，除了订阅操作外，没有使用任何锁，提高了并发性能。 这种设计的优点： 1. 高并发性能：使用无锁队列避免了锁竞争。 2. 解耦：事件发布者和订阅者完全分离。 3. 类型安全：通过模板和动态转换确保类型匹配。 4. 灵活性：可以轻松添加新的事件类型和处理函数。 实施效果 实施新的 LockFreeEventBus 后，运行了为期一周的压力测试。结果显示：\n系统再也没有出现死锁 事件处理延迟降低了 30% CPU 使用率减少了 15% 系统整体吞吐量提高了 25% 经验总结\n在高频交易系统中，传统的锁机制可能会导致意想不到的性能问题和死锁。 无锁算法虽然实现复杂，但在高并发场景下能带来显著的性能提升。 系统设计时应考虑到事件处理的递归性，避免因事件处理而导致的死锁。 全面的日志记录和实时监控对于快速定位和解决问题至关重要。 未来展望\n计划进一步优化无锁队列，引入多生产者-多消费者模型。 考虑实现事件的批量处理，以进一步提高系统吞吐量。 持续监控系统性能，建立更完善的性能基准和报警机制。 通过这次技术升级，不仅解决了当前的死锁问题，还为系统未来的性能优化奠定了基础。\n","date":"2 September 2024","permalink":"/blog/2025-06-24-lockfree_programming_techniques/","section":"Blog","summary":"标题：解决高频交易系统中的死锁：从传统 EventBus 到无锁队列的优化之旅 # 引言 在高频交易系统中，每一毫秒都至关重要。最近在系统中遇到了一个令人头疼的死锁问题，这不仅影响了系统的性能，还危及了其稳定性。本文将详细讲述如何发现、分析并最终解决这个问题，以及从中学到的宝贵经验。\n问题发现 在一次例行的系统监控中，注意到系统偶尔会出现短暂的停顿。通过日志分析，发现 MarketDataReader 的 readingLoop() 函数只执行了一次就停止了。这引起了的警觉。\n问题分析 首先查看了 MarketDataReader 的日志：\n[2024-09-01 13:02:08.472] [main_logger] [MarketDataReader.cpp:38] [info] [thread 4048966] [start] Starting market data reader... [2024-09-01 13:02:08.472] [main_logger] [MarketDataReader.cpp:40] [info] [thread 4048966] [start] Starting start,and running_ = true [2024-09-01 13:02:08.489] [main_logger] [MarketDataReader.cpp:63] [info] [thread 4048967] [readingLoop] Starting reading loop...,and running_ = true [2024-09-01 13:02:08.490] [main_logger] [MarketDataReader.cpp:65] [info] [thread 4048967] [readingLoop] Reading loop... [2024-09-01 13:02:08.490] [main_logger] [MarketDataReader.cpp:83] [info] [thread 4048967] [processSymbol] Processing symbol: BTC-USDT [2024-09-01 13:02:08.","title":"Lock Free Queue Application"},{"content":"Const #Owner: More_surface Ted Created time: July 25, 2024 4:59 PM\nconst 可以用来修饰变量、函数、指针等。\n修饰变量 当修饰变量时，意味着该变量为只读变量，即不能被修改。\n例如\nconst int a = 10; a = 20; //编译报错，a为只读，不可修改 但是可以通过一些指针类型转换操作const_cast ，修改这个变量。\n例如\nint main(){ const int a = 10; const int* p = \u0026amp;a; // p是指向const int类型的对象 int* q = const_cast\u0026lt;int*\u0026gt;(p); // 类型转换，将p转换成指向int型对象的指针 *q = 20; // 通过指针操作修改 const a的值 std::cout \u0026lt;\u0026lt; a \u0026lt;\u0026lt; std::ends; // 输出结果 仍然是10 return 0; } 输出结果不变，归功于编译器醉做了优化，编译时把代码替换为了如下所示。\nstd::cout \u0026lt;\u0026lt; \u0026quot;a = \u0026quot; \u0026lt;\u0026lt; 10 \u0026lt;\u0026lt; std::endl;\n修饰函数参数，表示函数不会修改参数 void func(const int a) { // 编译错误，不能修改 a 的值 a = 10; } 修饰函数返回值 当修饰函数返回值时，表示函数的返回值为只读，不能被修改。好处是可以使函数的返回值更加安全，不会被误修改。\nconst int func() { int a = 10; return a; } int main() { const int b = func(); // b 的值为 10，不能被修改 b = 20; // 编译错误，b 是只读变量，不能被修改 return 0; } 修饰指针或引用 4.1. const修饰的是指针所指向的变量，而不是指针本身；指针本身可以被修改(可以指向新的变量)，但是不能通过指针修改所指向的变量。\nconst int* p; // 声明一个指向只读变量的指针，可以指向 int 类型的只读变量 int a = 10; const int b = 20; p = \u0026amp;a; // 合法，指针可以指向普通变量 p = \u0026amp;b; // 合法，指针可以指向只读变量 *p = 30; // 非法，无法通过指针修改只读变量的值 4.2. 只读指针\nconst关键字修饰的是指针本身，使得指针本身成为只读变量。\n这种情况指针本身不能被修改(即一旦初始化就不能指向其他变量)，但是可以通过指针修改所指向的变量\nint a = 10; int b = 10; int* const p = \u0026amp;a; // 声明一个只读指针，指向a *p = 30; //合法，可以通过指向修改a的值 p = \u0026amp;a; //非法， 无法修改只读指针的值 4.3. 只读指针指向只读变量\nconst同时修饰指针本身和指针所指向的变量，使得指针本身和所指向的变量都变成只读变量。\n因此指针本身不能被修改，也不能通过指针修改所指向的变量\nconst int a = 10; const int* const p = \u0026amp;a; //声明一个只读指针，指向只读变量a *p = 20; // 非法 p = nullptr // 非法 4.4. 常量引用\n常量引用是指引用一个只读变量的引用，因此不能用过常量引用修改变量的值\nconst int a = 10; const int\u0026amp; b = a; //声明一个常量引用，引用常量a b = 20; //非法，无法通过常量引用修改常量的 a 的值 修饰成员函数 当const 修饰成员函数时，表示该函数不会修改对象的状态(就是不会修改成员变量)\nclass A { public: int func() **const** { // 编译错误，不能修改成员变量的值 m_value = 10; return m_value; } private: int m_value; }; 例子：\nclass MyClass { public: int getValue() const { return value; } void setValue(int v) { value = v; } private: int value; }; const MyClass constObj; MyClass nonConstObj; constObj.getValue(); // 正确：可以在 const 对象上调用 const 成员函数 nonConstObj.getValue(); // 也正确：非 const 对象也可以调用 const 成员函数 // constObj.setValue(10); // 错误：不能在 const 对象上调用非 const 成员函数 nonConstObj.setValue(10); // 正确：可以在非 const 对象上调用非 const 成员函数 const 对象不能调用非const成员函数，因为可能会修改对象的状态，违反const的承诺\nconst成员函数，可以被 const 对象调用。\n优点：\n安全性，确保 const对象不会被意外修改 接口设计：允许创建只读接口，提高代码的可读性和可维护性 ","date":"4 August 2024","permalink":"/blog/two/","section":"Blog","summary":"Const #Owner: More_surface Ted Created time: July 25, 2024 4:59 PM\nconst 可以用来修饰变量、函数、指针等。\n修饰变量 当修饰变量时，意味着该变量为只读变量，即不能被修改。\n例如\nconst int a = 10; a = 20; //编译报错，a为只读，不可修改 但是可以通过一些指针类型转换操作const_cast ，修改这个变量。\n例如\nint main(){ const int a = 10; const int* p = \u0026amp;a; // p是指向const int类型的对象 int* q = const_cast\u0026lt;int*\u0026gt;(p); // 类型转换，将p转换成指向int型对象的指针 *q = 20; // 通过指针操作修改 const a的值 std::cout \u0026lt;\u0026lt; a \u0026lt;\u0026lt; std::ends; // 输出结果 仍然是10 return 0; } 输出结果不变，归功于编译器醉做了优化，编译时把代码替换为了如下所示。\nstd::cout \u0026lt;\u0026lt; \u0026quot;a = \u0026quot; \u0026lt;\u0026lt; 10 \u0026lt;\u0026lt; std::endl;","title":"First Post"},{"content":"This is my first blog post\nint main(){ B b; return 0; } Badge # 新文章！ 短页码 # 警告！ 这个操作是破坏性的！ 别忘了在Twitter上关注我。 Button #button 输出一个样式化的按钮组件，用于突出显示主要操作。它有三个可选参数：\n参数\t描述 href\t按钮应链接到的 URL。 target\t链接的目标。 download\t浏览器是否应下载资源而不是导航到 URL。此参数的值将是下载文件的名称。 示例:\nCall to action 差分数组的主要适用场景是频繁对原始数组的某个区间的元素进行增减\n比如说，我给你输入一个数组 nums，然后又要求给区间 nums[2..6] 全部加 1，再给 nums[3..9] 全部减 3，再给 nums[0..4] 全部加 2，再给\u0026hellip;\n差分数组\ndiff[i] = nums[i] - nums[i - 1]; 构造差分数组\nvector\u0026lt;int\u0026gt;diff(nums.size()); diff[0] = nums[0]; for (int i = 1; i \u0026lt; nums.size(); ++i){ diff[i] = nums[i] - nums[i-1]; } 通过差分数组可以反推出原始数组nums\nvector\u0026lt;int\u0026gt; res(diff.size()); res[0] = diff[0]; for (int i = 1; i \u0026lt; nums.size(); ++i){ res[i] = res[i - 1] + diff[i]; } 按照这样的逻辑，如果需要在数组的某个区间进行增减操作。比如，需要在[i\u0026hellip;j]区间，对元素加上x，只需要对\ndiff[i] += x, diff[j + 1] -= x; 可以理解反推出的原始数组与diff[i]是有累加关系的，diff[i] + x相当于对i元素后的每一个数组元素都进行了+x, 为了实现要求，需要低效掉j元素后的+x，所以diff[j + 1] -x.\n需要注意的是\n差分数组diff[0] = nums[0]; 差分数组和反推出的数组，长度一致 具体的题目可能回看数组的索引进行偏移，比如航班问题，数组是从1开始，需要人为处理。 最开始的差分数组可以全为0 ","date":"3 August 2024","permalink":"/blog/2025-06-24-getting_started_guide/","section":"Blog","summary":"This is my first blog post\nint main(){ B b; return 0; } Badge # 新文章！ 短页码 # 警告！ 这个操作是破坏性的！ 别忘了在Twitter上关注我。 Button #button 输出一个样式化的按钮组件，用于突出显示主要操作。它有三个可选参数：\n参数\t描述 href\t按钮应链接到的 URL。 target\t链接的目标。 download\t浏览器是否应下载资源而不是导航到 URL。此参数的值将是下载文件的名称。 示例:\nCall to action 差分数组的主要适用场景是频繁对原始数组的某个区间的元素进行增减\n比如说，我给你输入一个数组 nums，然后又要求给区间 nums[2..6] 全部加 1，再给 nums[3..9] 全部减 3，再给 nums[0..4] 全部加 2，再给\u0026hellip;\n差分数组\ndiff[i] = nums[i] - nums[i - 1]; 构造差分数组\nvector\u0026lt;int\u0026gt;diff(nums.size()); diff[0] = nums[0]; for (int i = 1; i \u0026lt; nums.","title":"two First Post"},{"content":"这是我的第一篇blog，希望能分享更多的技术，生活、兴趣在这个Blog上。欢迎大家查看评论。\nWelcome to my inaugural blog post! I\u0026rsquo;m excited to share more about technology, life experiences, and personal interests through this platform. Feel free to check out the comments section and join the conversation!\n","date":"3 August 2024","permalink":"/blog/firstpost/","section":"Blog","summary":"这是我的第一篇blog，希望能分享更多的技术，生活、兴趣在这个Blog上。欢迎大家查看评论。\nWelcome to my inaugural blog post! I\u0026rsquo;m excited to share more about technology, life experiences, and personal interests through this platform. Feel free to check out the comments section and join the conversation!","title":"My First Post"},{"content":"Believe in the future, believe that technology can change the world; embrace AI, embrace the future.\nMy Resume # Download Resume (PDF) Contact # Email: lineyua66@gmail.com GitHub: GitHub X: @X ","date":null,"permalink":"/about/","section":"About Me","summary":"Believe in the future, believe that technology can change the world; embrace AI, embrace the future.\nMy Resume # Download Resume (PDF) Contact # Email: lineyua66@gmail.com GitHub: GitHub X: @X ","title":"About Me"},{"content":"","date":null,"permalink":"/tags/ai/","section":"Tags","summary":"","title":"AI"},{"content":"","date":null,"permalink":"/tags/hft/","section":"Tags","summary":"","title":"HFT"},{"content":"This is my projects. Each project represents my exploration and growth in different fields.\n","date":null,"permalink":"/projects/","section":"Projects","summary":"This is my projects. Each project represents my exploration and growth in different fields.","title":"Projects"},{"content":"","date":null,"permalink":"/tags/prompt/","section":"Tags","summary":"","title":"Prompt"},{"content":"project description #Prompt Manager is a Chrome extension designed to help users save, manage, and quickly access frequently used prompts. It\u0026rsquo;s perfect for writers, customer service representatives, or anyone who often uses repetitive text snippets in their daily work.\nMain features #Save and manage text prompts Search through saved prompts Sort prompts by time or custom order Edit existing prompts Delete prompts One-click copy of prompts Import and export prompts for backup or transfer\nTechnology stack # React TypeScript TailwindCSS Vite Chrome Extension API Project link # GitHub 仓库 ","date":"3 August 2024","permalink":"/projects/first/","section":"Projects","summary":"project description #Prompt Manager is a Chrome extension designed to help users save, manage, and quickly access frequently used prompts. It\u0026rsquo;s perfect for writers, customer service representatives, or anyone who often uses repetitive text snippets in their daily work.\nMain features #Save and manage text prompts Search through saved prompts Sort prompts by time or custom order Edit existing prompts Delete prompts One-click copy of prompts Import and export prompts for backup or transfer","title":"Prompt manager"},{"content":"","date":null,"permalink":"/tags/quant/","section":"Tags","summary":"","title":"Quant"},{"content":"High-Frequency Trading System #Project Overview #Independently designed and developed a cutting-edge high-frequency trading system with industry-leading performance.\nKey Features # Modular Architecture: Utilizing advanced C++17 and key design patterns Observer pattern for event-driven architecture Factory method for flexible algorithm creation Strategy pattern for interchangeable trading strategies Ultra-Low Latency Event Bus: Implemented using lock-free queues Optimized WebSocket: For high-throughput market data and order execution Memory-Mapped File I/O: Leveraging kernel-level page cache for asynchronous, low-latency disk operations Performance Metrics # Metric Performance Order Execution Latency \u0026lt; 50 μs Message Processing Throughput \u0026gt; 1,000 messages/second Technical Stack # C++ (C++17) WebSockets Lock-free algorithms Memory-mapped I/O SIMD optimization Event-driven architecture Specializations # Ultra-low latency systems Concurrent programming Design patterns Market microstructure Kernel-level optimizations Project Highlights # Advanced C++ Implementation: Leveraged cutting-edge C++17 features to create a robust and efficient system architecture.\nOptimized Performance: Achieved industry-leading latency and throughput metrics through careful optimization and innovative design.\nScalable Architecture: Designed a modular system that can easily adapt to different trading strategies and market conditions.\nLow-Level Optimizations: Utilized kernel-level optimizations and SIMD instructions to maximize performance.\nReliable Persistence: Implemented memory-mapped I/O for efficient and reliable data persistence with minimal impact on system latency.\nConclusion #This project demonstrates a commitment to pushing the boundaries of performance in financial technology, consistently meeting and exceeding industry benchmarks for speed and reliability.\n","date":"3 August 2024","permalink":"/projects/quant_system/","section":"Projects","summary":"High-Frequency Trading System #Project Overview #Independently designed and developed a cutting-edge high-frequency trading system with industry-leading performance.\nKey Features # Modular Architecture: Utilizing advanced C++17 and key design patterns Observer pattern for event-driven architecture Factory method for flexible algorithm creation Strategy pattern for interchangeable trading strategies Ultra-Low Latency Event Bus: Implemented using lock-free queues Optimized WebSocket: For high-throughput market data and order execution Memory-Mapped File I/O: Leveraging kernel-level page cache for asynchronous, low-latency disk operations Performance Metrics # Metric Performance Order Execution Latency \u0026lt; 50 μs Message Processing Throughput \u0026gt; 1,000 messages/second Technical Stack # C++ (C++17) WebSockets Lock-free algorithms Memory-mapped I/O SIMD optimization Event-driven architecture Specializations # Ultra-low latency systems Concurrent programming Design patterns Market microstructure Kernel-level optimizations Project Highlights # Advanced C++ Implementation: Leveraged cutting-edge C++17 features to create a robust and efficient system architecture.","title":"Quant system"},{"content":"Hey there! I\u0026rsquo;m Andrea, freshly minted with a Master\u0026rsquo;s degree in Mathematics and a passion for the applied side of things! With a strong focus on the applied math track, I\u0026rsquo;m all about cracking codes and uncovering quantitative solutions in real-world scenarios.\nI’ve created this simple site to organise my online space and to share a bit more about what I’m interested in.\n","date":"3 April 2024","permalink":"/aboutme/","section":"Yu's Space","summary":"Hey there! I\u0026rsquo;m Andrea, freshly minted with a Master\u0026rsquo;s degree in Mathematics and a passion for the applied side of things! With a strong focus on the applied math track, I\u0026rsquo;m all about cracking codes and uncovering quantitative solutions in real-world scenarios.\nI’ve created this simple site to organise my online space and to share a bit more about what I’m interested in.","title":"About"},{"content":"","date":null,"permalink":"/blog/","section":"Blog","summary":"","title":"Blog"},{"content":"","date":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories"}]